{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1ZNAtIM3nCfJjW+7yl3TE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmdiegoar/Quant-code-t0/blob/main/Caminos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAhj-FMopUPi"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium stable-baselines3 numpy matplotlib\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "from gym import spaces\n",
        "from stable_baselines3 import PPO\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Simular 1200 carreteras (reemplazar con tus datos reales)\n",
        "roads = [[0] + [np.clip(np.random.uniform(-0.5, 0.5) + y, -1, 1) for y in [0]*31] for _ in range(1200)]\n",
        "# Ejemplo de cómo cargar tus carreteras desde un CSV:\n",
        "# import pandas as pd\n",
        "# df = pd.read_csv(\"carreteras.csv\")  # 1200 filas, 32 columnas\n",
        "# roads = [row.tolist() for _, row in df.iterrows()]\n",
        "\n",
        "class RoadFollowingEnv(gym.Env):\n",
        "    def __init__(self, roads):\n",
        "        super(RoadFollowingEnv, self).__init__()\n",
        "        self.action_space = spaces.Discrete(3)  # 0: mantener, 1: izquierda, 2: derecha\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(6,), dtype=np.float32)  # x, y, ángulo, y_traza, delta_y1, delta_y2\n",
        "        self.roads = roads\n",
        "        self.current_road_idx = 0\n",
        "        self.position = np.array([0.0, 0.0])\n",
        "        self.angle = 0.0\n",
        "        self.velocity = 1.0\n",
        "        self.step_count = 0\n",
        "        self.max_steps = 1000\n",
        "        self.current_road = self.roads[0]\n",
        "\n",
        "    def reset(self):\n",
        "        self.position = np.array([0.0, 0.0])\n",
        "        self.angle = 0.0\n",
        "        self.step_count = 0\n",
        "        self.current_road_idx = (self.current_road_idx + 1) % len(self.roads)  # Siguiente carretera\n",
        "        self.current_road = self.roads[self.current_road_idx]\n",
        "        return self._get_observation()\n",
        "\n",
        "    def _get_observation(self):\n",
        "        x = self.position[0]\n",
        "        idx = int(x)\n",
        "        if idx >= len(self.current_road) - 1:\n",
        "            idx = len(self.current_road) - 2\n",
        "        frac = x - idx\n",
        "        y_traza = self.current_road[idx] + frac * (self.current_road[idx + 1] - self.current_road[idx])\n",
        "        delta_y1 = self.current_road[min(idx + 1, len(self.current_road) - 1)] - self.current_road[idx]\n",
        "        delta_y2 = self.current_road[min(idx + 2, len(self.current_road) - 1)] - self.current_road[min(idx + 1, len(self.current_road) - 1)]\n",
        "        return np.array([self.position[0], self.position[1], self.angle, y_traza, delta_y1, delta_y2], dtype=np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        self.step_count += 1\n",
        "        if action == 1:  # Izquierda\n",
        "            self.angle -= 0.1\n",
        "        elif action == 2:  # Derecha\n",
        "            self.angle += 0.1\n",
        "        self.position[0] += self.velocity * np.cos(self.angle) * 0.1\n",
        "        self.position[1] += self.velocity * np.sin(self.angle) * 0.1\n",
        "\n",
        "        x = self.position[0]\n",
        "        idx = int(x)\n",
        "        if idx >= len(self.current_road) - 1:\n",
        "            idx = len(self.current_road) - 2\n",
        "        frac = x - idx\n",
        "        y_traza = self.current_road[idx] + frac * (self.current_road[idx + 1] - self.current_road[idx])\n",
        "        distance_to_track = abs(self.position[1] - y_traza)\n",
        "        reward = 1.0 - distance_to_track\n",
        "        if distance_to_track > 0.5:\n",
        "            reward = -10.0\n",
        "            done = True\n",
        "        else:\n",
        "            done = self.step_count >= self.max_steps or x >= len(self.current_road) - 1\n",
        "        return self._get_observation(), reward, done, {}\n",
        "\n",
        "    def render(self):\n",
        "        plt.clf()\n",
        "        x = np.arange(0, len(self.current_road), 1)\n",
        "        y = self.current_road\n",
        "        plt.plot(x, y, 'b-', label='Traza ideal')\n",
        "        plt.plot(self.position[0], self.position[1], 'ro', label='Vehículo')\n",
        "        plt.legend()\n",
        "        plt.title(f\"Carretera {self.current_road_idx}\")\n",
        "        plt.show()\n",
        "\n",
        "# Entrenar el agente\n",
        "env = RoadFollowingEnv(roads)\n",
        "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
        "episode_rewards = []\n",
        "for episode in range(1200):  # Entrenar en cada carretera\n",
        "    obs = env.reset()\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action, _ = model.predict(obs)\n",
        "        obs, reward, done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "        if steps % 100 == 0:\n",
        "            env.render()\n",
        "    avg_reward = total_reward / steps\n",
        "    episode_rewards.append(avg_reward)\n",
        "    print(f\"Episodio {episode + 1}, Carretera {env.current_road_idx}, Recompensa promedio: {avg_reward:.2f}\")\n",
        "    if avg_reward > 0.8:  # Criterio de aprendizaje\n",
        "        print(f\"Carretera {env.current_road_idx} aprendida!\")\n",
        "    model.learn(total_timesteps=1000)  # Entrenar 1000 pasos por episodio\n",
        "\n",
        "# Guardar el modelo\n",
        "model.save(\"ppo_road_follower\")\n",
        "\n",
        "# Probar en una carretera nueva\n",
        "test_road = [0, 0.3, -0.2, -0.5, 0.1, 0.7, -0.2, 0.4] + [0]*24  # Nueva carretera con 32 puntos\n",
        "env.current_road = test_road\n",
        "obs = env.reset()\n",
        "positions = [(obs[0], obs[1])]\n",
        "for _ in range(1000):\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, reward, done, _ = env.step(action)\n",
        "    positions.append((obs[0], obs[1]))\n",
        "    env.render()\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "# Visualizar resultados\n",
        "plt.plot(range(32), test_road, 'b-', label='Traza ideal')\n",
        "plt.plot([p[0] for p in positions], [p[1] for p in positions], 'r-', label='Vehículo')\n",
        "plt.legend()\n",
        "plt.title(\"Prueba en carretera nueva\")\n",
        "plt.show()\n",
        "\n",
        "# Guardar modelo para descargar\n",
        "from google.colab import files\n",
        "files.download(\"ppo_road_follower.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Caminos"
      ],
      "metadata": {
        "id": "7tHU2Fcgpwvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"carreteras.csv\")  # 1200 filas, 32 columnas\n",
        "roads = [row.tolist() for _, row in df.iterrows()]"
      ],
      "metadata": {
        "id": "zLnhxUsWpuQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Giros"
      ],
      "metadata": {
        "id": "Qbm8NkfhpXoz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lxz_bJiPpZ7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium stable-baselines3 numpy matplotlib\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "from gym import spaces\n",
        "from stable_baselines3 import PPO\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Simular 1200 carreteras (reemplazar con tus datos reales)\n",
        "roads = [[0] + [np.clip(np.random.uniform(-0.5, 0.5) + y, -1, 1) for y in [0]*31] for _ in range(1200)]\n",
        "# Ejemplo de cómo cargar tus carreteras desde un CSV:\n",
        "# import pandas as pd\n",
        "# df = pd.read_csv(\"carreteras.csv\")  # 1200 filas, 32 columnas\n",
        "# roads = [row.tolist() for _, row in df.iterrows()]\n",
        "\n",
        "class RoadFollowingEnv(gym.Env):\n",
        "    def __init__(self, roads):\n",
        "        super(RoadFollowingEnv, self).__init__()\n",
        "        self.action_space = spaces.Discrete(3)  # 0: mantener, 1: izquierda, 2: derecha\n",
        "        self.observation_space = spaces.Box(low=-1, high=1, shape=(3,), dtype=np.float32)  # y_vehículo - y_traza, signo(delta_y1), signo(delta_y2)\n",
        "        self.roads = roads\n",
        "        self.current_road_idx = 0\n",
        "        self.position = np.array([0.0, 0.0])\n",
        "        self.angle = 0.0\n",
        "        self.velocity = 1.0\n",
        "        self.step_count = 0\n",
        "        self.max_steps = 1000\n",
        "        self.current_road = self.roads[0]\n",
        "\n",
        "    def reset(self):\n",
        "        self.position = np.array([0.0, 0.0])\n",
        "        self.angle = 0.0\n",
        "        self.step_count = 0\n",
        "        self.current_road_idx = (self.current_road_idx + 1) % len(self.roads)  # Siguiente carretera\n",
        "        self.current_road = self.roads[self.current_road_idx]\n",
        "        return self._get_observation()\n",
        "\n",
        "    def _get_observation(self):\n",
        "        x = self.position[0]\n",
        "        idx = int(x)\n",
        "        if idx >= len(self.current_road) - 1:\n",
        "            idx = len(self.current_road) - 2\n",
        "        frac = x - idx\n",
        "        y_traza = self.current_road[idx] + frac * (self.current_road[idx + 1] - self.current_road[idx])\n",
        "        delta_y1 = self.current_road[min(idx + 1, len(self.current_road) - 1)] - self.current_road[idx]\n",
        "        delta_y2 = self.current_road[min(idx + 2, len(self.current_road) - 1)] - self.current_road[min(idx + 1, len(self.current_road) - 1)]\n",
        "        sign_delta_y1 = np.sign(delta_y1)\n",
        "        sign_delta_y2 = np.sign(delta_y2)\n",
        "        return np.array([self.position[1] - y_traza, sign_delta_y1, sign_delta_y2], dtype=np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        self.step_count += 1\n",
        "        if action == 1:  # Izquierda\n",
        "            self.angle -= 0.1\n",
        "        elif action == 2:  # Derecha\n",
        "            self.angle += 0.1\n",
        "        self.position[0] += self.velocity * np.cos(self.angle) * 0.1\n",
        "        self.position[1] += self.velocity * np.sin(self.angle) * 0.1\n",
        "\n",
        "        x = self.position[0]\n",
        "        idx = int(x)\n",
        "        if idx >= len(self.current_road) - 1:\n",
        "            idx = len(self.current_road) - 2\n",
        "        frac = x - idx\n",
        "        y_traza = self.current_road[idx] + frac * (self.current_road[idx + 1] - self.current_road[idx])\n",
        "        distance_to_track = abs(self.position[1] - y_traza)\n",
        "        reward = 1.0 - distance_to_track\n",
        "        if action != 0:  # Penalizar giros innecesarios\n",
        "            delta_y1 = self.current_road[min(idx + 1, len(self.current_road) - 1)] - self.current_road[idx]\n",
        "            delta_y2 = self.current_road[min(idx + 2, len(self.current_road) - 1)] - self.current_road[min(idx + 1, len(self.current_road) - 1)]\n",
        "            if np.sign(delta_y1) == np.sign(delta_y2):  # No hay cambio de sentido\n",
        "                reward -= 0.1\n",
        "        if distance_to_track > 0.5:\n",
        "            reward = -10.0\n",
        "            done = True\n",
        "        else:\n",
        "            done = self.step_count >= self.max_steps or x >= len(self.current_road) - 1\n",
        "        return self._get_observation(), reward, done, {}\n",
        "\n",
        "    def render(self):\n",
        "        plt.clf()\n",
        "        x = np.arange(0, len(self.current_road), 1)\n",
        "        y = self.current_road\n",
        "        plt.plot(x, y, 'b-', label='Traza ideal')\n",
        "        plt.plot(self.position[0], self.position[1], 'ro', label='Vehículo')\n",
        "        plt.legend()\n",
        "        plt.title(f\"Carretera {self.current_road_idx}\")\n",
        "        plt.show()\n",
        "\n",
        "# Entrenar el agente\n",
        "env = RoadFollowingEnv(roads)\n",
        "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
        "episode_rewards = []\n",
        "for episode in range(1200):  # Entrenar en cada carretera\n",
        "    obs = env.reset()\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action, _ = model.predict(obs)\n",
        "        obs, reward, done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "        if steps % 100 == 0:\n",
        "            env.render()\n",
        "    avg_reward = total_reward / steps\n",
        "    episode_rewards.append(avg_reward)\n",
        "    print(f\"Episodio {episode + 1}, Carretera {env.current_road_idx}, Recompensa promedio: {avg_reward:.2f}\")\n",
        "    if avg_reward > 0.8:  # Criterio de aprendizaje\n",
        "        print(f\"Carretera {env.current_road_idx} aprendida!\")\n",
        "    model.learn(total_timesteps=1000)  # Entrenar 1000 pasos por episodio\n",
        "\n",
        "# Guardar el modelo\n",
        "model.save(\"ppo_road_follower_change\")\n",
        "\n",
        "# Probar en una carretera nueva\n",
        "test_road = [0, 0.3, -0.2, -0.5, 0.1, 0.7, -0.2, 0.4] + [0]*24  # Nueva carretera con 32 puntos\n",
        "env.current_road = test_road\n",
        "obs = env.reset()\n",
        "positions = [(obs[0], obs[1])]\n",
        "for _ in range(1000):\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, reward, done, _ = env.step(action)\n",
        "    positions.append((obs[0], obs[1]))\n",
        "    env.render()\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "# Visualizar resultados\n",
        "plt.plot(range(32), test_road, 'b-', label='Traza ideal')\n",
        "plt.plot([p[0] for p in positions], [p[1] for p in positions], 'r-', label='Vehículo')\n",
        "plt.legend()\n",
        "plt.title(\"Prueba en carretera nueva\")\n",
        "plt.show()\n",
        "\n",
        "# Guardar modelo para descargar\n",
        "from google.colab import files\n",
        "files.download(\"ppo_road_follower_change.zip\")"
      ],
      "metadata": {
        "id": "oM_XgBXMpbdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Probar modelos guardado"
      ],
      "metadata": {
        "id": "g0HV5dQyqZkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO\n",
        "model = PPO.load(\"ppo_road_follower_angle\")  # O ppo_road_follower_change\n",
        "env = RoadFollowingEnv(roads)  # Cargar con cualquier lista de carreteras\n",
        "test_road = [0, 0.3, -0.2, -0.5, 0.1, 0.7, -0.2, 0.4] + [0]*24\n",
        "env.current_road = test_road\n",
        "obs = env.reset()\n",
        "positions = [(obs[0], obs[1])]\n",
        "for _ in range(1000):\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, reward, done, _ = env.step(action)\n",
        "    positions.append((obs[0], obs[1]))\n",
        "    env.render()\n",
        "    if done:\n",
        "        break\n",
        "plt.plot(range(32), test_road, 'b-', label='Traza ideal')\n",
        "plt.plot([p[0] for p in positions], [p[1] for p in positions], 'r-', label='Vehículo')\n",
        "plt.legend()\n",
        "plt.title(\"Prueba en carretera nueva\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OTerAevHqcwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verla"
      ],
      "metadata": {
        "id": "kqIbSS8OtS5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Pega aquí tu carretera (32 puntos)\n",
        "road = [0, 0.3, -0.2, -0.5, 0.1, 0.7, -0.2, 0.4, 0.0, 0.2, -0.1, 0.3, -0.3, 0.5, -0.4, 0.1, 0.0, 0.2, -0.2, 0.4, -0.5, 0.0, 0.3, -0.1, 0.2, -0.4, 0.5, -0.3, 0.1, 0.0, -0.2, 0.3]  # Ejemplo, reemplaza con tus 32 puntos\n",
        "\n",
        "# Verificar que la carretera tenga 32 puntos\n",
        "if len(road) != 32:\n",
        "    print(\"Error: La carretera debe tener exactamente 32 puntos\")\n",
        "else:\n",
        "    # Graficar\n",
        "    x = np.arange(0, 32, 1)  # x = 0, 1, ..., 31\n",
        "    y = road\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(x, y, 'b-', marker='o', markersize=3, label='Carretera')\n",
        "    plt.xlabel('x')\n",
        "    plt.ylabel('y')\n",
        "    plt.title('Carretera con 32 puntos')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "V0hpuTmytU2U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}