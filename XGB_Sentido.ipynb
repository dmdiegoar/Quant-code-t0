{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmdiegoar/Quant-code-t0/blob/main/XGB_Sentido.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LA MEJORA DE GROK"
      ],
      "metadata": {
        "id": "oqaw_AHkEbnN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "mas listo que ayer"
      ],
      "metadata": {
        "id": "6x9IIbsCpVUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, f1_score\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "from google.colab import files  # Para descargar el CSV en Colab\n",
        "from scipy import stats\n",
        "\n",
        "# Funciones de features\n",
        "def add_lagged_price_features(df):\n",
        "    for lag in range(1, 6):\n",
        "        df[f'close_lag_{lag}'] = df['Close'].shift(lag)\n",
        "    return df\n",
        "\n",
        "def calculate_RSI(series, period=7):\n",
        "    delta = series.diff(1)\n",
        "    gain = delta.where(delta > 0, 0).rolling(window=period).mean()\n",
        "    loss = -delta.where(delta < 0, 0).rolling(window=period).mean()\n",
        "    rs = gain / loss\n",
        "    return 100 - (100 / (1 + rs))\n",
        "\n",
        "def calculate_ROC(series, period=5):\n",
        "    return ((series - series.shift(period)) / series.shift(period)) * 100\n",
        "\n",
        "def calculate_PPO(series, fast_period=5, slow_period=9, signal_period=5):\n",
        "    ema_fast = series.ewm(span=fast_period, adjust=False).mean()\n",
        "    ema_slow = series.ewm(span=slow_period, adjust=False).mean()\n",
        "    ppo = (ema_fast - ema_slow) / ema_slow * 100\n",
        "    signal_line = ppo.ewm(span=signal_period, adjust=False).mean()\n",
        "    histogram = ppo - signal_line\n",
        "    return ppo, signal_line, histogram\n",
        "\n",
        "def calculate_EWO(series, fast_period=5, slow_period=35, signal_period=5):\n",
        "    ema_fast = series.ewm(span=fast_period, adjust=False).mean()\n",
        "    ema_slow = series.ewm(span=slow_period, adjust=False).mean()\n",
        "    ewo = (ema_fast - ema_slow) / ema_slow * 100\n",
        "    signal_line = ewo.ewm(span=signal_period, adjust=False).mean()\n",
        "    histogram = ewo - signal_line\n",
        "    return ewo, signal_line, histogram\n",
        "\n",
        "def calculate_volatility(series, window=20):\n",
        "    return series.rolling(window).std().round(6)\n",
        "\n",
        "def calculate_sma5(series, period=5):\n",
        "    return series.rolling(window=period).mean().round(4)\n",
        "\n",
        "def calculate_sma13(series, period=13):\n",
        "    return series.rolling(window=period).mean().round(4)\n",
        "\n",
        "def calculate_sma26(series, period=26):\n",
        "    return series.rolling(window=period).mean().round(4)\n",
        "\n",
        "def calculate_sma50(series, period=50):\n",
        "    return series.rolling(window=period).mean().round(4)\n",
        "\n",
        "def calculate_sma200(series, period=200):\n",
        "    return series.rolling(window=period).mean().round(4)\n",
        "\n",
        "\n",
        "#def calculate_earnings_season(df):\n",
        "#    df['Is_Earnings_Season'] = df.index.month.isin([1, 4, 7, 10])\n",
        "#    return df\n",
        "\n",
        "#def calculate_christmas_rally(df):\n",
        "#    df['Is_Christmas_Rally'] = df.index.month.isin([11, 12])\n",
        "#    return df\n",
        "\n",
        "def create_features(df):\n",
        "    df = add_lagged_price_features(df)\n",
        "    df['Pct_change'] = df['Close'].pct_change()\n",
        "    for lag in range(1, 6):\n",
        "        df[f'lag_change{lag}'] = df['Pct_change'].shift(lag)\n",
        "    df['RSI'] = calculate_RSI(df['Close'])\n",
        "    df['ROC'] = calculate_ROC(df['Close'])\n",
        "    df['PPO'], df['PPO_Signal'], df['PPO_Histogram'] = calculate_PPO(df['Close'])\n",
        "    df['EWO'], df['EWO_Signal'], df['EWO_Histogram'] = calculate_EWO(df['Close'])\n",
        "    df['SMA5'] = calculate_sma5(df['Close'])\n",
        "    df['SMA13'] = calculate_sma13(df['Close'])\n",
        "    df['SMA26'] = calculate_sma26(df['Close'])\n",
        "    df['SMA50'] = calculate_sma50(df['Close'])\n",
        "    df['SMA200'] = calculate_sma200(df['Close'])\n",
        "    df['Volatility'] = calculate_volatility(df['Close'])\n",
        "    df['Label'] = (df['Pct_change'] > 0).astype(int)\n",
        "    df['Return'] = np.log(df['Close'] / df['Close'].shift())\n",
        "    #df = calculate_earnings_season(df)\n",
        "    #df = calculate_christmas_rally(df)\n",
        "    df.dropna(inplace=True)\n",
        "    return df\n",
        "\n",
        "# Definir fecha de corte manualmente (cambiar diariamente)\n",
        "end_date = dt.datetime(2025, 7, 20)  # Ejemplo: cambiar a 2025-07-18 mañana\n",
        "symbol=\"ALUA.BA\"\n",
        "#symbol=\"COME.BA\"\n",
        "# Fechas dinámicas\n",
        "start_date = dt.datetime(2001, 1, 1)  # Inicio fijo\n",
        "train_end = end_date - pd.Timedelta(days=780)  # 6 meses antes de end_date (ajustable)\n",
        "next_day = end_date + pd.Timedelta(days=1)  # Predicción para el día siguiente\n",
        "backtest_start = end_date - pd.Timedelta(days=200)  # Inicio del backtesting 6 meses antes de end_date (ajustable)\n",
        "\n",
        "# Descargar datos\n",
        "df = yf.download(symbol, start=start_date, end=end_date, auto_adjust=False)\n",
        "\n",
        "# Verificar datos\n",
        "if df.empty:\n",
        "    raise ValueError(\"No se pudieron descargar datos. Verifica el símbolo, las fechas o la conexión.\")\n",
        "\n",
        "# Aplanar MultiIndex si existe\n",
        "if isinstance(df.columns, pd.MultiIndex):\n",
        "    print(\"MultiIndex detectado en columnas. Aplanando...\")\n",
        "    df.columns = ['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']\n",
        "    print(\"Columnas asignadas después de aplanamiento:\", df.columns.tolist())\n",
        "    print(\"Últimas filas antes de corrección:\", df.tail())\n",
        "\n",
        "    # Corregir el orden de las columnas según tu mapeo\n",
        "    columns = df.columns.tolist()\n",
        "    df = df[[columns[4], columns[2], columns[3], columns[0], columns[5], columns[1]]]  # Reordenar\n",
        "    df.columns = ['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']  # Asignar nombres correctos\n",
        "    print(\"Últimas filas después de corrección:\", df.tail())\n",
        "\n",
        "if isinstance(df.index, pd.MultiIndex):\n",
        "    print(\"MultiIndex detectado en índice. Seleccionando ticker...\")\n",
        "    df = df.xs(symbol, level='Ticker', axis=0)\n",
        "df.index = pd.to_datetime(df.index)  # Asegurar índice datetime\n",
        "if not df.index.is_unique:\n",
        "    print(\"Advertencia: Índice con fechas duplicadas. Eliminando duplicados...\")\n",
        "    df = df[~df.index.duplicated(keep='first')]\n",
        "\n",
        "# Verificar columnas\n",
        "print(\"Columnas del DataFrame después de descargar y corregir:\")\n",
        "print(df.columns)\n",
        "\n",
        "df['Open']= df['Open'].round(2)\n",
        "df['High']= df['High'].round(2)\n",
        "df['Low']= df['Low'].round(2)\n",
        "df['Close']= df['Close'].round(2)\n",
        "df['Adj Close']= df['Adj Close'].round(2)\n",
        "\n",
        "print(\"Últimas filas del DataFrame antes de crear features:\")\n",
        "print(df.tail())\n",
        "\n",
        "# Crear features\n",
        "df = create_features(df)\n",
        "\n",
        "# Verificar datos después de crear features\n",
        "print(\"\\nÚltimas filas del DataFrame después de crear features:\")\n",
        "print(df.tail())\n",
        "\n",
        "# Seleccionar features\n",
        "features = ['RSI', 'ROC', 'PPO', 'PPO_Signal', 'PPO_Histogram', 'EWO', 'EWO_Signal', 'EWO_Histogram', 'Volatility', 'SMA5', 'SMA13', 'SMA26', 'SMA50', 'SMA200' ] + [f'lag_change{i}' for i in range(1, 6)] + \\\n",
        "           [f'close_lag_{i}' for i in range(1, 6)]\n",
        "X = df[features]\n",
        "y = df['Label']\n",
        "\n",
        "# Dividir datos en entrenamiento y prueba\n",
        "X_train_full = X[df.index <= train_end]\n",
        "y_train_full = y[df.index <= train_end]\n",
        "X_test = X[(df.index > train_end) & (df.index <= end_date)]  # Hasta end_date\n",
        "y_test = y[(df.index > train_end) & (df.index <= end_date)]\n",
        "\n",
        "# Optimizar hiperparámetros con RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7, 9],\n",
        "    'n_estimators': [100, 500, 900],\n",
        "    'subsample': [0.6, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "    'gamma': [0, 0.1, 0.2]\n",
        "}\n",
        "xgb = XGBClassifier(objective='binary:logistic', random_state=42)\n",
        "random_search = RandomizedSearchCV(xgb, param_distributions=param_dist, n_iter=20, cv=5, scoring='f1', n_jobs=-1, random_state=42)\n",
        "random_search.fit(X_train_full, y_train_full)\n",
        "print(\"Mejores hiperparámetros:\", random_search.best_params_)\n",
        "\n",
        "# Usar el mejor modelo\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Optimizar el umbral\n",
        "y_train_prob = best_model.predict_proba(X_train_full)[:, 1]\n",
        "thresholds = np.arange(0.1, 0.9, 0.1)\n",
        "best_threshold = 0.5\n",
        "best_f1 = 0\n",
        "for threshold in thresholds:\n",
        "    y_pred_threshold = (y_train_prob >= threshold).astype(int)\n",
        "    f1 = f1_score(y_train_full, y_pred_threshold)\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_threshold = threshold\n",
        "print(\"Mejor umbral:\", best_threshold)\n",
        "\n",
        "#best_threshold=0.15\n",
        "\n",
        "\n",
        "# Backtesting con escalado dinámico para evitar data leak\n",
        "dates = df[(df.index >= backtest_start) & (df.index <= end_date)].index\n",
        "results = []\n",
        "\n",
        "for test_date in dates:\n",
        "    train_data = df[df.index < test_date].copy()\n",
        "    if train_data.empty or train_data['Label'].isna().all():\n",
        "        continue\n",
        "    train_data = train_data.dropna()\n",
        "\n",
        "    X_train_loop = train_data[features]\n",
        "    y_train_loop = train_data['Label']\n",
        "\n",
        "    scaler = StandardScaler()  # Reinicio dinámico del escalador\n",
        "    X_train_scaled = scaler.fit_transform(X_train_loop)\n",
        "    print(f\"Escalando datos hasta {train_data.index[-1]} para predecir {test_date}\")\n",
        "\n",
        "    best_model.fit(X_train_scaled, y_train_loop)\n",
        "\n",
        "    if test_date in df.index:\n",
        "        test_row = df.loc[[test_date]][features]\n",
        "        if test_row.empty:\n",
        "            continue\n",
        "        test_row_scaled = scaler.transform(test_row)\n",
        "\n",
        "        prediction_prob = best_model.predict_proba(test_row_scaled)[0][1]\n",
        "        prediction = 1 if prediction_prob >= best_threshold else 0\n",
        "\n",
        "        real_direction = df.loc[test_date, 'Label'] if pd.notna(df.loc[test_date, 'Label']) else None\n",
        "        close_price = df.loc[test_date, 'Close']\n",
        "\n",
        "        is_correct = int(prediction == real_direction) if real_direction is not None else None\n",
        "        data_date = df.index[df.index < test_date][-1] if not df[df.index < test_date].empty else None\n",
        "\n",
        "        results.append({\n",
        "            'Fecha Predicción': test_date,\n",
        "            'Fecha Datos': data_date,\n",
        "            'Predicción': 'Alcista' if prediction == 1 else 'Bajista',\n",
        "            'Resultado Real': 'Alcista' if real_direction == 1 else 'Bajista' if real_direction == 0 else None,\n",
        "            'Precio Cierre': close_price,\n",
        "            'Probabilidad Alcista': prediction_prob,\n",
        "            'Correcta': 'Sí' if is_correct == 1 else 'No' if is_correct == 0 else None\n",
        "        })\n",
        "\n",
        "# Crear tabla de resultados\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.set_index('Fecha Predicción', inplace=True)\n",
        "\n",
        "# Mostrar resultados\n",
        "pd.set_option('display.max_columns', None)\n",
        "print(\"\\nResultados del backtesting (hasta\", end_date.strftime('%Y-%m-%d'), \"):\")\n",
        "print(\"Nota: 'Fecha Predicción' es la fecha predicha; 'Fecha Datos' es la fecha de los datos usados.\")\n",
        "print(results_df)\n",
        "\n",
        "# Guardar y descargar el CSV\n",
        "results_df.to_csv(f\"backtesting_results_{symbol}_{end_date.strftime('%Y-%m-%d')}.csv\", sep=\";\")\n",
        "files.download(f\"backtesting_results_{symbol}_{end_date.strftime('%Y-%m-%d')}.csv\")\n",
        "print(f\"\\nArchivo 'backtesting_results_{symbol}_{end_date.strftime('%Y-%m-%d')}.csv' generado y descargado.\")\n",
        "\n",
        "# Métricas del backtesting\n",
        "#if results_df['Correcta'].notna().sum() > 0:\n",
        "#    accuracy = (results_df['Correcta'] == 'Sí').sum() / results_df['Correcta'].notna().sum()\n",
        "#    print(f\"\\nAccuracy del backtesting: {accuracy:.2%}\")\n",
        "\n",
        "#    valid_results = results_df[results_df['Correcta'].notna()]\n",
        "#    y_true = [1 if r == 'Alcista' else 0 for r in valid_results['Resultado Real']]\n",
        "#    y_pred = [1 if p == 'Alcista' else 0 for p in valid_results['Predicción']]\n",
        "#    print(\"\\nMatriz de Confusión:\")\n",
        "#    print(confusion_matrix(y_true, y_pred))\n",
        "#    print(\"\\nInforme de Clasificación:\")\n",
        "#    print(classification_report(y_true, y_pred))\n",
        "\n",
        "# ROC-AUC para entrenamiento y prueba\n",
        "if not X_train_full.empty and not X_test.empty:\n",
        "    X_train_scaled = StandardScaler().fit_transform(X_train_full)\n",
        "    X_test_scaled = StandardScaler().fit_transform(X_test)\n",
        "    best_model.fit(X_train_scaled, y_train_full)\n",
        "    train_pred_proba = best_model.predict_proba(X_train_scaled)[:, 1]\n",
        "    test_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
        "    roc_auc_train = roc_auc_score(y_train_full, train_pred_proba)\n",
        "    roc_auc_test = roc_auc_score(y_test, test_pred_proba)\n",
        "    # Verificar tamaños de las muestras\n",
        "    print(f\"Tamaño de y_train_full: {y_train_full.size}\")\n",
        "    print(f\"Tamaño de y_test: {y_test.size}\")\n",
        "    print(\"Distribución de clases en y_train_full:\")\n",
        "    print(y_train_full.value_counts())\n",
        "    print(\"Distribución de clases en y_test:\")\n",
        "    print(y_test.value_counts())\n",
        "\n",
        "\n",
        "    print(f\"\\nROC-AUC en el conjunto de entrenamiento: {roc_auc_train:.4f}\")\n",
        "    print(roc_auc_train)\n",
        "    print(f\"ROC-AUC en el conjunto de prueba: {roc_auc_test:.4f}\")\n",
        "    y_pred_test = (best_model.predict_proba(X_test_scaled)[:, 1] >= 0.3).astype(int)\n",
        "    print(\"\\nMatriz de Confusión (prueba):\")\n",
        "    print(confusion_matrix(y_test, y_pred_test))\n",
        "    print(\"\\nInforme de Clasificación (prueba):\")\n",
        "    print(classification_report(y_test, y_pred_test))\n",
        "else:\n",
        "    print(\"\\nAdvertencia: Conjunto de prueba o entrenamiento insuficiente. No se calculó ROC-AUC.\")\n",
        "\n",
        "# Predicción para el día siguiente\n",
        "last_features = df[features].iloc[-1:]\n",
        "scaler = StandardScaler()\n",
        "last_features_scaled = scaler.fit_transform(last_features)\n",
        "future_pred_prob = best_model.predict_proba(last_features_scaled)[0][1].round(4)\n",
        "future_pred = 1 if future_pred_prob >= best_threshold else 0\n",
        "\n",
        "returns = df['Return'].dropna()\n",
        "mean_return = returns.mean()\n",
        "std_return = returns.std()\n",
        "last_close = df['Close'].iloc[-1]\n",
        "expected_price = last_close * np.exp(mean_return + 0.5 * std_return**2)\n",
        "price_prob = future_pred_prob if future_pred == 1 else 1 - future_pred_prob\n",
        "\n",
        "action = 'BUY' if future_pred == 1 else 'SELL'\n",
        "direction = 1 if future_pred == 1 else -1\n",
        "\n",
        "print(f\"\\nPredicción para {next_day.strftime('%Y-%m-%d')} (basada en datos hasta {df.index[-1].strftime('%Y-%m-%d')}):\")\n",
        "print(f\"Papel: {symbol}\")\n",
        "print(f\"Precio actual: [{last_close:.4f}]\")\n",
        "print(f\"Precio esperado para el siguiente día (distribución log-normal): [{expected_price:.4f}]\")\n",
        "print(f\"Probabilidad de que el precio predicho sea correcto: [{price_prob:.4f}]\")\n",
        "print(f\"Corte: {df.index[-1]}\")\n",
        "print(f\"\\nPredicción para {next_day.strftime('%Y-%m-%d')}: {'Alcista' if future_pred == 1 else 'Bajista'} (Probabilidad Alcista: {future_pred_prob:.2%})\")\n",
        "print(f\"Pronóstico de dirección del activo (1: subida, -1: bajada): {direction}\")\n",
        "print(f\"Acción sugerida por la estrategia de trading: {action}\")"
      ],
      "metadata": {
        "id": "OReUSMZapafA",
        "outputId": "c6516afb-facd-46e5-de79-0a5c4b27bf24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultiIndex detectado en columnas. Aplanando...\n",
            "Columnas asignadas después de aplanamiento: ['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']\n",
            "Últimas filas antes de corrección:              Open   High    Low  Close  Volume  Adj Close\n",
            "Date                                                     \n",
            "2025-07-14  729.0  729.0  734.0  685.0   687.0    1188785\n",
            "2025-07-15  717.0  717.0  750.0  703.0   735.0    1002659\n",
            "2025-07-16  706.0  706.0  725.0  694.0   725.0     500054\n",
            "2025-07-17  699.0  699.0  715.0  696.0   708.0     858480\n",
            "2025-07-18  723.0  723.0  726.0  686.0   702.0     567030\n",
            "Últimas filas después de corrección:              Open   High    Low  Close   Volume  Adj Close\n",
            "Date                                                      \n",
            "2025-07-14  687.0  734.0  685.0  729.0  1188785      729.0\n",
            "2025-07-15  735.0  750.0  703.0  717.0  1002659      717.0\n",
            "2025-07-16  725.0  725.0  694.0  706.0   500054      706.0\n",
            "2025-07-17  708.0  715.0  696.0  699.0   858480      699.0\n",
            "2025-07-18  702.0  726.0  686.0  723.0   567030      723.0\n",
            "Columnas del DataFrame después de descargar y corregir:\n",
            "Index(['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close'], dtype='object')\n",
            "Últimas filas del DataFrame antes de crear features:\n",
            "             Open   High    Low  Close   Volume  Adj Close\n",
            "Date                                                      \n",
            "2025-07-14  687.0  734.0  685.0  729.0  1188785      729.0\n",
            "2025-07-15  735.0  750.0  703.0  717.0  1002659      717.0\n",
            "2025-07-16  725.0  725.0  694.0  706.0   500054      706.0\n",
            "2025-07-17  708.0  715.0  696.0  699.0   858480      699.0\n",
            "2025-07-18  702.0  726.0  686.0  723.0   567030      723.0\n",
            "\n",
            "Últimas filas del DataFrame después de crear features:\n",
            "             Open   High    Low  Close   Volume  Adj Close  close_lag_1  \\\n",
            "Date                                                                      \n",
            "2025-07-14  687.0  734.0  685.0  729.0  1188785      729.0        683.0   \n",
            "2025-07-15  735.0  750.0  703.0  717.0  1002659      717.0        729.0   \n",
            "2025-07-16  725.0  725.0  694.0  706.0   500054      706.0        717.0   \n",
            "2025-07-17  708.0  715.0  696.0  699.0   858480      699.0        706.0   \n",
            "2025-07-18  702.0  726.0  686.0  723.0   567030      723.0        699.0   \n",
            "\n",
            "            close_lag_2  close_lag_3  close_lag_4  ...  EWO_Signal  \\\n",
            "Date                                               ...               \n",
            "2025-07-14        683.0        698.0        713.0  ...   -0.022748   \n",
            "2025-07-15        683.0        683.0        698.0  ...    0.218820   \n",
            "2025-07-16        729.0        683.0        683.0  ...    0.314669   \n",
            "2025-07-17        717.0        729.0        683.0  ...    0.242494   \n",
            "2025-07-18        706.0        717.0        729.0  ...    0.422993   \n",
            "\n",
            "            EWO_Histogram   SMA5     SMA13     SMA26   SMA50    SMA200  \\\n",
            "Date                                                                     \n",
            "2025-07-14       0.326609  701.2  706.1538  684.4231  702.84  813.7878   \n",
            "2025-07-15       0.483137  702.0  710.8462  684.8846  703.60  812.7538   \n",
            "2025-07-16       0.191698  703.6  712.0769  684.8077  704.54  811.6996   \n",
            "2025-07-17      -0.144350  706.8  711.5385  684.5000  705.34  810.6953   \n",
            "2025-07-18       0.360997  714.8  711.8462  686.2308  706.22  809.8659   \n",
            "\n",
            "            Volatility  Label    Return  \n",
            "Date                                     \n",
            "2025-07-14   43.524313      1  0.065179  \n",
            "2025-07-15   44.245131      0 -0.016598  \n",
            "2025-07-16   44.147957      0 -0.015461  \n",
            "2025-07-17   42.431214      0 -0.009964  \n",
            "2025-07-18   40.609825      1  0.033758  \n",
            "\n",
            "[5 rows x 33 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mejores hiperparámetros: {'subsample': 0.6, 'n_estimators': 900, 'max_depth': 5, 'learning_rate': 0.2, 'gamma': 0.1, 'colsample_bytree': 1.0}\n",
            "Mejor umbral: 0.2\n",
            "Escalando datos hasta 2024-12-30 00:00:00 para predecir 2025-01-02 00:00:00\n",
            "Escalando datos hasta 2025-01-02 00:00:00 para predecir 2025-01-03 00:00:00\n",
            "Escalando datos hasta 2025-01-03 00:00:00 para predecir 2025-01-06 00:00:00\n",
            "Escalando datos hasta 2025-01-06 00:00:00 para predecir 2025-01-07 00:00:00\n",
            "Escalando datos hasta 2025-01-07 00:00:00 para predecir 2025-01-08 00:00:00\n",
            "Escalando datos hasta 2025-01-08 00:00:00 para predecir 2025-01-09 00:00:00\n",
            "Escalando datos hasta 2025-01-09 00:00:00 para predecir 2025-01-10 00:00:00\n",
            "Escalando datos hasta 2025-01-10 00:00:00 para predecir 2025-01-13 00:00:00\n",
            "Escalando datos hasta 2025-01-13 00:00:00 para predecir 2025-01-14 00:00:00\n",
            "Escalando datos hasta 2025-01-14 00:00:00 para predecir 2025-01-15 00:00:00\n",
            "Escalando datos hasta 2025-01-15 00:00:00 para predecir 2025-01-16 00:00:00\n",
            "Escalando datos hasta 2025-01-16 00:00:00 para predecir 2025-01-17 00:00:00\n",
            "Escalando datos hasta 2025-01-17 00:00:00 para predecir 2025-01-20 00:00:00\n",
            "Escalando datos hasta 2025-01-20 00:00:00 para predecir 2025-01-21 00:00:00\n",
            "Escalando datos hasta 2025-01-21 00:00:00 para predecir 2025-01-22 00:00:00\n",
            "Escalando datos hasta 2025-01-22 00:00:00 para predecir 2025-01-23 00:00:00\n",
            "Escalando datos hasta 2025-01-23 00:00:00 para predecir 2025-01-24 00:00:00\n",
            "Escalando datos hasta 2025-01-24 00:00:00 para predecir 2025-01-27 00:00:00\n",
            "Escalando datos hasta 2025-01-27 00:00:00 para predecir 2025-01-28 00:00:00\n",
            "Escalando datos hasta 2025-01-28 00:00:00 para predecir 2025-01-29 00:00:00\n",
            "Escalando datos hasta 2025-01-29 00:00:00 para predecir 2025-01-30 00:00:00\n",
            "Escalando datos hasta 2025-01-30 00:00:00 para predecir 2025-01-31 00:00:00\n",
            "Escalando datos hasta 2025-01-31 00:00:00 para predecir 2025-02-03 00:00:00\n",
            "Escalando datos hasta 2025-02-03 00:00:00 para predecir 2025-02-04 00:00:00\n",
            "Escalando datos hasta 2025-02-04 00:00:00 para predecir 2025-02-05 00:00:00\n",
            "Escalando datos hasta 2025-02-05 00:00:00 para predecir 2025-02-06 00:00:00\n",
            "Escalando datos hasta 2025-02-06 00:00:00 para predecir 2025-02-07 00:00:00\n",
            "Escalando datos hasta 2025-02-07 00:00:00 para predecir 2025-02-10 00:00:00\n",
            "Escalando datos hasta 2025-02-10 00:00:00 para predecir 2025-02-11 00:00:00\n",
            "Escalando datos hasta 2025-02-11 00:00:00 para predecir 2025-02-12 00:00:00\n",
            "Escalando datos hasta 2025-02-12 00:00:00 para predecir 2025-02-13 00:00:00\n",
            "Escalando datos hasta 2025-02-13 00:00:00 para predecir 2025-02-14 00:00:00\n",
            "Escalando datos hasta 2025-02-14 00:00:00 para predecir 2025-02-17 00:00:00\n",
            "Escalando datos hasta 2025-02-17 00:00:00 para predecir 2025-02-18 00:00:00\n",
            "Escalando datos hasta 2025-02-18 00:00:00 para predecir 2025-02-19 00:00:00\n",
            "Escalando datos hasta 2025-02-19 00:00:00 para predecir 2025-02-20 00:00:00\n",
            "Escalando datos hasta 2025-02-20 00:00:00 para predecir 2025-02-21 00:00:00\n",
            "Escalando datos hasta 2025-02-21 00:00:00 para predecir 2025-02-24 00:00:00\n",
            "Escalando datos hasta 2025-02-24 00:00:00 para predecir 2025-02-25 00:00:00\n",
            "Escalando datos hasta 2025-02-25 00:00:00 para predecir 2025-02-26 00:00:00\n",
            "Escalando datos hasta 2025-02-26 00:00:00 para predecir 2025-02-27 00:00:00\n",
            "Escalando datos hasta 2025-02-27 00:00:00 para predecir 2025-02-28 00:00:00\n",
            "Escalando datos hasta 2025-02-28 00:00:00 para predecir 2025-03-05 00:00:00\n",
            "Escalando datos hasta 2025-03-05 00:00:00 para predecir 2025-03-06 00:00:00\n",
            "Escalando datos hasta 2025-03-06 00:00:00 para predecir 2025-03-07 00:00:00\n",
            "Escalando datos hasta 2025-03-07 00:00:00 para predecir 2025-03-10 00:00:00\n",
            "Escalando datos hasta 2025-03-10 00:00:00 para predecir 2025-03-11 00:00:00\n",
            "Escalando datos hasta 2025-03-11 00:00:00 para predecir 2025-03-12 00:00:00\n",
            "Escalando datos hasta 2025-03-12 00:00:00 para predecir 2025-03-13 00:00:00\n",
            "Escalando datos hasta 2025-03-13 00:00:00 para predecir 2025-03-14 00:00:00\n",
            "Escalando datos hasta 2025-03-14 00:00:00 para predecir 2025-03-17 00:00:00\n",
            "Escalando datos hasta 2025-03-17 00:00:00 para predecir 2025-03-18 00:00:00\n",
            "Escalando datos hasta 2025-03-18 00:00:00 para predecir 2025-03-19 00:00:00\n",
            "Escalando datos hasta 2025-03-19 00:00:00 para predecir 2025-03-20 00:00:00\n",
            "Escalando datos hasta 2025-03-20 00:00:00 para predecir 2025-03-21 00:00:00\n",
            "Escalando datos hasta 2025-03-21 00:00:00 para predecir 2025-03-25 00:00:00\n",
            "Escalando datos hasta 2025-03-25 00:00:00 para predecir 2025-03-26 00:00:00\n",
            "Escalando datos hasta 2025-03-26 00:00:00 para predecir 2025-03-27 00:00:00\n",
            "Escalando datos hasta 2025-03-27 00:00:00 para predecir 2025-03-28 00:00:00\n",
            "Escalando datos hasta 2025-03-28 00:00:00 para predecir 2025-03-31 00:00:00\n",
            "Escalando datos hasta 2025-03-31 00:00:00 para predecir 2025-04-01 00:00:00\n",
            "Escalando datos hasta 2025-04-01 00:00:00 para predecir 2025-04-03 00:00:00\n",
            "Escalando datos hasta 2025-04-03 00:00:00 para predecir 2025-04-04 00:00:00\n",
            "Escalando datos hasta 2025-04-04 00:00:00 para predecir 2025-04-07 00:00:00\n",
            "Escalando datos hasta 2025-04-07 00:00:00 para predecir 2025-04-08 00:00:00\n",
            "Escalando datos hasta 2025-04-08 00:00:00 para predecir 2025-04-09 00:00:00\n",
            "Escalando datos hasta 2025-04-09 00:00:00 para predecir 2025-04-10 00:00:00\n",
            "Escalando datos hasta 2025-04-10 00:00:00 para predecir 2025-04-11 00:00:00\n",
            "Escalando datos hasta 2025-04-11 00:00:00 para predecir 2025-04-14 00:00:00\n",
            "Escalando datos hasta 2025-04-14 00:00:00 para predecir 2025-04-15 00:00:00\n",
            "Escalando datos hasta 2025-04-15 00:00:00 para predecir 2025-04-16 00:00:00\n",
            "Escalando datos hasta 2025-04-16 00:00:00 para predecir 2025-04-21 00:00:00\n",
            "Escalando datos hasta 2025-04-21 00:00:00 para predecir 2025-04-22 00:00:00\n",
            "Escalando datos hasta 2025-04-22 00:00:00 para predecir 2025-04-23 00:00:00\n",
            "Escalando datos hasta 2025-04-23 00:00:00 para predecir 2025-04-24 00:00:00\n",
            "Escalando datos hasta 2025-04-24 00:00:00 para predecir 2025-04-25 00:00:00\n",
            "Escalando datos hasta 2025-04-25 00:00:00 para predecir 2025-04-28 00:00:00\n",
            "Escalando datos hasta 2025-04-28 00:00:00 para predecir 2025-04-29 00:00:00\n",
            "Escalando datos hasta 2025-04-29 00:00:00 para predecir 2025-04-30 00:00:00\n",
            "Escalando datos hasta 2025-04-30 00:00:00 para predecir 2025-05-05 00:00:00\n",
            "Escalando datos hasta 2025-05-05 00:00:00 para predecir 2025-05-06 00:00:00\n",
            "Escalando datos hasta 2025-05-06 00:00:00 para predecir 2025-05-07 00:00:00\n",
            "Escalando datos hasta 2025-05-07 00:00:00 para predecir 2025-05-08 00:00:00\n",
            "Escalando datos hasta 2025-05-08 00:00:00 para predecir 2025-05-09 00:00:00\n",
            "Escalando datos hasta 2025-05-09 00:00:00 para predecir 2025-05-12 00:00:00\n",
            "Escalando datos hasta 2025-05-12 00:00:00 para predecir 2025-05-13 00:00:00\n",
            "Escalando datos hasta 2025-05-13 00:00:00 para predecir 2025-05-14 00:00:00\n",
            "Escalando datos hasta 2025-05-14 00:00:00 para predecir 2025-05-15 00:00:00\n",
            "Escalando datos hasta 2025-05-15 00:00:00 para predecir 2025-05-16 00:00:00\n",
            "Escalando datos hasta 2025-05-16 00:00:00 para predecir 2025-05-19 00:00:00\n",
            "Escalando datos hasta 2025-05-19 00:00:00 para predecir 2025-05-20 00:00:00\n",
            "Escalando datos hasta 2025-05-20 00:00:00 para predecir 2025-05-21 00:00:00\n",
            "Escalando datos hasta 2025-05-21 00:00:00 para predecir 2025-05-22 00:00:00\n",
            "Escalando datos hasta 2025-05-22 00:00:00 para predecir 2025-05-23 00:00:00\n",
            "Escalando datos hasta 2025-05-23 00:00:00 para predecir 2025-05-26 00:00:00\n",
            "Escalando datos hasta 2025-05-26 00:00:00 para predecir 2025-05-27 00:00:00\n",
            "Escalando datos hasta 2025-05-27 00:00:00 para predecir 2025-05-28 00:00:00\n",
            "Escalando datos hasta 2025-05-28 00:00:00 para predecir 2025-05-29 00:00:00\n",
            "Escalando datos hasta 2025-05-29 00:00:00 para predecir 2025-05-30 00:00:00\n",
            "Escalando datos hasta 2025-05-30 00:00:00 para predecir 2025-06-02 00:00:00\n",
            "Escalando datos hasta 2025-06-02 00:00:00 para predecir 2025-06-03 00:00:00\n",
            "Escalando datos hasta 2025-06-03 00:00:00 para predecir 2025-06-04 00:00:00\n",
            "Escalando datos hasta 2025-06-04 00:00:00 para predecir 2025-06-05 00:00:00\n",
            "Escalando datos hasta 2025-06-05 00:00:00 para predecir 2025-06-06 00:00:00\n",
            "Escalando datos hasta 2025-06-06 00:00:00 para predecir 2025-06-09 00:00:00\n",
            "Escalando datos hasta 2025-06-09 00:00:00 para predecir 2025-06-10 00:00:00\n",
            "Escalando datos hasta 2025-06-10 00:00:00 para predecir 2025-06-11 00:00:00\n",
            "Escalando datos hasta 2025-06-11 00:00:00 para predecir 2025-06-12 00:00:00\n",
            "Escalando datos hasta 2025-06-12 00:00:00 para predecir 2025-06-13 00:00:00\n",
            "Escalando datos hasta 2025-06-13 00:00:00 para predecir 2025-06-17 00:00:00\n",
            "Escalando datos hasta 2025-06-17 00:00:00 para predecir 2025-06-18 00:00:00\n",
            "Escalando datos hasta 2025-06-18 00:00:00 para predecir 2025-06-19 00:00:00\n",
            "Escalando datos hasta 2025-06-19 00:00:00 para predecir 2025-06-23 00:00:00\n",
            "Escalando datos hasta 2025-06-23 00:00:00 para predecir 2025-06-24 00:00:00\n",
            "Escalando datos hasta 2025-06-24 00:00:00 para predecir 2025-06-25 00:00:00\n",
            "Escalando datos hasta 2025-06-25 00:00:00 para predecir 2025-06-26 00:00:00\n",
            "Escalando datos hasta 2025-06-26 00:00:00 para predecir 2025-06-27 00:00:00\n",
            "Escalando datos hasta 2025-06-27 00:00:00 para predecir 2025-06-30 00:00:00\n",
            "Escalando datos hasta 2025-06-30 00:00:00 para predecir 2025-07-01 00:00:00\n",
            "Escalando datos hasta 2025-07-01 00:00:00 para predecir 2025-07-02 00:00:00\n",
            "Escalando datos hasta 2025-07-02 00:00:00 para predecir 2025-07-03 00:00:00\n",
            "Escalando datos hasta 2025-07-03 00:00:00 para predecir 2025-07-04 00:00:00\n",
            "Escalando datos hasta 2025-07-04 00:00:00 para predecir 2025-07-07 00:00:00\n",
            "Escalando datos hasta 2025-07-07 00:00:00 para predecir 2025-07-08 00:00:00\n",
            "Escalando datos hasta 2025-07-08 00:00:00 para predecir 2025-07-10 00:00:00\n",
            "Escalando datos hasta 2025-07-10 00:00:00 para predecir 2025-07-11 00:00:00\n",
            "Escalando datos hasta 2025-07-11 00:00:00 para predecir 2025-07-14 00:00:00\n",
            "Escalando datos hasta 2025-07-14 00:00:00 para predecir 2025-07-15 00:00:00\n",
            "Escalando datos hasta 2025-07-15 00:00:00 para predecir 2025-07-16 00:00:00\n",
            "Escalando datos hasta 2025-07-16 00:00:00 para predecir 2025-07-17 00:00:00\n",
            "Escalando datos hasta 2025-07-17 00:00:00 para predecir 2025-07-18 00:00:00\n",
            "\n",
            "Resultados del backtesting (hasta 2025-07-20 ):\n",
            "Nota: 'Fecha Predicción' es la fecha predicha; 'Fecha Datos' es la fecha de los datos usados.\n",
            "                 Fecha Datos Predicción Resultado Real  Precio Cierre  \\\n",
            "Fecha Predicción                                                        \n",
            "2025-01-02        2024-12-30    Alcista        Alcista          893.0   \n",
            "2025-01-03        2025-01-02    Alcista        Bajista          890.0   \n",
            "2025-01-06        2025-01-03    Bajista        Bajista          874.0   \n",
            "2025-01-07        2025-01-06    Bajista        Bajista          874.0   \n",
            "2025-01-08        2025-01-07    Alcista        Alcista          884.0   \n",
            "...                      ...        ...            ...            ...   \n",
            "2025-07-14        2025-07-11    Alcista        Alcista          729.0   \n",
            "2025-07-15        2025-07-14    Alcista        Bajista          717.0   \n",
            "2025-07-16        2025-07-15    Bajista        Bajista          706.0   \n",
            "2025-07-17        2025-07-16    Bajista        Bajista          699.0   \n",
            "2025-07-18        2025-07-17    Alcista        Alcista          723.0   \n",
            "\n",
            "                  Probabilidad Alcista Correcta  \n",
            "Fecha Predicción                                 \n",
            "2025-01-02                    0.736145       Sí  \n",
            "2025-01-03                    0.385522       No  \n",
            "2025-01-06                    0.000340       Sí  \n",
            "2025-01-07                    0.113646       Sí  \n",
            "2025-01-08                    0.997898       Sí  \n",
            "...                                ...      ...  \n",
            "2025-07-14                    1.000000       Sí  \n",
            "2025-07-15                    0.448188       No  \n",
            "2025-07-16                    0.009720       Sí  \n",
            "2025-07-17                    0.031067       Sí  \n",
            "2025-07-18                    0.999864       Sí  \n",
            "\n",
            "[131 rows x 6 columns]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c67a084c-8e35-49a5-9cf6-b935f807a366\", \"backtesting_results_ALUA.BA_2025-07-20.csv\", 7852)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Archivo 'backtesting_results_ALUA.BA_2025-07-20.csv' generado y descargado.\n",
            "Tamaño de y_train_full: 4969\n",
            "Tamaño de y_test: 519\n",
            "Distribución de clases en y_train_full:\n",
            "Label\n",
            "0    2761\n",
            "1    2208\n",
            "Name: count, dtype: int64\n",
            "Distribución de clases en y_test:\n",
            "Label\n",
            "0    266\n",
            "1    253\n",
            "Name: count, dtype: int64\n",
            "\n",
            "ROC-AUC en el conjunto de entrenamiento: 1.0000\n",
            "1.0\n",
            "ROC-AUC en el conjunto de prueba: 0.9433\n",
            "\n",
            "Matriz de Confusión (prueba):\n",
            "[[237  29]\n",
            " [ 40 213]]\n",
            "\n",
            "Informe de Clasificación (prueba):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.89      0.87       266\n",
            "           1       0.88      0.84      0.86       253\n",
            "\n",
            "    accuracy                           0.87       519\n",
            "   macro avg       0.87      0.87      0.87       519\n",
            "weighted avg       0.87      0.87      0.87       519\n",
            "\n",
            "\n",
            "Predicción para 2025-07-21 (basada en datos hasta 2025-07-18):\n",
            "Papel: ALUA.BA\n",
            "Precio actual: [723.0000]\n",
            "Precio esperado para el siguiente día (distribución log-normal): [724.0232]\n",
            "Probabilidad de que el precio predicho sea correcto: [0.8433]\n",
            "Corte: 2025-07-18 00:00:00\n",
            "\n",
            "Predicción para 2025-07-21: Alcista (Probabilidad Alcista: 84.33%)\n",
            "Pronóstico de dirección del activo (1: subida, -1: bajada): 1\n",
            "Acción sugerida por la estrategia de trading: BUY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "COMIENZA EL UMBRAL"
      ],
      "metadata": {
        "id": "xjiSoEl0ijkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import datetime as dt\n",
        "\n",
        "# Descargar datos (si no lo hiciste aún)\n",
        "end_date = dt.datetime(2025, 7, 20)\n",
        "df = yf.download(\"COME.BA\", start=dt.datetime(2001, 1, 1), end=end_date)\n",
        "\n",
        "# Definir umbral (por ejemplo, 2%)\n",
        "umbral = 0.02\n",
        "\n",
        "# Calcular la diferencia relativa y etiquetar\n",
        "df['Label'] = ((df['High'] - df['Open']) / df['Open'] > umbral).astype(int) * 2 - 1\n",
        "\n",
        "# Contar etiquetas y calcular porcentajes\n",
        "label_counts = df['Label'].value_counts()\n",
        "total_dias = len(df)\n",
        "percentages = (label_counts / total_dias) * 100\n",
        "\n",
        "# Imprimir resultados\n",
        "print(\"Conteo de etiquetas:\")\n",
        "print(label_counts)\n",
        "print(\"\\nPorcentajes de cada clase (%):\")\n",
        "print(percentages.round(2))\n",
        "\n",
        "# Opcional: Ver los primeros días etiquetados\n",
        "print(\"\\nPrimeros días etiquetados:\")\n",
        "print(df[['Open', 'High', 'Label']].tail(20))\n",
        "print(\"mambral\")\n",
        "umbral = df['High'].sub(df['Open']).div(df['Open']).quantile(0.75)\n",
        "print(umbral)\n",
        "df['Label'] = ((df['High'] - df['Open']) / df['Open'] > umbral).astype(int) * 2 - 1\n",
        "print(df['Label'].value_counts(normalize=True) * 100)\n",
        "\n",
        "# Contar etiquetas y calcular porcentajes\n",
        "label_counts = df['Label'].value_counts()\n",
        "total_dias = len(df)\n",
        "percentages = (label_counts / total_dias) * 100\n",
        "\n",
        "# Imprimir resultados\n",
        "print(\"Conteo de etiquetas:\")\n",
        "print(label_counts)\n",
        "print(\"\\nPorcentajes de cada clase (%):\")\n",
        "print(percentages.round(2))"
      ],
      "metadata": {
        "id": "-ItKP7EgLl9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import datetime as dt\n",
        "\n",
        "# Descargar datos\n",
        "end_date = dt.datetime(2025, 7, 17)\n",
        "df = yf.download(\"GGAL\", start=dt.datetime(2001, 1, 1), end=end_date)\n",
        "\n",
        "# Aplanar el MultiIndex a columnas simples\n",
        "df.columns = df.columns.map(lambda x: x[0])\n",
        "\n",
        "# Limpiar datos\n",
        "df = df.dropna(subset=['Open', 'High'])\n",
        "\n",
        "# Calcular umbral dinámico\n",
        "differences = (df['High'] - df['Open']) / df['Open']\n",
        "umbral = differences.quantile(0.6).item()\n",
        "print(f\"Umbral calculado: {umbral:.4f}\")\n",
        "\n",
        "# Calcular etiqueta sin desfase (para verificar)\n",
        "df['Label_raw'] = ((df['High'] - df['Open']) / df['Open'] > umbral).astype(int) * 2 - 1\n",
        "\n",
        "# Desplazar la etiqueta un día hacia atrás (target del día siguiente)\n",
        "df['Label'] = df['Label_raw'].shift(-1)\n",
        "\n",
        "# Eliminar la última fila (no tiene etiqueta para predecir)\n",
        "df = df.dropna(subset=['Label'])\n",
        "\n",
        "# Contar etiquetas y calcular porcentajes\n",
        "label_counts = df['Label'].value_counts()\n",
        "total_dias = len(df)\n",
        "percentages = (label_counts / total_dias) * 100\n",
        "\n",
        "print(\"\\nConteo de etiquetas (desfasadas):\")\n",
        "print(label_counts)\n",
        "print(\"\\nPorcentajes de cada clase (%):\")\n",
        "print(percentages.round(2))\n",
        "\n",
        "# Opcional: Ver los primeros días etiquetados\n",
        "print(\"\\nPrimeros días etiquetados (features del día anterior, label del día siguiente):\")\n",
        "print(df[['Open', 'High', 'Label']].tail(22))"
      ],
      "metadata": {
        "id": "BYoBb-mnRDDT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}