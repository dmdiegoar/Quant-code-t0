{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmdiegoar/Quant-code-t0/blob/main/XGB_Sentido.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LA MEJORA DE GROK"
      ],
      "metadata": {
        "id": "oqaw_AHkEbnN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "mas listo que ayer"
      ],
      "metadata": {
        "id": "6x9IIbsCpVUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, f1_score\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "from google.colab import files  # Para descargar el CSV en Colab\n",
        "from scipy import stats\n",
        "\n",
        "# Funciones de features\n",
        "def add_lagged_price_features(df):\n",
        "    for lag in range(1, 6):\n",
        "        df[f'close_lag_{lag}'] = df['Close'].shift(lag)\n",
        "    return df\n",
        "\n",
        "def calculate_RSI(series, period=7):\n",
        "    delta = series.diff(1)\n",
        "    gain = delta.where(delta > 0, 0).rolling(window=period).mean()\n",
        "    loss = -delta.where(delta < 0, 0).rolling(window=period).mean()\n",
        "    rs = gain / loss\n",
        "    return 100 - (100 / (1 + rs))\n",
        "\n",
        "def calculate_ROC(series, period=5):\n",
        "    return ((series - series.shift(period)) / series.shift(period)) * 100\n",
        "\n",
        "def calculate_PPO(series, fast_period=5, slow_period=9, signal_period=5):\n",
        "    ema_fast = series.ewm(span=fast_period, adjust=False).mean()\n",
        "    ema_slow = series.ewm(span=slow_period, adjust=False).mean()\n",
        "    ppo = (ema_fast - ema_slow) / ema_slow * 100\n",
        "    signal_line = ppo.ewm(span=signal_period, adjust=False).mean()\n",
        "    histogram = ppo - signal_line\n",
        "    return ppo, signal_line, histogram\n",
        "\n",
        "def calculate_volatility(series, window=20):\n",
        "    return series.rolling(window).std().round(6)\n",
        "\n",
        "def calculate_sma(series, period=5):\n",
        "    return series.rolling(window=period).mean().round(4)\n",
        "\n",
        "def calculate_sma21(series, period=21):\n",
        "    return series.rolling(window=period).mean().round(4)\n",
        "\n",
        "def calculate_sma30(series, period=30):\n",
        "    return series.rolling(window=period).mean().round(4)\n",
        "\n",
        "def calculate_earnings_season(df):\n",
        "    df['Is_Earnings_Season'] = df.index.month.isin([1, 4, 7, 10])\n",
        "    return df\n",
        "\n",
        "def calculate_christmas_rally(df):\n",
        "    df['Is_Christmas_Rally'] = df.index.month.isin([11, 12])\n",
        "    return df\n",
        "\n",
        "def create_features(df):\n",
        "    df = add_lagged_price_features(df)\n",
        "    df['Pct_change'] = df['Close'].pct_change()\n",
        "    for lag in range(1, 6):\n",
        "        df[f'lag_{lag}'] = df['Pct_change'].shift(lag)\n",
        "    df['RSI'] = calculate_RSI(df['Close'])\n",
        "    df['ROC'] = calculate_ROC(df['Close'])\n",
        "    df['PPO'], df['PPO_Signal'], df['PPO_Histogram'] = calculate_PPO(df['Close'])\n",
        "    df['SMA'] = calculate_sma(df['Close'])\n",
        "    df['SMA21'] = calculate_sma21(df['Close'])\n",
        "    df['SMA30'] = calculate_sma30(df['Close'])\n",
        "    df['Volatility'] = calculate_volatility(df['Close'])\n",
        "    df['Label'] = (df['Pct_change'] > 0).astype(int)\n",
        "    df['Return'] = np.log(df['Close'] / df['Close'].shift())\n",
        "    df = calculate_earnings_season(df)\n",
        "    df = calculate_christmas_rally(df)\n",
        "    df.dropna(inplace=True)\n",
        "    return df\n",
        "\n",
        "# Definir fecha de corte manualmente (cambiar diariamente)\n",
        "end_date = dt.datetime(2025, 7, 18)  # Ejemplo: cambiar a 2025-07-18 mañana\n",
        "symbol=\"BBAR.BA\"\n",
        "#symbol=\"COME.BA\"\n",
        "# Fechas dinámicas\n",
        "start_date = dt.datetime(2001, 1, 1)  # Inicio fijo\n",
        "train_end = end_date - pd.Timedelta(days=780)  # 6 meses antes de end_date (ajustable)\n",
        "next_day = end_date + pd.Timedelta(days=1)  # Predicción para el día siguiente\n",
        "backtest_start = end_date - pd.Timedelta(days=200)  # Inicio del backtesting 6 meses antes de end_date (ajustable)\n",
        "\n",
        "# Descargar datos\n",
        "df = yf.download(symbol, start=start_date, end=end_date, auto_adjust=False)\n",
        "\n",
        "# Verificar datos\n",
        "if df.empty:\n",
        "    raise ValueError(\"No se pudieron descargar datos. Verifica el símbolo, las fechas o la conexión.\")\n",
        "\n",
        "# Aplanar MultiIndex si existe\n",
        "if isinstance(df.columns, pd.MultiIndex):\n",
        "    print(\"MultiIndex detectado en columnas. Aplanando...\")\n",
        "    df.columns = ['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']\n",
        "    print(\"Columnas asignadas después de aplanamiento:\", df.columns.tolist())\n",
        "    print(\"Últimas filas antes de corrección:\", df.tail())\n",
        "\n",
        "    # Corregir el orden de las columnas según tu mapeo\n",
        "    columns = df.columns.tolist()\n",
        "    df = df[[columns[4], columns[2], columns[3], columns[0], columns[5], columns[1]]]  # Reordenar\n",
        "    df.columns = ['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']  # Asignar nombres correctos\n",
        "    print(\"Últimas filas después de corrección:\", df.tail())\n",
        "\n",
        "if isinstance(df.index, pd.MultiIndex):\n",
        "    print(\"MultiIndex detectado en índice. Seleccionando ticker...\")\n",
        "    df = df.xs(symbol, level='Ticker', axis=0)\n",
        "df.index = pd.to_datetime(df.index)  # Asegurar índice datetime\n",
        "if not df.index.is_unique:\n",
        "    print(\"Advertencia: Índice con fechas duplicadas. Eliminando duplicados...\")\n",
        "    df = df[~df.index.duplicated(keep='first')]\n",
        "\n",
        "# Verificar columnas\n",
        "print(\"Columnas del DataFrame después de descargar y corregir:\")\n",
        "print(df.columns)\n",
        "\n",
        "df['Open']= df['Open'].round(2)\n",
        "df['High']= df['High'].round(2)\n",
        "df['Low']= df['Low'].round(2)\n",
        "df['Close']= df['Close'].round(2)\n",
        "df['Adj Close']= df['Adj Close'].round(2)\n",
        "\n",
        "print(\"Últimas filas del DataFrame antes de crear features:\")\n",
        "print(df.tail())\n",
        "\n",
        "# Crear features\n",
        "df = create_features(df)\n",
        "\n",
        "# Verificar datos después de crear features\n",
        "print(\"\\nÚltimas filas del DataFrame después de crear features:\")\n",
        "print(df.tail())\n",
        "\n",
        "# Seleccionar features\n",
        "features = ['RSI', 'ROC', 'PPO', 'PPO_Signal', 'PPO_Histogram', 'Volatility', 'SMA', 'SMA21', 'SMA30',\n",
        "            'Is_Christmas_Rally', 'Is_Earnings_Season'] + [f'lag_{i}' for i in range(1, 6)] + \\\n",
        "           [f'close_lag_{i}' for i in range(1, 6)]\n",
        "X = df[features]\n",
        "y = df['Label']\n",
        "\n",
        "# Dividir datos en entrenamiento y prueba\n",
        "X_train_full = X[df.index <= train_end]\n",
        "y_train_full = y[df.index <= train_end]\n",
        "X_test = X[(df.index > train_end) & (df.index <= end_date)]  # Hasta end_date\n",
        "y_test = y[(df.index > train_end) & (df.index <= end_date)]\n",
        "\n",
        "# Optimizar hiperparámetros con RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7, 9],\n",
        "    'n_estimators': [100, 500, 900],\n",
        "    'subsample': [0.6, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "    'gamma': [0, 0.1, 0.2]\n",
        "}\n",
        "xgb = XGBClassifier(objective='binary:logistic', random_state=42)\n",
        "random_search = RandomizedSearchCV(xgb, param_distributions=param_dist, n_iter=20, cv=5, scoring='f1', n_jobs=-1, random_state=42)\n",
        "random_search.fit(X_train_full, y_train_full)\n",
        "print(\"Mejores hiperparámetros:\", random_search.best_params_)\n",
        "\n",
        "# Usar el mejor modelo\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Optimizar el umbral\n",
        "y_train_prob = best_model.predict_proba(X_train_full)[:, 1]\n",
        "thresholds = np.arange(0.1, 0.9, 0.1)\n",
        "best_threshold = 0.5\n",
        "best_f1 = 0\n",
        "for threshold in thresholds:\n",
        "    y_pred_threshold = (y_train_prob >= threshold).astype(int)\n",
        "    f1 = f1_score(y_train_full, y_pred_threshold)\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_threshold = threshold\n",
        "print(\"Mejor umbral:\", best_threshold)\n",
        "\n",
        "#best_threshold=0.15\n",
        "\n",
        "\n",
        "# Backtesting con escalado dinámico para evitar data leak\n",
        "dates = df[(df.index >= backtest_start) & (df.index <= end_date)].index\n",
        "results = []\n",
        "\n",
        "for test_date in dates:\n",
        "    train_data = df[df.index < test_date].copy()\n",
        "    if train_data.empty or train_data['Label'].isna().all():\n",
        "        continue\n",
        "    train_data = train_data.dropna()\n",
        "\n",
        "    X_train_loop = train_data[features]\n",
        "    y_train_loop = train_data['Label']\n",
        "\n",
        "    scaler = StandardScaler()  # Reinicio dinámico del escalador\n",
        "    X_train_scaled = scaler.fit_transform(X_train_loop)\n",
        "    print(f\"Escalando datos hasta {train_data.index[-1]} para predecir {test_date}\")\n",
        "\n",
        "    best_model.fit(X_train_scaled, y_train_loop)\n",
        "\n",
        "    if test_date in df.index:\n",
        "        test_row = df.loc[[test_date]][features]\n",
        "        if test_row.empty:\n",
        "            continue\n",
        "        test_row_scaled = scaler.transform(test_row)\n",
        "\n",
        "        prediction_prob = best_model.predict_proba(test_row_scaled)[0][1]\n",
        "        prediction = 1 if prediction_prob >= best_threshold else 0\n",
        "\n",
        "        real_direction = df.loc[test_date, 'Label'] if pd.notna(df.loc[test_date, 'Label']) else None\n",
        "        close_price = df.loc[test_date, 'Close']\n",
        "\n",
        "        is_correct = int(prediction == real_direction) if real_direction is not None else None\n",
        "        data_date = df.index[df.index < test_date][-1] if not df[df.index < test_date].empty else None\n",
        "\n",
        "        results.append({\n",
        "            'Fecha Predicción': test_date,\n",
        "            'Fecha Datos': data_date,\n",
        "            'Predicción': 'Alcista' if prediction == 1 else 'Bajista',\n",
        "            'Resultado Real': 'Alcista' if real_direction == 1 else 'Bajista' if real_direction == 0 else None,\n",
        "            'Precio Cierre': close_price,\n",
        "            'Probabilidad Alcista': prediction_prob,\n",
        "            'Correcta': 'Sí' if is_correct == 1 else 'No' if is_correct == 0 else None\n",
        "        })\n",
        "\n",
        "# Crear tabla de resultados\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.set_index('Fecha Predicción', inplace=True)\n",
        "\n",
        "# Mostrar resultados\n",
        "pd.set_option('display.max_columns', None)\n",
        "print(\"\\nResultados del backtesting (hasta\", end_date.strftime('%Y-%m-%d'), \"):\")\n",
        "print(\"Nota: 'Fecha Predicción' es la fecha predicha; 'Fecha Datos' es la fecha de los datos usados.\")\n",
        "print(results_df)\n",
        "\n",
        "# Guardar y descargar el CSV\n",
        "results_df.to_csv(f\"backtesting_results_{symbol}_{end_date.strftime('%Y-%m-%d')}.csv\", sep=\";\")\n",
        "files.download(f\"backtesting_results_{symbol}_{end_date.strftime('%Y-%m-%d')}.csv\")\n",
        "print(f\"\\nArchivo 'backtesting_results_{symbol}_{end_date.strftime('%Y-%m-%d')}.csv' generado y descargado.\")\n",
        "\n",
        "# Métricas del backtesting\n",
        "#if results_df['Correcta'].notna().sum() > 0:\n",
        "#    accuracy = (results_df['Correcta'] == 'Sí').sum() / results_df['Correcta'].notna().sum()\n",
        "#    print(f\"\\nAccuracy del backtesting: {accuracy:.2%}\")\n",
        "\n",
        "#    valid_results = results_df[results_df['Correcta'].notna()]\n",
        "#    y_true = [1 if r == 'Alcista' else 0 for r in valid_results['Resultado Real']]\n",
        "#    y_pred = [1 if p == 'Alcista' else 0 for p in valid_results['Predicción']]\n",
        "#    print(\"\\nMatriz de Confusión:\")\n",
        "#    print(confusion_matrix(y_true, y_pred))\n",
        "#    print(\"\\nInforme de Clasificación:\")\n",
        "#    print(classification_report(y_true, y_pred))\n",
        "\n",
        "# ROC-AUC para entrenamiento y prueba\n",
        "if not X_train_full.empty and not X_test.empty:\n",
        "    X_train_scaled = StandardScaler().fit_transform(X_train_full)\n",
        "    X_test_scaled = StandardScaler().fit_transform(X_test)\n",
        "    best_model.fit(X_train_scaled, y_train_full)\n",
        "    train_pred_proba = best_model.predict_proba(X_train_scaled)[:, 1]\n",
        "    test_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
        "    roc_auc_train = roc_auc_score(y_train_full, train_pred_proba)\n",
        "    roc_auc_test = roc_auc_score(y_test, test_pred_proba)\n",
        "    # Verificar tamaños de las muestras\n",
        "    print(f\"Tamaño de y_train_full: {y_train_full.size}\")\n",
        "    print(f\"Tamaño de y_test: {y_test.size}\")\n",
        "    print(\"Distribución de clases en y_train_full:\")\n",
        "    print(y_train_full.value_counts())\n",
        "    print(\"Distribución de clases en y_test:\")\n",
        "    print(y_test.value_counts())\n",
        "\n",
        "\n",
        "    print(f\"\\nROC-AUC en el conjunto de entrenamiento: {roc_auc_train:.4f}\")\n",
        "    print(roc_auc_train)\n",
        "    print(f\"ROC-AUC en el conjunto de prueba: {roc_auc_test:.4f}\")\n",
        "    y_pred_test = (best_model.predict_proba(X_test_scaled)[:, 1] >= 0.3).astype(int)\n",
        "    print(\"\\nMatriz de Confusión (prueba):\")\n",
        "    print(confusion_matrix(y_test, y_pred_test))\n",
        "    print(\"\\nInforme de Clasificación (prueba):\")\n",
        "    print(classification_report(y_test, y_pred_test))\n",
        "else:\n",
        "    print(\"\\nAdvertencia: Conjunto de prueba o entrenamiento insuficiente. No se calculó ROC-AUC.\")\n",
        "\n",
        "# Predicción para el día siguiente\n",
        "last_features = df[features].iloc[-1:]\n",
        "scaler = StandardScaler()\n",
        "last_features_scaled = scaler.fit_transform(last_features)\n",
        "future_pred_prob = best_model.predict_proba(last_features_scaled)[0][1]\n",
        "future_pred = 1 if future_pred_prob >= best_threshold else 0\n",
        "\n",
        "returns = df['Return'].dropna()\n",
        "mean_return = returns.mean()\n",
        "std_return = returns.std()\n",
        "last_close = df['Close'].iloc[-1]\n",
        "expected_price = last_close * np.exp(mean_return + 0.5 * std_return**2)\n",
        "price_prob = future_pred_prob if future_pred == 1 else 1 - future_pred_prob\n",
        "\n",
        "action = 'BUY' if future_pred == 1 else 'SELL'\n",
        "direction = 1 if future_pred == 1 else -1\n",
        "\n",
        "print(f\"\\nPredicción para {next_day.strftime('%Y-%m-%d')} (basada en datos hasta {df.index[-1].strftime('%Y-%m-%d')}):\")\n",
        "print(f\"Papel: {symbol}\")\n",
        "print(f\"Precio actual: [{last_close:.4f}]\")\n",
        "print(f\"Precio esperado para el siguiente día (distribución log-normal): [{expected_price:.4f}]\")\n",
        "print(f\"Probabilidad de que el precio predicho sea correcto: [{price_prob:.4f}]\")\n",
        "print(f\"Corte: {df.index[-1]}\")\n",
        "print(f\"\\nPredicción para {next_day.strftime('%Y-%m-%d')}: {'Alcista' if future_pred == 1 else 'Bajista'} (Probabilidad Alcista: {future_pred_prob:.2%})\")\n",
        "print(f\"Pronóstico de dirección del activo (1: subida, -1: bajada): {direction}\")\n",
        "print(f\"Acción sugerida por la estrategia de trading: {action}\")"
      ],
      "metadata": {
        "id": "OReUSMZapafA",
        "outputId": "1ce83045-2223-48c3-b44a-450602e143bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultiIndex detectado en columnas. Aplanando...\n",
            "Columnas asignadas después de aplanamiento: ['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']\n",
            "Últimas filas antes de corrección:               Open    High     Low   Close  Volume  Adj Close\n",
            "Date                                                         \n",
            "2025-07-11  6210.0  6210.0  6470.0  6170.0  6470.0     334610\n",
            "2025-07-14  6330.0  6330.0  6360.0  6000.0  6050.0     379370\n",
            "2025-07-15  6430.0  6430.0  6470.0  6250.0  6330.0     216847\n",
            "2025-07-16  6270.0  6270.0  6600.0  5950.0  6420.0     255442\n",
            "2025-07-17  6500.0  6500.0  6540.0  6260.0  6290.0     267819\n",
            "Últimas filas después de corrección:               Open    High     Low   Close  Volume  Adj Close\n",
            "Date                                                         \n",
            "2025-07-11  6470.0  6470.0  6170.0  6210.0  334610     6210.0\n",
            "2025-07-14  6050.0  6360.0  6000.0  6330.0  379370     6330.0\n",
            "2025-07-15  6330.0  6470.0  6250.0  6430.0  216847     6430.0\n",
            "2025-07-16  6420.0  6600.0  5950.0  6270.0  255442     6270.0\n",
            "2025-07-17  6290.0  6540.0  6260.0  6500.0  267819     6500.0\n",
            "Columnas del DataFrame después de descargar y corregir:\n",
            "Index(['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close'], dtype='object')\n",
            "Últimas filas del DataFrame antes de crear features:\n",
            "              Open    High     Low   Close  Volume  Adj Close\n",
            "Date                                                         \n",
            "2025-07-11  6470.0  6470.0  6170.0  6210.0  334610     6210.0\n",
            "2025-07-14  6050.0  6360.0  6000.0  6330.0  379370     6330.0\n",
            "2025-07-15  6330.0  6470.0  6250.0  6430.0  216847     6430.0\n",
            "2025-07-16  6420.0  6600.0  5950.0  6270.0  255442     6270.0\n",
            "2025-07-17  6290.0  6540.0  6260.0  6500.0  267819     6500.0\n",
            "\n",
            "Últimas filas del DataFrame después de crear features:\n",
            "              Open    High     Low   Close  Volume  Adj Close  close_lag_1  \\\n",
            "Date                                                                         \n",
            "2025-07-11  6470.0  6470.0  6170.0  6210.0  334610     6210.0       6510.0   \n",
            "2025-07-14  6050.0  6360.0  6000.0  6330.0  379370     6330.0       6210.0   \n",
            "2025-07-15  6330.0  6470.0  6250.0  6430.0  216847     6430.0       6330.0   \n",
            "2025-07-16  6420.0  6600.0  5950.0  6270.0  255442     6270.0       6430.0   \n",
            "2025-07-17  6290.0  6540.0  6260.0  6500.0  267819     6500.0       6270.0   \n",
            "\n",
            "            close_lag_2  close_lag_3  close_lag_4  close_lag_5  Pct_change  \\\n",
            "Date                                                                         \n",
            "2025-07-11       6770.0       6530.0       6670.0       6640.0   -0.046083   \n",
            "2025-07-14       6510.0       6770.0       6530.0       6670.0    0.019324   \n",
            "2025-07-15       6210.0       6510.0       6770.0       6530.0    0.015798   \n",
            "2025-07-16       6330.0       6210.0       6510.0       6770.0   -0.024883   \n",
            "2025-07-17       6430.0       6330.0       6210.0       6510.0    0.036683   \n",
            "\n",
            "               lag_1     lag_2     lag_3     lag_4     lag_5        RSI  \\\n",
            "Date                                                                      \n",
            "2025-07-11 -0.038405  0.036753 -0.020990  0.004518 -0.005988  30.188679   \n",
            "2025-07-14 -0.046083 -0.038405  0.036753 -0.020990  0.004518  34.513274   \n",
            "2025-07-15  0.019324 -0.046083 -0.038405  0.036753 -0.020990  41.176471   \n",
            "2025-07-16  0.015798  0.019324 -0.046083 -0.038405  0.036753  34.848485   \n",
            "2025-07-17 -0.024883  0.015798  0.019324 -0.046083 -0.038405  48.936170   \n",
            "\n",
            "                 ROC       PPO  PPO_Signal  PPO_Histogram     SMA      SMA21  \\\n",
            "Date                                                                           \n",
            "2025-07-11 -6.475904 -1.296286   -0.851224      -0.445061  6538.0  6757.7295   \n",
            "2025-07-14 -5.097451 -1.354926   -1.019125      -0.335801  6470.0  6713.3100   \n",
            "2025-07-15 -1.531394 -1.089325   -1.042525      -0.046800  6450.0  6679.8281   \n",
            "2025-07-16 -7.385524 -1.209803   -1.098285      -0.111519  6350.0  6624.9505   \n",
            "2025-07-17 -0.153610 -0.713185   -0.969918       0.256733  6348.0  6595.7524   \n",
            "\n",
            "                SMA30  Volatility  Label    Return  Is_Earnings_Season  \\\n",
            "Date                                                                     \n",
            "2025-07-11  6962.7937  267.911951      0 -0.047179                True   \n",
            "2025-07-14  6902.1033  264.871358      1  0.019139                True   \n",
            "2025-07-15  6855.7203  207.679434      1  0.015674                True   \n",
            "2025-07-16  6803.3390  192.149553      0 -0.025198                True   \n",
            "2025-07-17  6770.9287  190.711833      1  0.036026                True   \n",
            "\n",
            "            Is_Christmas_Rally  \n",
            "Date                            \n",
            "2025-07-11               False  \n",
            "2025-07-14               False  \n",
            "2025-07-15               False  \n",
            "2025-07-16               False  \n",
            "2025-07-17               False  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-8-1598879408.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0mxgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary:logistic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0mrandom_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_distributions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'f1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m \u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_full\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mejores hiperparámetros:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1022\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1949\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1950\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1951\u001b[0;31m         evaluate_candidates(\n\u001b[0m\u001b[1;32m   1952\u001b[0m             ParameterSampler(\n\u001b[1;32m   1953\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    968\u001b[0m                     )\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    971\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    972\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2070\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2072\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1798\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_PENDING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m                 ):\n\u001b[0;32m-> 1800\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1801\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "COMIENZA EL UMBRAL"
      ],
      "metadata": {
        "id": "xjiSoEl0ijkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import datetime as dt\n",
        "\n",
        "# Descargar datos (si no lo hiciste aún)\n",
        "end_date = dt.datetime(2025, 7, 20)\n",
        "df = yf.download(\"COME.BA\", start=dt.datetime(2001, 1, 1), end=end_date)\n",
        "\n",
        "# Definir umbral (por ejemplo, 2%)\n",
        "umbral = 0.02\n",
        "\n",
        "# Calcular la diferencia relativa y etiquetar\n",
        "df['Label'] = ((df['High'] - df['Open']) / df['Open'] > umbral).astype(int) * 2 - 1\n",
        "\n",
        "# Contar etiquetas y calcular porcentajes\n",
        "label_counts = df['Label'].value_counts()\n",
        "total_dias = len(df)\n",
        "percentages = (label_counts / total_dias) * 100\n",
        "\n",
        "# Imprimir resultados\n",
        "print(\"Conteo de etiquetas:\")\n",
        "print(label_counts)\n",
        "print(\"\\nPorcentajes de cada clase (%):\")\n",
        "print(percentages.round(2))\n",
        "\n",
        "# Opcional: Ver los primeros días etiquetados\n",
        "print(\"\\nPrimeros días etiquetados:\")\n",
        "print(df[['Open', 'High', 'Label']].tail(20))\n",
        "print(\"mambral\")\n",
        "umbral = df['High'].sub(df['Open']).div(df['Open']).quantile(0.75)\n",
        "print(umbral)\n",
        "df['Label'] = ((df['High'] - df['Open']) / df['Open'] > umbral).astype(int) * 2 - 1\n",
        "print(df['Label'].value_counts(normalize=True) * 100)\n",
        "\n",
        "# Contar etiquetas y calcular porcentajes\n",
        "label_counts = df['Label'].value_counts()\n",
        "total_dias = len(df)\n",
        "percentages = (label_counts / total_dias) * 100\n",
        "\n",
        "# Imprimir resultados\n",
        "print(\"Conteo de etiquetas:\")\n",
        "print(label_counts)\n",
        "print(\"\\nPorcentajes de cada clase (%):\")\n",
        "print(percentages.round(2))"
      ],
      "metadata": {
        "id": "-ItKP7EgLl9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import datetime as dt\n",
        "\n",
        "# Descargar datos\n",
        "end_date = dt.datetime(2025, 7, 17)\n",
        "df = yf.download(\"GGAL\", start=dt.datetime(2001, 1, 1), end=end_date)\n",
        "\n",
        "# Aplanar el MultiIndex a columnas simples\n",
        "df.columns = df.columns.map(lambda x: x[0])\n",
        "\n",
        "# Limpiar datos\n",
        "df = df.dropna(subset=['Open', 'High'])\n",
        "\n",
        "# Calcular umbral dinámico\n",
        "differences = (df['High'] - df['Open']) / df['Open']\n",
        "umbral = differences.quantile(0.6).item()\n",
        "print(f\"Umbral calculado: {umbral:.4f}\")\n",
        "\n",
        "# Calcular etiqueta sin desfase (para verificar)\n",
        "df['Label_raw'] = ((df['High'] - df['Open']) / df['Open'] > umbral).astype(int) * 2 - 1\n",
        "\n",
        "# Desplazar la etiqueta un día hacia atrás (target del día siguiente)\n",
        "df['Label'] = df['Label_raw'].shift(-1)\n",
        "\n",
        "# Eliminar la última fila (no tiene etiqueta para predecir)\n",
        "df = df.dropna(subset=['Label'])\n",
        "\n",
        "# Contar etiquetas y calcular porcentajes\n",
        "label_counts = df['Label'].value_counts()\n",
        "total_dias = len(df)\n",
        "percentages = (label_counts / total_dias) * 100\n",
        "\n",
        "print(\"\\nConteo de etiquetas (desfasadas):\")\n",
        "print(label_counts)\n",
        "print(\"\\nPorcentajes de cada clase (%):\")\n",
        "print(percentages.round(2))\n",
        "\n",
        "# Opcional: Ver los primeros días etiquetados\n",
        "print(\"\\nPrimeros días etiquetados (features del día anterior, label del día siguiente):\")\n",
        "print(df[['Open', 'High', 'Label']].tail(22))"
      ],
      "metadata": {
        "id": "BYoBb-mnRDDT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}