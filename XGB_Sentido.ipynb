{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmdiegoar/Quant-code-t0/blob/main/XGB_Sentido.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LA MEJORA DE GROK"
      ],
      "metadata": {
        "id": "oqaw_AHkEbnN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "mas listo que ayer"
      ],
      "metadata": {
        "id": "6x9IIbsCpVUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, f1_score\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "from google.colab import files  # Para descargar el CSV en Colab\n",
        "from scipy import stats\n",
        "\n",
        "\n",
        "# Funciones de features\n",
        "def add_lagged_price_features(df):\n",
        "    for lag in range(1, 6):\n",
        "        df[f'close_lag_{lag}'] = df['Close'].shift(lag)\n",
        "    return df\n",
        "\n",
        "def calculate_RSI(series, period=7):\n",
        "    delta = series.diff(1)\n",
        "    gain = delta.where(delta > 0, 0).rolling(window=period).mean()\n",
        "    loss = -delta.where(delta < 0, 0).rolling(window=period).mean()\n",
        "    rs = gain / loss\n",
        "    return 100 - (100 / (1 + rs))\n",
        "\n",
        "def calculate_ROC(series, period=5):\n",
        "    return ((series - series.shift(period)) / series.shift(period)) * 100\n",
        "\n",
        "def calculate_PPO(series, fast_period=5, slow_period=9, signal_period=5):\n",
        "    ema_fast = series.ewm(span=fast_period, adjust=False).mean()\n",
        "    ema_slow = series.ewm(span=slow_period, adjust=False).mean()\n",
        "    ppo = (ema_fast - ema_slow) / ema_slow * 100\n",
        "    signal_line = ppo.ewm(span=signal_period, adjust=False).mean()\n",
        "    histogram = ppo - signal_line\n",
        "    return ppo, signal_line, histogram\n",
        "\n",
        "def calculate_EWO(series, fast_period=5, slow_period=35, signal_period=5):\n",
        "    ema_fast = series.ewm(span=fast_period, adjust=False).mean()\n",
        "    ema_slow = series.ewm(span=slow_period, adjust=False).mean()\n",
        "    ewo = (ema_fast - ema_slow) / ema_slow * 100\n",
        "    signal_line = ewo.ewm(span=signal_period, adjust=False).mean()\n",
        "    histogram = ewo - signal_line\n",
        "    return ewo, signal_line, histogram\n",
        "\n",
        "def calculate_volatility(series, window=20):\n",
        "    return series.rolling(window).std().round(6)\n",
        "\n",
        "def calculate_sma5(series, period=5):\n",
        "    return series.rolling(window=period).mean().round(4)\n",
        "\n",
        "def calculate_sma13(series, period=13):\n",
        "    return series.rolling(window=period).mean().round(4)\n",
        "\n",
        "def calculate_sma26(series, period=26):\n",
        "    return series.rolling(window=period).mean().round(4)\n",
        "\n",
        "def calculate_sma50(series, period=50):\n",
        "    return series.rolling(window=period).mean().round(4)\n",
        "\n",
        "def calculate_sma200(series, period=200):\n",
        "    return series.rolling(window=period).mean().round(4)\n",
        "\n",
        "\n",
        "#def calculate_earnings_season(df):\n",
        "#    df['Is_Earnings_Season'] = df.index.month.isin([1, 4, 7, 10])\n",
        "#    return df\n",
        "\n",
        "#def calculate_christmas_rally(df):\n",
        "#    df['Is_Christmas_Rally'] = df.index.month.isin([11, 12])\n",
        "#    return df\n",
        "\n",
        "def create_features(df):\n",
        "    df = add_lagged_price_features(df)\n",
        "    df['Pct_change'] = df['Close'].pct_change()\n",
        "    for lag in range(1, 6):\n",
        "        df[f'lag_change{lag}'] = df['Pct_change'].shift(lag)\n",
        "    df['RSI'] = calculate_RSI(df['Close'])\n",
        "    df['ROC'] = calculate_ROC(df['Close'])\n",
        "    df['PPO'], df['PPO_Signal'], df['PPO_Histogram'] = calculate_PPO(df['Close'])\n",
        "    df['EWO'], df['EWO_Signal'], df['EWO_Histogram'] = calculate_EWO(df['Close'])\n",
        "    df['SMA5'] = calculate_sma5(df['Close'])\n",
        "    df['SMA13'] = calculate_sma13(df['Close'])\n",
        "    df['SMA26'] = calculate_sma26(df['Close'])\n",
        "    df['SMA50'] = calculate_sma50(df['Close'])\n",
        "    df['SMA200'] = calculate_sma200(df['Close'])\n",
        "    df['Volatility'] = calculate_volatility(df['Close'])\n",
        "    df['Label'] = (df['Pct_change'] > 0).astype(int)\n",
        "    df['Return'] = np.log(df['Close'] / df['Close'].shift())\n",
        "    #df = calculate_earnings_season(df)\n",
        "    #df = calculate_christmas_rally(df)\n",
        "    df.dropna(inplace=True)\n",
        "    return df\n",
        "\n",
        "# Definir fecha de corte manualmente (cambiar diariamente)\n",
        "end_date = dt.datetime(2025, 7, 23)  # Ejemplo: cambiar a 2025-07-18 mañana\n",
        "\n",
        "tk =[ \"ALUA.BA\", \"BBAR.BA\", \"BMA.BA\", \"COME.BA\", \"CRES.BA\", \"EDN.BA\", \"GGAL.BA\", \"IRSA.BA\", \"LOMA.BA\", \"METR.BA\", \"PAMP.BA\", \"SUPV.BA\", \"TECO2.BA\", \"TGNO4.BA\", \"TGSU2.BA\", \"TRAN.BA\", \"TXAR.BA\", \"VALO.BA\", \"YPFD.BA\"]\n",
        "\n",
        "# pinchas = \"BYMA.BA\" start_date dt.datetime(2021, 1, 1),  \"CEPU.BA\" start_date  dt.datetime(2018, 1, 1)\n",
        "\n",
        "results = [] # el del test, para que no lo reinicie\n",
        "resultsp = [] # las predicciones, para que no lo reinicie\n",
        "\n",
        "for papel in tk:\n",
        "\n",
        "  symbol=papel\n",
        "  #symbol=\"COME.BA\"\n",
        "  # Fechas dinámicas\n",
        "  start_date = dt.datetime(2001, 1, 1)  # Inicio fijo\n",
        "  train_end = end_date - pd.Timedelta(days=780)  # 6 meses antes de end_date (ajustable)\n",
        "  next_day = end_date + pd.Timedelta(days=1)  # Predicción para el día siguiente\n",
        "  backtest_start = end_date - pd.Timedelta(days=2)  # Inicio del backtesting 6 meses antes de end_date (ajustable)\n",
        "\n",
        "  # Descargar datos\n",
        "  df = yf.download(symbol, start=start_date, end=end_date, auto_adjust=False)\n",
        "\n",
        "  # Verificar datos\n",
        "\n",
        "  for intentos in range(1, 20):\n",
        "    if df.empty:\n",
        "      print(\"No se pudieron descargar datos. Verifica el símbolo, las fechas o la conexión.\")\n",
        "      print(f\"\\n Reintentando {symbol}: {intentos} de 20\")\n",
        "      df = yf.download(symbol, start=start_date, end=end_date, auto_adjust=False)\n",
        "      intentos +=1\n",
        "      time.sleep(8)\n",
        "    else:\n",
        "      print(f\"\\n Datos descargados en {intentos} vueltas: {symbol} , seguimos\")\n",
        "\n",
        "\n",
        "  # Aplanar MultiIndex si existe\n",
        "  if isinstance(df.columns, pd.MultiIndex):\n",
        "      print(\"MultiIndex detectado en columnas. Aplanando...\")\n",
        "      df.columns = ['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']\n",
        "      print(\"Columnas asignadas después de aplanamiento:\", df.columns.tolist())\n",
        "      print(\"Últimas filas antes de corrección:\", df.tail())\n",
        "\n",
        "      # Corregir el orden de las columnas según tu mapeo\n",
        "      columns = df.columns.tolist()\n",
        "      df = df[[columns[4], columns[2], columns[3], columns[0], columns[5], columns[1]]]  # Reordenar\n",
        "      df.columns = ['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']  # Asignar nombres correctos\n",
        "      print(\"Últimas filas después de corrección:\", df.tail())\n",
        "\n",
        "  if isinstance(df.index, pd.MultiIndex):\n",
        "      print(\"MultiIndex detectado en índice. Seleccionando ticker...\")\n",
        "      df = df.xs(symbol, level='Ticker', axis=0)\n",
        "  df.index = pd.to_datetime(df.index)  # Asegurar índice datetime\n",
        "  if not df.index.is_unique:\n",
        "      print(\"Advertencia: Índice con fechas duplicadas. Eliminando duplicados...\")\n",
        "      df = df[~df.index.duplicated(keep='first')]\n",
        "\n",
        "  # Verificar columnas\n",
        "  print(\"Columnas del DataFrame después de descargar y corregir:\")\n",
        "  print(df.columns)\n",
        "\n",
        "  df['Open']= df['Open'].round(2)\n",
        "  df['High']= df['High'].round(2)\n",
        "  df['Low']= df['Low'].round(2)\n",
        "  df['Close']= df['Close'].round(2)\n",
        "  df['Adj Close']= df['Adj Close'].round(2)\n",
        "\n",
        "  print(\"Últimas filas del DataFrame antes de crear features:\")\n",
        "  print(df.tail())\n",
        "\n",
        "  # Crear features\n",
        "  df = create_features(df)\n",
        "\n",
        "  # Verificar datos después de crear features\n",
        "  print(\"\\nÚltimas filas del DataFrame después de crear features:\")\n",
        "  print(df.tail())\n",
        "\n",
        "  # Seleccionar features\n",
        "  features = ['RSI', 'ROC', 'PPO', 'PPO_Signal', 'PPO_Histogram', 'EWO', 'EWO_Signal', 'EWO_Histogram', 'Volatility', 'SMA5', 'SMA13', 'SMA26', 'SMA50', 'SMA200' ] + [f'lag_change{i}' for i in range(1, 6)] + \\\n",
        "            [f'close_lag_{i}' for i in range(1, 6)]\n",
        "  X = df[features]\n",
        "  y = df['Label']\n",
        "\n",
        "  # Dividir datos en entrenamiento y prueba\n",
        "  X_train_full = X[df.index <= train_end]\n",
        "  y_train_full = y[df.index <= train_end]\n",
        "  X_test = X[(df.index > train_end) & (df.index <= end_date)]  # Hasta end_date\n",
        "  y_test = y[(df.index > train_end) & (df.index <= end_date)]\n",
        "\n",
        "  # Optimizar hiperparámetros con RandomizedSearchCV\n",
        "  print(\"Optimizar hiperparámetros con RandomizedSearchCV\")\n",
        "  param_dist = {\n",
        "      'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "      'max_depth': [3, 5, 7, 9],\n",
        "      'n_estimators': [100, 500, 900],\n",
        "      'subsample': [0.6, 0.8, 1.0],\n",
        "      'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "      'gamma': [0, 0.1, 0.2]\n",
        "  }\n",
        "  xgb = XGBClassifier(objective='binary:logistic', random_state=42)\n",
        "  random_search = RandomizedSearchCV(xgb, param_distributions=param_dist, n_iter=20, cv=5, scoring='f1', n_jobs=-1, random_state=42)\n",
        "  random_search.fit(X_train_full, y_train_full)\n",
        "  print(\"Mejores hiperparámetros:\", random_search.best_params_)\n",
        "\n",
        "  # Usar el mejor modelo\n",
        "  best_model = random_search.best_estimator_\n",
        "\n",
        "  # Optimizar el umbral\n",
        "  y_train_prob = best_model.predict_proba(X_train_full)[:, 1]\n",
        "  thresholds = np.arange(0.28, 0.88, 0.01) # de 0.28 a 0.88  con pasos de 0.01????\n",
        "  best_threshold = 0.5\n",
        "  best_f1 = 0\n",
        "  for threshold in thresholds:\n",
        "      y_pred_threshold = (y_train_prob >= threshold).astype(int)\n",
        "      f1 = f1_score(y_train_full, y_pred_threshold)\n",
        "      if f1 > best_f1:\n",
        "          best_f1 = f1\n",
        "          best_threshold = threshold\n",
        "  best_threshold.round(2)\n",
        "  print(\"Mejor umbral:\", best_threshold)\n",
        "\n",
        "\n",
        "\n",
        "  # Backtesting con escalado dinámico para evitar data leak\n",
        "  dates = df[(df.index >= backtest_start) & (df.index <= end_date)].index\n",
        "  #results = []\n",
        "\n",
        "  for test_date in dates:\n",
        "      train_data = df[df.index < test_date].copy()\n",
        "      if train_data.empty or train_data['Label'].isna().all():\n",
        "          continue\n",
        "      train_data = train_data.dropna()\n",
        "\n",
        "      X_train_loop = train_data[features]\n",
        "      y_train_loop = train_data['Label']\n",
        "\n",
        "      scaler = StandardScaler()  # Reinicio dinámico del escalador\n",
        "      X_train_scaled = scaler.fit_transform(X_train_loop)\n",
        "      print(f\"Escalando datos hasta {train_data.index[-1]} para predecir {test_date}\")\n",
        "\n",
        "      best_model.fit(X_train_scaled, y_train_loop)\n",
        "\n",
        "      if test_date in df.index:\n",
        "          test_row = df.loc[[test_date]][features]\n",
        "          if test_row.empty:\n",
        "              continue\n",
        "          test_row_scaled = scaler.transform(test_row)\n",
        "\n",
        "          prediction_prob = best_model.predict_proba(test_row_scaled)[0][1].round(4)\n",
        "          prediction = 1 if prediction_prob >= best_threshold else 0\n",
        "\n",
        "          real_direction = df.loc[test_date, 'Label'] if pd.notna(df.loc[test_date, 'Label']) else None\n",
        "          close_price = df.loc[test_date, 'Close']\n",
        "\n",
        "          is_correct = int(prediction == real_direction) if real_direction is not None else None\n",
        "          data_date = df.index[df.index < test_date][-1] if not df[df.index < test_date].empty else None\n",
        "\n",
        "          results.append({\n",
        "              'Papel': papel,\n",
        "              'Fecha Predicción': test_date,\n",
        "              'Fecha Datos': data_date,\n",
        "              'Predicción': 'Alcista' if prediction == 1 else 'Bajista',\n",
        "              'Resultado Real': 'Alcista' if real_direction == 1 else 'Bajista' if real_direction == 0 else None,\n",
        "              'Precio Cierre': close_price,\n",
        "              'Probabilidad Alcista': prediction_prob,\n",
        "              'Correcta': 'Sí' if is_correct == 1 else 'No' if is_correct == 0 else None\n",
        "          })\n",
        "\n",
        "  # Crear tabla de resultados\n",
        "  results_df = pd.DataFrame(results)\n",
        "  results_df.set_index('Fecha Predicción', inplace=True)\n",
        "\n",
        "  # Mostrar resultados\n",
        "  pd.set_option('display.max_columns', None)\n",
        "  print(\"\\nResultados del backtesting (hasta\", end_date.strftime('%Y-%m-%d'), \"):\")\n",
        "  print(\"Nota: 'Fecha Predicción' es la fecha predicha; 'Fecha Datos' es la fecha de los datos usados.\")\n",
        "  print(results_df)\n",
        "\n",
        "  # Guardar y descargar el CSV\n",
        "  #results_df.to_csv(f\"backtesting_results_{symbol}_{end_date.strftime('%Y-%m-%d')}.csv\", sep=\";\")\n",
        "  #files.download(f\"backtesting_results_{symbol}_{end_date.strftime('%Y-%m-%d')}.csv\")\n",
        "  #print(f\"\\nArchivo 'backtesting_results_{symbol}_{end_date.strftime('%Y-%m-%d')}.csv' generado y descargado.\")\n",
        "\n",
        "  # Métricas del backtesting\n",
        "  #if results_df['Correcta'].notna().sum() > 0:\n",
        "  #    accuracy = (results_df['Correcta'] == 'Sí').sum() / results_df['Correcta'].notna().sum()\n",
        "  #    print(f\"\\nAccuracy del backtesting: {accuracy:.2%}\")\n",
        "\n",
        "  #    valid_results = results_df[results_df['Correcta'].notna()]\n",
        "  #    y_true = [1 if r == 'Alcista' else 0 for r in valid_results['Resultado Real']]\n",
        "  #    y_pred = [1 if p == 'Alcista' else 0 for p in valid_results['Predicción']]\n",
        "  #    print(\"\\nMatriz de Confusión:\")\n",
        "  #    print(confusion_matrix(y_true, y_pred))\n",
        "  #    print(\"\\nInforme de Clasificación:\")\n",
        "  #    print(classification_report(y_true, y_pred))\n",
        "\n",
        "  # ROC-AUC para entrenamiento y prueba\n",
        "  if not X_train_full.empty and not X_test.empty:\n",
        "      X_train_scaled = StandardScaler().fit_transform(X_train_full)\n",
        "      X_test_scaled = StandardScaler().fit_transform(X_test)\n",
        "      best_model.fit(X_train_scaled, y_train_full)\n",
        "      train_pred_proba = best_model.predict_proba(X_train_scaled)[:, 1]\n",
        "      test_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
        "      roc_auc_train = roc_auc_score(y_train_full, train_pred_proba)\n",
        "      roc_auc_test = roc_auc_score(y_test, test_pred_proba).round(6)\n",
        "      # Verificar tamaños de las muestras\n",
        "      print(f\"Tamaño de y_train_full: {y_train_full.size}\")\n",
        "      print(f\"Tamaño de y_test: {y_test.size}\")\n",
        "      print(\"Distribución de clases en y_train_full:\")\n",
        "      print(y_train_full.value_counts())\n",
        "      ratio_1_train=(y_train_full.value_counts()[1]/y_train_full.size).round(4)\n",
        "      print(f\"% clase 1 train: {ratio_1_train} \")\n",
        "\n",
        "      print(\"Distribución de clases en y_test:\")\n",
        "      print(y_test.value_counts())\n",
        "      ratio_1_test=(y_test.value_counts()[1]/y_test.size).round(4)\n",
        "      print(f\"% clase 1 test: {ratio_1_test} \")\n",
        "\n",
        "      print(f\"\\nROC-AUC en el conjunto de entrenamiento: {roc_auc_train:.4f}\")\n",
        "      print(roc_auc_train)\n",
        "      print(f\"ROC-AUC en el conjunto de prueba: {roc_auc_test:.4f}\")\n",
        "      y_pred_test = (best_model.predict_proba(X_test_scaled)[:, 1] >= 0.3).astype(int)\n",
        "      print(\"\\nMatriz de Confusión (prueba):\")\n",
        "      print(confusion_matrix(y_test, y_pred_test))\n",
        "      print(\"\\nInforme de Clasificación (prueba):\")\n",
        "      print(classification_report(y_test, y_pred_test))\n",
        "  else:\n",
        "      print(\"\\nAdvertencia: Conjunto de prueba o entrenamiento insuficiente. No se calculó ROC-AUC.\")\n",
        "\n",
        "  print(\"Mejores hiperparámetros:\", random_search.best_params_)\n",
        "  print(\"Mejor umbral:\", best_threshold)\n",
        "\n",
        "\n",
        "  # Predicción para el día siguiente\n",
        "  last_features = df[features].iloc[-1:]\n",
        "  scaler = StandardScaler()\n",
        "  last_features_scaled = scaler.fit_transform(last_features)\n",
        "  future_pred_prob = best_model.predict_proba(last_features_scaled)[0][1].round(4)\n",
        "  future_pred = 1 if future_pred_prob >= best_threshold else 0\n",
        "\n",
        "  returns = df['Return'].dropna()\n",
        "  mean_return = returns.mean()\n",
        "  std_return = returns.std()\n",
        "  last_close = df['Close'].iloc[-1]\n",
        "  expected_price = last_close * np.exp(mean_return + 0.5 * std_return**2)\n",
        "  price_prob = future_pred_prob if future_pred == 1 else 1 - future_pred_prob\n",
        "\n",
        "  action = 'BUY' if future_pred == 1 else 'SELL'\n",
        "  direction = 1 if future_pred == 1 else -1\n",
        "\n",
        "  resultsp.append({\n",
        "              'Papel': papel,\n",
        "              'Fecha Predicción': next_day,\n",
        "              'Fecha Datos': df.index[-1],\n",
        "              'Predicción': 'Alcista' if future_pred == 1 else 'Bajista',\n",
        "              'Resultado Real': \"Veremos\",\n",
        "              'Precio actual': last_close,\n",
        "              'Precio Cierre': \"futuro\",\n",
        "              'Correcta': \"verificar\",\n",
        "              'Probabilidad Alcista': future_pred_prob,\n",
        "              'Umbral': best_threshold,\n",
        "              'ROC-AUC prueba': roc_auc_test ,\n",
        "              'clase 1 en train': str(ratio_1_train),\n",
        "              'clase 1 en test': str(ratio_1_test),\n",
        "              'Mejores hiperparámetros': str(random_search.best_params_),\n",
        "              'Matrix prueba': str(classification_report(y_test, y_pred_test))\n",
        "              #si la pasas a tolist() perdes los titulos\n",
        "              #'Matrix prueba': str((classification_report(y_test, y_pred_test)).tolist())\n",
        "\n",
        "          })\n",
        "  # Crear tabla de resultados\n",
        "  resultsp_df = pd.DataFrame(resultsp)\n",
        "  resultsp_df.set_index('Fecha Predicción', inplace=True)\n",
        "\n",
        "  # Mostrar resultados\n",
        "  pd.set_option('display.max_columns', None)\n",
        "  print(\"\\nPrediccion para el proximo dia (hasta\", next_day.strftime('%Y-%m-%d'), \"):\")\n",
        "  print(\"Nota: 'Fecha Predicción' es la fecha predicha; 'Fecha Datos' es la fecha de los datos usados.\")\n",
        "  print(resultsp_df)\n",
        "\n",
        "  print(f\"\\nPredicción para {next_day.strftime('%Y-%m-%d')} (basada en datos hasta {df.index[-1].strftime('%Y-%m-%d')}):\")\n",
        "  print(f\"Papel: {symbol}\")\n",
        "  print(f\"Precio actual: [{last_close:.4f}]\")\n",
        "  print(f\"Precio esperado para el siguiente día (distribución log-normal): [{expected_price:.4f}]\")\n",
        "  print(f\"Probabilidad de que el precio predicho sea correcto: [{price_prob:.4f}]\")\n",
        "  print(f\"Corte: {df.index[-1]}\")\n",
        "  print(f\"\\nPredicción para {next_day.strftime('%Y-%m-%d')}: {'Alcista' if future_pred == 1 else 'Bajista'} (Probabilidad Alcista: {future_pred_prob:.2%})\")\n",
        "  print(f\"Pronóstico de dirección del activo (1: subida, -1: bajada): {direction}\")\n",
        "  print(f\"Acción sugerida por la estrategia de trading: {action}\")\n",
        "\n",
        "  print(f\"\\n {symbol}, {next_day.strftime('%d-%m-%y')}, {df.index[-1].strftime('%d-%m-%y')}, {action}, , ,{future_pred_prob:.4}\")\n",
        "\n",
        "\n",
        "# Guardar y descargar el CSV\n",
        "results_df.to_csv(f\"backtesting_results_{symbol}_{end_date.strftime('%Y-%m-%d')}.csv\", sep=\";\")\n",
        "files.download(f\"backtesting_results_{symbol}_{end_date.strftime('%Y-%m-%d')}.csv\")\n",
        "print(f\"\\nArchivo 'backtesting_results_{symbol}_{end_date.strftime('%Y-%m-%d')}.csv' generado y descargado.\")\n",
        "import time\n",
        "time.sleep(6)\n",
        "\n",
        "resultsp_df.to_csv(f\"Predic_results_{symbol}_{end_date.strftime('%Y-%m-%d')}.csv\", sep=\";\")\n",
        "files.download(f\"Predic_results_{symbol}_{end_date.strftime('%Y-%m-%d')}.csv\")\n",
        "print(f\"\\nArchivo 'Predic_results_{symbol}_{end_date.strftime('%Y-%m-%d')}.csv' generado y descargado.\")"
      ],
      "metadata": {
        "id": "OReUSMZapafA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YIHUxFvhthtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "COMIENZA EL UMBRAL"
      ],
      "metadata": {
        "id": "xjiSoEl0ijkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import datetime as dt\n",
        "\n",
        "# Descargar datos (si no lo hiciste aún)\n",
        "end_date = dt.datetime(2025, 7, 20)\n",
        "df = yf.download(\"COME.BA\", start=dt.datetime(2001, 1, 1), end=end_date)\n",
        "\n",
        "# Definir umbral (por ejemplo, 2%)\n",
        "umbral = 0.02\n",
        "\n",
        "# Calcular la diferencia relativa y etiquetar\n",
        "df['Label'] = ((df['High'] - df['Open']) / df['Open'] > umbral).astype(int) * 2 - 1\n",
        "\n",
        "# Contar etiquetas y calcular porcentajes\n",
        "label_counts = df['Label'].value_counts()\n",
        "total_dias = len(df)\n",
        "percentages = (label_counts / total_dias) * 100\n",
        "\n",
        "# Imprimir resultados\n",
        "print(\"Conteo de etiquetas:\")\n",
        "print(label_counts)\n",
        "print(\"\\nPorcentajes de cada clase (%):\")\n",
        "print(percentages.round(2))\n",
        "\n",
        "# Opcional: Ver los primeros días etiquetados\n",
        "print(\"\\nPrimeros días etiquetados:\")\n",
        "print(df[['Open', 'High', 'Label']].tail(20))\n",
        "print(\"mambral\")\n",
        "umbral = df['High'].sub(df['Open']).div(df['Open']).quantile(0.75)\n",
        "print(umbral)\n",
        "df['Label'] = ((df['High'] - df['Open']) / df['Open'] > umbral).astype(int) * 2 - 1\n",
        "print(df['Label'].value_counts(normalize=True) * 100)\n",
        "\n",
        "# Contar etiquetas y calcular porcentajes\n",
        "label_counts = df['Label'].value_counts()\n",
        "total_dias = len(df)\n",
        "percentages = (label_counts / total_dias) * 100\n",
        "\n",
        "# Imprimir resultados\n",
        "print(\"Conteo de etiquetas:\")\n",
        "print(label_counts)\n",
        "print(\"\\nPorcentajes de cada clase (%):\")\n",
        "print(percentages.round(2))"
      ],
      "metadata": {
        "id": "-ItKP7EgLl9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import datetime as dt\n",
        "\n",
        "# Descargar datos\n",
        "end_date = dt.datetime(2025, 7, 17)\n",
        "df = yf.download(\"GGAL\", start=dt.datetime(2001, 1, 1), end=end_date)\n",
        "\n",
        "# Aplanar el MultiIndex a columnas simples\n",
        "df.columns = df.columns.map(lambda x: x[0])\n",
        "\n",
        "# Limpiar datos\n",
        "df = df.dropna(subset=['Open', 'High'])\n",
        "\n",
        "# Calcular umbral dinámico\n",
        "differences = (df['High'] - df['Open']) / df['Open']\n",
        "umbral = differences.quantile(0.6).item()\n",
        "print(f\"Umbral calculado: {umbral:.4f}\")\n",
        "\n",
        "# Calcular etiqueta sin desfase (para verificar)\n",
        "df['Label_raw'] = ((df['High'] - df['Open']) / df['Open'] > umbral).astype(int) * 2 - 1\n",
        "\n",
        "# Desplazar la etiqueta un día hacia atrás (target del día siguiente)\n",
        "df['Label'] = df['Label_raw'].shift(-1)\n",
        "\n",
        "# Eliminar la última fila (no tiene etiqueta para predecir)\n",
        "df = df.dropna(subset=['Label'])\n",
        "\n",
        "# Contar etiquetas y calcular porcentajes\n",
        "label_counts = df['Label'].value_counts()\n",
        "total_dias = len(df)\n",
        "percentages = (label_counts / total_dias) * 100\n",
        "\n",
        "print(\"\\nConteo de etiquetas (desfasadas):\")\n",
        "print(label_counts)\n",
        "print(\"\\nPorcentajes de cada clase (%):\")\n",
        "print(percentages.round(2))\n",
        "\n",
        "# Opcional: Ver los primeros días etiquetados\n",
        "print(\"\\nPrimeros días etiquetados (features del día anterior, label del día siguiente):\")\n",
        "print(df[['Open', 'High', 'Label']].tail(22))"
      ],
      "metadata": {
        "id": "BYoBb-mnRDDT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}