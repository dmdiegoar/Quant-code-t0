{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmdiegoar/Quant-code-t0/blob/main/XGB_Sinsentido.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YIHUxFvhthtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "COMIENZA EL UMBRAL"
      ],
      "metadata": {
        "id": "xjiSoEl0ijkm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**apruebas**"
      ],
      "metadata": {
        "id": "j2LNBCdg3z79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, f1_score, precision_score, recall_score, make_scorer\n",
        "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
        "from xgboost import XGBClassifier\n",
        "from google.colab import files\n",
        "from scipy import stats\n",
        "import time\n",
        "from imblearn.over_sampling import SMOTE # Import SMOTE\n",
        "import matplotlib.pyplot as plt # Import for plotting (optional for this step)\n",
        "import seaborn as sns # Import for plotting (optional for this step)\n",
        "import os # Import os for creating directories\n",
        "\n",
        "\n",
        "# --- Control Flag ---\n",
        "run_full_model = True # Set to True to run model training, tuning, and prediction; Set to False to only run data/feature analysis\n",
        "debug_mode = False\n",
        "# --- End Control Flag ---\n",
        "\n",
        "\n",
        "# Functions for features\n",
        "def add_lagged_price_features(df, etiqueta=\"close_lag\", dato=\"Close\"):\n",
        "    for lag in range(1, 6):\n",
        "        df[f'{etiqueta}_{lag}'] = df[dato].shift(lag)\n",
        "    return df\n",
        "\n",
        "def calculate_RSI(series, period=7):\n",
        "    delta = series.diff(1)\n",
        "    gain = delta.where(delta > 0, 0).rolling(window=period).mean()\n",
        "    loss = -delta.where(delta < 0, 0).rolling(window=period).mean()\n",
        "    rs = gain / loss\n",
        "    return 100 - (100 / (1 + rs))\n",
        "\n",
        "def calculate_ROC(series, period=5):\n",
        "    return ((series - series.shift(period)) / series.shift(period)) * 100\n",
        "\n",
        "def calculate_PPO(series, fast_period=5, slow_period=9, signal_period=5):\n",
        "    ema_fast = series.ewm(span=fast_period, adjust=False).mean()\n",
        "    ema_slow = series.ewm(span=slow_period, adjust=False).mean()\n",
        "    ppo = (ema_fast - ema_slow) / ema_slow * 100\n",
        "    signal_line = ppo.ewm(span=signal_period, adjust=False).mean()\n",
        "    histogram = ppo - signal_line\n",
        "    return ppo, signal_line, histogram\n",
        "\n",
        "def calculate_EWO(series, fast_period=5, slow_period=35, signal_period=5):\n",
        "    ema_fast = series.ewm(span=fast_period, adjust=False).mean()\n",
        "    ema_slow = series.ewm(span=slow_period, adjust=False).mean()\n",
        "    ewo = (ema_fast - ema_slow) / ema_slow * 100\n",
        "    signal_line = ewo.ewm(span=signal_period, adjust=False).mean()\n",
        "    histogram = ewo - signal_line\n",
        "    return ewo, signal_line, histogram\n",
        "\n",
        "def calculate_volatility(series, window=20):\n",
        "    return series.rolling(window).std().round(6)\n",
        "\n",
        "def calculate_sma5(series, period=5):\n",
        "    return series.rolling(window=period).mean().round(4)\n",
        "\n",
        "def calculate_sma13(series, period=13):\n",
        "    return series.rolling(window=period).mean().round(4)\n",
        "\n",
        "def calculate_sma26(series, period=26):\n",
        "    return series.rolling(window=period).mean().round(4)\n",
        "\n",
        "def calculate_sma50(series, period=50):\n",
        "    return series.rolling(window=period).mean().round(4)\n",
        "\n",
        "def calculate_sma200(series, period=200):\n",
        "    return series.rolling(window=period).mean().round(4)\n",
        "\n",
        "def calculate_cyclical_price_position(df, price_col, window_size, prefix):\n",
        "    \"\"\"Calculates cyclical encoding of price position within a rolling window.\"\"\"\n",
        "    # Calculate rolling max and min based on the specific price column\n",
        "    rolling_max = df[price_col].rolling(window=window_size, min_periods=1).max()\n",
        "    rolling_min = df[price_col].rolling(window=window_size, min_periods=1).min()\n",
        "\n",
        "    # Calculate normalized position (handle division by zero if max == min)\n",
        "    price_range = rolling_max - rolling_min\n",
        "    # Use epsilon to avoid division by zero, but also consider the case where max == min\n",
        "    epsilon = 1e-9\n",
        "    # Add a small value to the denominator only if price_range is effectively zero\n",
        "    denominator = price_range + np.where(price_range < epsilon, epsilon, 0)\n",
        "    normalized_price = (df[price_col] - rolling_min) / denominator\n",
        "\n",
        "\n",
        "    # Apply cyclical transformation\n",
        "    df[f'{prefix}_{price_col.lower()}_pos_{window_size}d_sin'] = np.sin(2 * np.pi * normalized_price)\n",
        "    df[f'{prefix}_{price_col.lower()}_pos_{window_size}d_cos'] = np.cos(2 * np.pi * normalized_price)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def create_features(df, umbral, n_days_high=1):\n",
        "    df = add_lagged_price_features(df, \"close_lag\", \"Close\")\n",
        "    df = add_lagged_price_features(df, \"open_lag\", \"Open\")\n",
        "    df = add_lagged_price_features(df, \"high_lag\", \"High\")\n",
        "    df['Pct_change'] = df['Close'].pct_change()\n",
        "    for lag in range(1, 6):\n",
        "        df[f'lag_change{lag}'] = df['Pct_change'].shift(lag)\n",
        "    df['RSI'] = calculate_RSI(df['Close'])\n",
        "    df['ROC'] = calculate_ROC(df['Close'])\n",
        "    df['PPO'], df['PPO_Signal'], df['PPO_Histogram'] = calculate_PPO(df['Close'])\n",
        "    df['EWO'], df['EWO_Signal'], df['EWO_Histogram'] = calculate_EWO(df['Close'])\n",
        "    df['SMA5'] = calculate_sma5(df['Close'])\n",
        "    df['SMA13'] = calculate_sma13(df['Close'])\n",
        "    df['SMA26'] = calculate_sma26(df['Close'])\n",
        "    df['SMA50'] = calculate_sma50(df['Close'])\n",
        "    df['SMA200'] = calculate_sma200(df['Close'])\n",
        "    df['Volatility'] = calculate_volatility(df['Close'])\n",
        "\n",
        "    # --- New Feature: Max Gain from Open over Past N Days ---\n",
        "    # Calculate the maximum High price over the *next N days* for *each historical day*.\n",
        "    # Use rolling().max() with min_periods=1 to handle ends of series.\n",
        "    # Then shift to align with the start of the N-day window (the current day's Open).\n",
        "    max_high_over_next_n_days_hist = df['High'].rolling(window=n_days_high, min_periods=1).max().shift(-n_days_high + 1)\n",
        "\n",
        "\n",
        "    # Calculate the potential max gain from Open for *each historical day*\n",
        "    # Using the Open price of that historical day\n",
        "    epsilon = 1e-9 # To prevent division by zero\n",
        "    df['Max_Gain_from_Open_Current'] = (max_high_over_next_n_days_hist - df['Open']) / (df['Open'] + epsilon)\n",
        "\n",
        "    # --- Add lagged versions of the new feature ---\n",
        "    for lag in range(1, 7): # Create lags from 1 to 6\n",
        "        df[f'Max_Gain_from_Open_Lag_{lag}'] = df['Max_Gain_from_Open_Current'].shift(lag)\n",
        "\n",
        "    # --- Add Cyclical Features for Day of Week ---\n",
        "    # Day of week: Monday=0, Sunday=6\n",
        "    df['day_of_week'] = df.index.dayofweek\n",
        "    # Apply sine and cosine transformations\n",
        "    df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
        "    df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
        "    # Drop the original day_of_week column\n",
        "    df.drop('day_of_week', axis=1, inplace=True)\n",
        "\n",
        "    # --- Add Cyclical Features for Price Position within Rolling Ranges ---\n",
        "    window_sizes = [5, 20, 252] # 5d, 20d, approx 52 weeks\n",
        "    price_cols = ['Open', 'High', 'Low', 'Close']\n",
        "\n",
        "    for window_size in window_sizes:\n",
        "        for price_col in price_cols:\n",
        "             df = calculate_cyclical_price_position(df, price_col, window_size, prefix='price')\n",
        "\n",
        "\n",
        "    # Calculate the target based on tomorrow's Open vs Max High over next n_days_high days\n",
        "    # Use rolling().max() with min_periods=1 for the target as well.\n",
        "    # Shift to align with the start of the N-day window for the target (tomorrow's Open).\n",
        "    max_high_next_n_days_target = df['High'].rolling(window=n_days_high, min_periods=1).max().shift(-n_days_high + 1)\n",
        "    open_next_day = df['Open'].shift(-1)\n",
        "    df['Label_raw'] = ((max_high_next_n_days_target - open_next_day) / (open_next_day + epsilon) > umbral).astype(int)\n",
        "    df['Label'] = df['Label_raw'].shift(-1) # Target for the next day\n",
        "\n",
        "\n",
        "\n",
        "    original_index = df.index\n",
        "\n",
        "\n",
        "    return df\n",
        "\n",
        "# Definir fecha de corte manualmente (cambiar diariamente)\n",
        "end_date = dt.datetime(2025, 9, 14)  # Ejemplo: cambiar a 2025-07-18 mañana\n",
        "\n",
        "tk =[ \"ALUA.BA\", \"BBAR.BA\", \"BMA.BA\", \"COME.BA\", \"CRES.BA\", \"EDN.BA\", \"GGAL.BA\", \"IRSA.BA\", \"LOMA.BA\", \"METR.BA\", \"PAMP.BA\", \"SUPV.BA\", \"TECO2.BA\", \"TGNO4.BA\", \"TGSU2.BA\", \"TRAN.BA\", \"TXAR.BA\", \"VALO.BA\", \"YPFD.BA\"]\n",
        "\n",
        "\n",
        "# results = [] # el del test, para que no lo reinicie - REMOVED\n",
        "resultsp = [] # las predicciones, para que no lo reinicie\n",
        "\n",
        "umbral = 0.019\n",
        "lapso = 1 # Lapso is no longer directly used for the target definition, but keeping it doesn't hurt\n",
        "n_days_high_target = 1 # Define the number of days for the High target (used for both target and new feature)\n",
        "\n",
        "# Define clipping bounds - adjust based on feature distributions\n",
        "lower_bound = -1e9\n",
        "upper_bound = 1e9\n",
        "\n",
        "\n",
        "# Select features - Add the new feature and its lags\n",
        "features = ['RSI', 'ROC', 'PPO', 'PPO_Signal', 'PPO_Histogram', 'EWO', 'EWO_Signal', 'EWO_Histogram', 'Volatility', 'SMA5', 'SMA13', 'SMA26', 'SMA50', 'SMA200' ] + [f'lag_change{i}' for i in range(1, 6)] + \\\n",
        "          [f'close_lag_{i}' for i in range(1, 6)] + [f'open_lag_{i}' for i in range(1, 6)]+ [f'high_lag_{i}' for i in range(1, 6)] + \\\n",
        "          ['Max_Gain_from_Open_Current'] + [f'Max_Gain_from_Open_Lag_{i}' for i in range(1, 7)] + \\\n",
        "          ['day_of_week_sin', 'day_of_week_cos']\n",
        "\n",
        "# Add all the new cyclical price position features\n",
        "window_sizes = [5, 20, 252]\n",
        "price_cols = ['Open', 'High', 'Low', 'Close']\n",
        "for window_size in window_sizes:\n",
        "    for price_col in price_cols:\n",
        "        features.append(f'price_{price_col.lower()}_pos_{window_size}d_sin')\n",
        "        features.append(f'price_{price_col.lower()}_pos_{window_size}d_cos')\n",
        "\n",
        "\n",
        "for papel in tk:\n",
        "\n",
        "  symbol=papel\n",
        "  #symbol=\"COME.BA\"\n",
        "  # Fechas dinámicas\n",
        "  start_date = dt.datetime(2001, 1, 1)  # Inicio fijo\n",
        "  train_end = end_date - pd.Timedelta(days=780)  # 6 meses antes de end_date (ajustable)\n",
        "  next_day = end_date + pd.Timedelta(days=1)  # Predicción para el día siguiente\n",
        "\n",
        "\n",
        "  # Download data for the current ticker inside the loop\n",
        "  print(f\"\\nDownloading data for {symbol}...\")\n",
        "  # Download data up to the day BEFORE the prediction date (end_date)\n",
        "  # Corrected: Download data up to end_date. The last row of this data will be end_date.\n",
        "  df = yf.download(symbol, start=start_date, end=end_date, auto_adjust=False)\n",
        "\n",
        "  # Verify data download\n",
        "  if df.empty:\n",
        "      print(f\"Warning: No data downloaded for {symbol}. Skipping.\")\n",
        "      continue # Skip to the next ticker\n",
        "\n",
        "\n",
        "  # Handle MultiIndex columns and ensure standard column names - More robust logic\n",
        "  required_cols = ['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']\n",
        "  processed_df = None # Initialize processed_df\n",
        "\n",
        "  if isinstance(df.columns, pd.MultiIndex):\n",
        "      print(f\"MultiIndex columns detected for {symbol}.\")\n",
        "      try:\n",
        "          # Attempt to extract columns by looking for standard names in ANY level of the MultiIndex tuple\n",
        "          extracted_data = {}\n",
        "          for std_name in required_cols:\n",
        "              matching_col_tuple = None\n",
        "              # Iterate through all column tuples\n",
        "              for col_tuple in df.columns:\n",
        "                  # Check if the standard name exists in ANY level of the current tuple\n",
        "                  if std_name in col_tuple:\n",
        "                       matching_col_tuple = col_tuple\n",
        "                       break # Found a match in this tuple\n",
        "\n",
        "              if matching_col_tuple:\n",
        "                  extracted_data[std_name] = df[matching_col_tuple]\n",
        "              else:\n",
        "                  print(f\"Warning: Could not find standard column '{std_name}' in any level of MultiIndex for {symbol}. Column missing.\")\n",
        "                  # Continue to look for other required columns, processed_df will be checked later\n",
        "\n",
        "          if len(extracted_data) == len(required_cols):\n",
        "              processed_df = pd.DataFrame(extracted_data)\n",
        "              processed_df.index = df.index # Preserve original index\n",
        "              print(f\"Successfully extracted and flattened MultiIndex columns for {symbol}.\")\n",
        "          else:\n",
        "              missing_cols = [name for name in required_cols if name not in extracted_data]\n",
        "              print(f\"Warning: Could not extract all required columns from MultiIndex for {symbol}. Missing: {missing_cols}. Skipping ticker.\")\n",
        "              continue # Skip to the next ticker\n",
        "\n",
        "      except Exception as e:\n",
        "          print(f\"Warning: An error occurred while processing MultiIndex columns for {symbol}: {e}. Skipping.\")\n",
        "          #print(f\"Original columns: {df.columns.tolist()}\")\n",
        "          continue # Skip to the next ticker\n",
        "\n",
        "  else: # If not MultiIndex columns, assume standard flat DataFrame is already present\n",
        "      print(f\"No MultiIndex columns detected for {symbol}. Checking for standard columns.\")\n",
        "      # Check if the required columns are directly present\n",
        "      if all(col in df.columns for col in required_cols):\n",
        "          processed_df = df[required_cols].copy() # Select required columns and make a copy\n",
        "          print(f\"Using existing standard columns for {symbol}.\")\n",
        "      else:\n",
        "          missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "          print(f\"Warning: Required standard columns not found in flat DataFrame for {symbol}. Missing: {missing_cols}. Skipping ticker.\")\n",
        "          #print(f\"Available columns: {df.columns.tolist()}\")\n",
        "          continue # Skip to the next ticker\n",
        "\n",
        "  # Ensure df is set to processed_df if successful\n",
        "  df = processed_df\n",
        "\n",
        "  # Handle MultiIndex index if present (less common with single ticker download but possible)\n",
        "  if isinstance(df.index, pd.MultiIndex):\n",
        "      print(f\"MultiIndex index detected for {symbol}. Attempting to flatten index.\")\n",
        "      try:\n",
        "          # Assuming the MultiIndex index structure is ('Ticker', 'Date')\n",
        "          if 'Ticker' in df.index.names:\n",
        "               df = df.xs(symbol, level='Ticker', axis=0)\n",
        "               print(f\"Índice aplanado para {symbol}.\")\n",
        "          else:\n",
        "               print(f\"Warning: MultiIndex index detected for {symbol} but 'Ticker' level not found. Skipping index flattening.\")\n",
        "               # If 'Ticker' level is not there, maybe it's just a date/time MultiIndex?\n",
        "               # Or a different structure. For now, proceed without flattening index if Ticker level is missing.\n",
        "\n",
        "\n",
        "      except KeyError:\n",
        "          print(f\"Warning: Could not select ticker from MultiIndex index for {symbol}. Skipping.\")\n",
        "          continue # Skip to the next ticker\n",
        "      except Exception as e:\n",
        "          print(f\"Warning: An error occurred while flattening MultiIndex index for {symbol}: {e}. Skipping.\")\n",
        "          continue # Skip to the next ticker\n",
        "\n",
        "\n",
        "  df.index = pd.to_datetime(df.index)\n",
        "  if not df.index.is_unique:\n",
        "      print(f\"Advertencia: Índice con fechas duplicadas para {symbol}. Eliminando duplicados...\")\n",
        "      df = df[~df.index.duplicated(keep='first')]\n",
        "\n",
        "  if df.empty:\n",
        "      print(f\"Warning: DataFrame is empty after initial processing and cleaning for {symbol}. Skipping.\")\n",
        "      continue\n",
        "\n",
        "\n",
        "  # Ensure numeric types and handle potential non-numeric data\n",
        "  for col in required_cols:\n",
        "      if col in df.columns: # Ensure column exists before processing\n",
        "          df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "      else:\n",
        "           # This should ideally not happen if previous checks passed, but as a safeguard:\n",
        "           print(f\"Error: Required column '{col}' not found in df for {symbol} before numeric conversion. Skipping ticker.\")\n",
        "           df = pd.DataFrame() # Set df to empty to skip further processing\n",
        "           break # Exit column processing loop\n",
        "\n",
        "\n",
        "  if df.empty: # Check again if df became empty due to missing columns\n",
        "       continue # Skip to the next ticker\n",
        "\n",
        "  # Drop rows where essential price data is missing after coercion\n",
        "  df.dropna(subset=['Open', 'High', 'Low', 'Close'], inplace=True)\n",
        "\n",
        "\n",
        "  if df.empty:\n",
        "      print(f\"Warning: DataFrame is empty after dropping rows with missing price data for {symbol}. Skipping.\")\n",
        "      continue\n",
        "\n",
        "\n",
        "  df['Open']= df['Open'].round(2)\n",
        "  df['High']= df['High'].round(2)\n",
        "  df['Low']= df['Low'].round(2)\n",
        "  df['Close']= df['Close'].round(2)\n",
        "  df['Adj Close']= df['Adj Close'].round(2)\n",
        "\n",
        "  print(\"Últimas filas del DataFrame antes de crear features:\")\n",
        "  print(df.tail())\n",
        "\n",
        "\n",
        "  # Crear features with the new target definition\n",
        "  df = create_features(df, umbral=umbral, n_days_high=n_days_high_target) # Pass n_days_high_target, removed lapso\n",
        "\n",
        "  # Verify data is not empty after feature creation and dropna\n",
        "  if df.empty:\n",
        "      print(f\"Warning: DataFrame is empty after feature creation and dropna for {symbol}. Skipping.\")\n",
        "      continue\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # The last row of the DataFrame is used for the final prediction\n",
        "  # This row contains features calculated using data up to the last available date (end_date)\n",
        "  # Corrected: Ensure X_predict_latest is taken from the *end* of the DataFrame AFTER feature creation and dropna\n",
        "  X_predict_latest = df[features].iloc[-1:] # Get the very last row for prediction\n",
        "\n",
        "  print(X_predict_latest)\n",
        "  df.dropna(inplace=True)\n",
        "  # Ensure y_train_full and y_test are aligned with the DataFrame index after dropping NaNs\n",
        "  # This alignment will be done later when splitting the data\n",
        "# Verify data after creating features\n",
        "  print(f\"\\nÚltimas filas del DataFrame después de crear features:\")\n",
        "  print(df.tail())\n",
        "  print(df.columns)\n",
        "\n",
        "  time.sleep(2)\n",
        "\n",
        "  # --- Add Historical Target Distribution Check ---\n",
        "  print(f\"\\nOverall historical target distribution for {symbol} (before train/test split):\")\n",
        "  historical_target_distribution = df[\"Label\"].value_counts(normalize=True)\n",
        "  print(historical_target_distribution)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  if debug_mode:\n",
        "\n",
        "    print(f\"\\nDEBUG: Reached feature distribution table analysis section for {symbol}.\")\n",
        "    print(f\"Generating feature distribution analysis table for {symbol} by target class...\")\n",
        "\n",
        "    # Use the full historical data (after feature creation and dropna) for this analysis\n",
        "    analysis_df = df[features + ['Label']].copy() # Use all features for table analysis\n",
        "\n",
        "    if analysis_df.empty:\n",
        "        print(f\"Warning: DataFrame is empty for feature distribution analysis for {symbol}.\")\n",
        "    elif 'Label' not in analysis_df.columns:\n",
        "        print(f\"Error: 'Label' column not found in DataFrame for feature distribution analysis for {symbol}.\")\n",
        "    elif len(analysis_df['Label'].unique()) < 2:\n",
        "        print(f\"Warning: Only one class exists in 'Label' for feature distribution analysis for {symbol}. Cannot group by class.\")\n",
        "        # Even if only one class, we can still show the describe table for that class\n",
        "        print(f\"\\nFeature Distribution Analysis Table for {symbol} (Only one class):\")\n",
        "        display(analysis_df.describe().transpose()) # Show describe for the single class\n",
        "    else: # This else should be aligned with the if/elif above\n",
        "        # Group by Label and calculate descriptive statistics for all features\n",
        "        feature_distribution_table = analysis_df.groupby('Label').describe().transpose()\n",
        "\n",
        "        print(f\"\\nFeature Distribution Analysis Table for {symbol} by Target Class:\")\n",
        "        display(feature_distribution_table) # Use display for better formatting\n",
        "\n",
        "    print(f\"Finished generating feature distribution analysis table for {symbol}.\") # This print should be aligned with the if/elif/else block\n",
        "    # --- End Feature Distribution Analysis Table ---\n",
        "\n",
        "\n",
        "  # --- Start of block for full model run (training, tuning, prediction, results table) ---\n",
        "  if run_full_model:\n",
        "      # Dividir datos en entrenamiento y prueba (moved inside the if run_full_model block)\n",
        "      # X and y now include the latest data up to end_date + 1 day before splitting\n",
        "      X = df[features]\n",
        "      y = df['Label']\n",
        "\n",
        "      # Train data is up to train_end (exclusive of train_end + 1)\n",
        "      X_train_full = X[df.index <= train_end]\n",
        "      y_train_full = y[df.index <= train_end]\n",
        "\n",
        "      # Test data is from train_end + 1 day up to end_date (inclusive)\n",
        "      X_test = X[(df.index > train_end) & (df.index <= end_date)]\n",
        "      y_test = y[(df.index > train_end) & (df.index <= end_date)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      # Check if original training data is sufficient BEFORE proceeding\n",
        "      if not X_train_full.empty and not y_train_full.empty and len(y_train_full.unique()) > 1:\n",
        "\n",
        "          # Ensure all features are present in X_train_full before calculating correlation or training\n",
        "          missing_features_train = [f for f in features if f not in X_train_full.columns]\n",
        "          if missing_features_train:\n",
        "              print(f\"ERROR: Missing features in training data for {symbol}: {missing_features_train}. Skipping model training and prediction.\")\n",
        "              # Append entry indicating skip due to missing features\n",
        "              last_data_date = df.index[-1] if not df.empty else None\n",
        "              last_close = df['Close'].iloc[-1] if not df.empty and 'Close' in df.columns else None\n",
        "              resultsp.append({\n",
        "                   'Papel': symbol,\n",
        "                   'Fecha Predicción': next_day,\n",
        "                   'Fecha Datos': last_data_date,\n",
        "                   'Predicción': f'Skipped (Missing Features in Train Data: {\", \".join(missing_features_train)})',\n",
        "                   'Precio actual': last_close,\n",
        "                   'Probabilidad Alcista (Modelo)': None,\n",
        "                   'Umbral de Clasificación': None,\n",
        "                   'Mejores hiperparámetros (Incluye scale_pos_weight)': 'Skipped (Missing Features in Train Data)',\n",
        "                   'Precision Test (Alcista)': None,\n",
        "                   'Recall Test (Alcista)': None,\n",
        "                   'F1 Test (Alcista)': None,\n",
        "                   'ROC-AUC Test': None,\n",
        "                   'clase 1 en test (cleaned)': None,\n",
        "                   'Features Limpias (Predicción)': None,\n",
        "                   'Features Escaladas (Predicción)': None\n",
        "              })\n",
        "              continue # Skip to the next ticker\n",
        "\n",
        "\n",
        "          correlation = df[features + [\"Label\"]].corr()[\"Label\"].sort_values(ascending=False)\n",
        "          print(f\"Correlacion con label para {symbol}:\")\n",
        "          print(correlation)\n",
        "\n",
        "          # Initialize test metrics before evaluation (only if model runs)\n",
        "          precision_test_alcista = None\n",
        "          recall_test_alcista = None\n",
        "          f1_test_alcista = None\n",
        "          roc_auc_test = None\n",
        "          ratio_1_test = None\n",
        "          best_model = None # Initialize best_model to None\n",
        "          best_threshold = 0.5 # Initialize best_threshold to default\n",
        "\n",
        "\n",
        "          # Optimizar hiperparámetros con RandomizedSearchCV\n",
        "          print(f\"Optimizar hiperparámetros con RandomizedSearchCV para {symbol}\")\n",
        "          param_dist = {\n",
        "              'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "              'max_depth': [3, 5, 7, 9],\n",
        "              'n_estimators': [100, 500, 900],\n",
        "              'subsample': [0.6, 0.8, 1.0],\n",
        "              'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "              'gamma': [0, 0.1, 0.2],\n",
        "              'scale_pos_weight': [0.5, 1, 2, 5, 10, 20, 50, 100] # Incluir scale_pos_weight en la búsqueda\n",
        "          }\n",
        "\n",
        "          # Inicializar el clasificador XGBoost sin scale_pos_weight fijo (se tuneará)\n",
        "          xgb = XGBClassifier(objective='binary:logistic', random_state=42)\n",
        "\n",
        "          # Usar TimeSeriesSplit para cross-validation\n",
        "          n_splits = 5  # Puedes ajustar el número de splits\n",
        "          tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "          # Definir scorer para maximizar Precision de la Clase 1 (for RandomizedSearchCV)\n",
        "          precision_scorer = make_scorer(precision_score, pos_label=1, zero_division=0) # zero_division=0 para manejar casos sin predicciones positivas\n",
        "\n",
        "          # Clean X_train_full and y_train_full before fitting RandomizedSearchCV and SMOTE\n",
        "          X_train_full_cleaned_for_tuning = X_train_full.replace([np.inf, -np.inf], np.nan)\n",
        "          X_train_full_cleaned_for_tuning.dropna(inplace=True)\n",
        "          y_train_full_cleaned_for_tuning = y_train_full.loc[X_train_full_cleaned_for_tuning.index] # Ensure y matches cleaned X\n",
        "\n",
        "          # Explicit check, conversion, and fallback for non-finite values before fitting RandomizedSearchCV\n",
        "          X_train_full_cleaned_for_tuning = X_train_full_cleaned_for_tuning.astype(np.float64) # Ensure dtype\n",
        "          if not np.isfinite(X_train_full_cleaned_for_tuning).all().all():\n",
        "              print(f\"\\nWarning: Non-finite values detected in X_train_full_cleaned_for_tuning for {symbol} before RandomizedSearchCV fit. Attempting to fill with median.\")\n",
        "              for col in X_train_full_cleaned_for_tuning.columns:\n",
        "                  finite_values = X_train_full_cleaned_for_tuning[col][np.isfinite(X_train_full_cleaned_for_tuning[col])]\n",
        "                  if not finite_values.empty:\n",
        "                      median_val = finite_values.median()\n",
        "                      X_train_full_cleaned_for_tuning[col].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "                      X_train_full_cleaned_for_tuning[col].fillna(median_val, inplace=True)\n",
        "                  else:\n",
        "                      print(f\"Warning: Column '{col}' in X_train_full_cleaned_for_tuning is all non-finite. Filling with 0.\")\n",
        "                      X_train_full_cleaned_for_tuning[col].fillna(0, inplace=True)\n",
        "\n",
        "          # --- Apply SMOTE to the training data ---\n",
        "          X_train_res, y_train_res = X_train_full_cleaned_for_tuning.copy(), y_train_full_cleaned_for_tuning.copy() # Initialize with cleaned data\n",
        "          # Check if resampling is needed and possible\n",
        "          if len(y_train_full_cleaned_for_tuning.unique()) > 1 and y_train_full_cleaned_for_tuning.value_counts().min() > 1: # SMOTE needs at least 2 samples of the minority class\n",
        "              try:\n",
        "                  print(f\"\\nApplying SMOTE to training data for {symbol}...\")\n",
        "                  sm = SMOTE(random_state=42)\n",
        "                  X_train_res, y_train_res = sm.fit_resample(X_train_full_cleaned_for_tuning, y_train_full_cleaned_for_tuning)\n",
        "                  print(f\"Original training data shape: {X_train_full_cleaned_for_tuning.shape}, Resampled shape: {X_train_res.shape}\")\n",
        "                  print(f\"Original training target distribution: {y_train_full_cleaned_for_tuning.value_counts()}, Resampled target distribution: {y_train_res.value_counts()}\")\n",
        "              except Exception as e:\n",
        "                  print(f\"\\nWarning: Could not apply SMOTE to training data for {symbol}: {e}. Proceeding with original imbalanced training data.\")\n",
        "                  # X_train_res and y_train_res remain the original cleaned training data\n",
        "          elif len(y_train_full_cleaned_for_tuning.unique()) == 1:\n",
        "               print(f\"\\nWarning: Only one class in training data for {symbol} after cleaning. Cannot apply SMOTE.\")\n",
        "               # X_train_res and y_train_res remain the original cleaned training data\n",
        "          else: # Not enough samples for SMOTE (e.g., only 1 minority sample)\n",
        "              print(f\"\\nWarning: Not enough samples in minority class for SMOTE for {symbol}. Proceeding with original imbalanced training data.\")\n",
        "              # X_train_res and y_train_res remain the original cleaned training data\n",
        "\n",
        "\n",
        "          # Check if resampled data is sufficient for tuning and evaluation\n",
        "          if not X_train_res.empty and not y_train_res.empty and len(y_train_res.unique()) > 1:\n",
        "              # Perform tuning, evaluation, and prediction within a general try-except block\n",
        "              # to prevent script crash on problematic tickers\n",
        "              try:\n",
        "                  # Fit RandomizedSearchCV on the resampled training data\n",
        "                  random_search = RandomizedSearchCV(xgb, param_distributions=param_dist, n_iter=20, cv=tscv, scoring=precision_scorer, n_jobs=-1, random_state=42) # Usar precision_scorer\n",
        "                  random_search.fit(X_train_res, y_train_res) # Fit on resampled data\n",
        "                  print(f\"Mejores hiperparámetros para {symbol}:\", random_search.best_params_)\n",
        "\n",
        "                  # Usar el mejor modelo encontrado por RandomizedSearchCV\n",
        "                  best_model = random_search.best_estimator_\n",
        "\n",
        "                  # Ensure all features are present in the fitted model before calculating importance\n",
        "                  model_features = list(best_model.get_booster().feature_names)\n",
        "                  missing_features_model = [f for f in features if f not in model_features]\n",
        "                  if missing_features_model:\n",
        "                       print(f\"Warning: Missing features in fitted model for {symbol}: {missing_features_model}. Feature importance will only be shown for available features.\")\n",
        "                       # Update the features list to only include features the model was trained on\n",
        "                       # This prevents errors later when trying to select features for prediction/evaluation\n",
        "                       features_for_prediction = model_features\n",
        "                  else:\n",
        "                       features_for_prediction = features # Use the full list if all were trained\n",
        "\n",
        "\n",
        "                  # Get and print feature importance (fitted on resampled data) - Use available features\n",
        "                  feature_importance_dict = best_model.get_booster().get_score(importance_type='weight')\n",
        "                  feature_importance = pd.Series(feature_importance_dict).sort_values(ascending=False)\n",
        "                  print(f\"\\nFeature Importance for {symbol}:\")\n",
        "                  print(feature_importance)\n",
        "\n",
        "\n",
        "                  # Optimize the threshold for maximum F1-score on the full original training set (NOT resampled)\n",
        "                  # Threshold optimization should reflect real-world data distribution\n",
        "                  X_train_full_cleaned_for_threshold = X_train_full.replace([np.inf, -np.inf], np.nan)\n",
        "                  X_train_full_cleaned_for_threshold.dropna(inplace=True)\n",
        "                  y_train_full_cleaned_for_threshold = y_train_full.loc[X_train_full_cleaned_for_threshold.index]\n",
        "\n",
        "                  # Ensure features used for threshold optimization match model features\n",
        "                  X_train_full_cleaned_for_threshold = X_train_full_cleaned_for_threshold[features_for_prediction]\n",
        "\n",
        "\n",
        "                  if not X_train_full_cleaned_for_threshold.empty and not y_train_full_cleaned_for_threshold.empty and len(y_train_full_cleaned_for_threshold.unique()) > 1:\n",
        "                      # Predict probabilities on the ORIGINAL cleaned training data for threshold optimization\n",
        "                      y_train_prob = best_model.predict_proba(X_train_full_cleaned_for_threshold)[:, 1]\n",
        "                      thresholds = np.arange(0.5, 0.9, 0.1)\n",
        "                      best_threshold = 0.5\n",
        "                      best_f1 = 0 # Changed from best_precision to best_f1\n",
        "\n",
        "                      print(f\"Optimizing threshold for maximum F1-score on training data (ORIGINAL distribution) for {symbol}...\") # Updated message\n",
        "                      # Calculate F1-score for each threshold\n",
        "                      for threshold in thresholds:\n",
        "                          y_pred_threshold = (y_train_prob >= threshold).astype(int)\n",
        "                          # Calculate f1_score (handles zero_division internally based on scikit-learn version)\n",
        "                          f1 = f1_score(y_train_full_cleaned_for_threshold, y_pred_threshold, zero_division=0)\n",
        "                          if f1 > best_f1:\n",
        "                              best_f1 = f1\n",
        "                              best_threshold = threshold\n",
        "\n",
        "                      print(f\"Mejor umbral para maximizar F1-score en entrenamiento (ORIGINAL distribution) para {symbol}: {best_threshold:.4f} (F1-score: {best_f1:.4f})\") # Updated message and metric\n",
        "                  else:\n",
        "                      print(f\"\\nWarning: Original training set for {symbol} contains no samples or only one class after cleaning for threshold optimization. Cannot optimize threshold for F1-score. Using default threshold 0.5.\") # Updated message\n",
        "                      best_threshold = 0.5\n",
        "\n",
        "\n",
        "                  # Evaluar el modelo en el conjunto de prueba con el best threshold (Test set is NOT resampled)\n",
        "                  if not X_test.empty and not y_test.empty and len(y_test.unique()) > 1:\n",
        "                      print(f\"\\nEvaluating best model on test set for {symbol} with best threshold ({best_threshold:.4f}):\")\n",
        "\n",
        "                      # Clean X_test before evaluation\n",
        "                      X_test_cleaned = X_test.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "                      y_test_cleaned = y_test.loc[X_test_cleaned.index]\n",
        "\n",
        "                      # Ensure features used for evaluation match model features\n",
        "                      missing_features_test = [f for f in features_for_prediction if f not in X_test_cleaned.columns]\n",
        "                      if missing_features_test:\n",
        "                           print(f\"ERROR: Missing features in test data for {symbol} that are present in the model: {missing_features_test}. Skipping test evaluation.\")\n",
        "                           precision_test_alcista = None\n",
        "                           recall_test_alcista = None\n",
        "                           f1_test_alcista = None\n",
        "                           roc_auc_test = None\n",
        "                           ratio_1_test = None\n",
        "                           # Skip the rest of the evaluation for this ticker\n",
        "                           continue # Skip to the next ticker\n",
        "\n",
        "                      X_test_cleaned = X_test_cleaned[features_for_prediction] # Select only the features the model was trained on\n",
        "\n",
        "\n",
        "                      # Scale X_test using the scaler fitted on the ORIGINAL training data\n",
        "                      scaler = RobustScaler()\n",
        "                      # Fit scaler on the ORIGINAL cleaned training data (NOT resampled)\n",
        "                      X_train_full_cleaned_for_scaler_eval = X_train_full.replace([np.inf, -np.inf], np.nan)\n",
        "                      X_train_full_cleaned_for_scaler_eval.dropna(inplace=True)\n",
        "                      X_train_full_cleaned_for_scaler_eval = X_train_full_cleaned_for_scaler_eval.clip(lower=lower_bound, upper=upper_bound)\n",
        "                      X_train_full_cleaned_for_scaler_eval = X_train_full_cleaned_for_scaler_eval.astype(np.float64)\n",
        "                      # Ensure features for scaler fitting match model features\n",
        "                      X_train_full_cleaned_for_scaler_eval = X_train_full_cleaned_for_scaler_eval[features_for_prediction]\n",
        "\n",
        "\n",
        "                      if not X_train_full_cleaned_for_scaler_eval.empty and np.isfinite(X_train_full_cleaned_for_scaler_eval).all().all():\n",
        "                          scaler.fit(X_train_full_cleaned_for_scaler_eval)\n",
        "\n",
        "                          X_test_cleaned = X_test_cleaned.astype(np.float64)\n",
        "                          if not np.isfinite(X_test_cleaned).all().all():\n",
        "                               print(f\"\\nWarning: Non-finite values detected in X_test_cleaned for {symbol} before scaler transform. Attempting to fill with median (from train data).\")\n",
        "                               train_medians = X_train_full_cleaned_for_scaler_eval.median()\n",
        "                               for col in X_test_cleaned.columns:\n",
        "                                   median_val = train_medians.get(col, 0)\n",
        "                                   X_test_cleaned[col].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "                                   X_test_cleaned[col].fillna(median_val, inplace=True)\n",
        "                               if not np.isfinite(X_test_cleaned).all().all():\n",
        "                                    print(f\"\\nERROR: Non-finite values STILL detected in X_test_cleaned for {symbol} after filling with median!\")\n",
        "\n",
        "\n",
        "                          if not X_test_cleaned.empty and np.isfinite(X_test_cleaned).all().all():\n",
        "                              X_test_scaled = scaler.transform(X_test_cleaned)\n",
        "\n",
        "                              y_test_pred_prob = best_model.predict_proba(X_test_scaled)[:, 1]\n",
        "                              y_test_pred = (y_test_pred_prob >= best_threshold).astype(int)\n",
        "\n",
        "                              if len(y_test_cleaned.unique()) > 1:\n",
        "                                  print(f\"\\nClassification Report (Test Set) for {symbol}:\")\n",
        "                                  print(classification_report(y_test_cleaned, y_test_pred, zero_division=0))\n",
        "                                  precision_test_alcista = precision_score(y_test_cleaned, y_test_pred, pos_label=1, zero_division=0)\n",
        "                                  recall_test_alcista = recall_score(y_test_cleaned, y_test_pred, pos_label=1, zero_division=0)\n",
        "                                  f1_test_alcista = f1_score(y_test_cleaned, y_test_pred, pos_label=1, zero_division=0)\n",
        "\n",
        "                                  print(f\"Tamaño de y_test (cleaned): {y_test_cleaned.size}\")\n",
        "                                  print(f\"Distribución de clases en y_test (cleaned) para {symbol}:\")\n",
        "                                  print(y_test_cleaned.value_counts())\n",
        "                                  if 1 in y_test_cleaned.value_counts():\n",
        "                                      ratio_1_test=(y_test_cleaned.value_counts()[1]/y_test_cleaned.size).round(4)\n",
        "                                  else:\n",
        "                                      ratio_1_test = 0\n",
        "                                  print(f\"% clase 1 test para {symbol}: {ratio_1_test} \")\n",
        "\n",
        "                                  if len(y_test_cleaned.unique()) > 1:\n",
        "                                       roc_auc_test = roc_auc_score(y_test_cleaned, y_test_pred_prob).round(6)\n",
        "                                       print(f\"\\nROC-AUC (Test Set) para {symbol}: {roc_auc_test:.4f}\")\n",
        "                                  else:\n",
        "                                       roc_auc_test = None\n",
        "                                       print(f\"\\nWarning: Test set for {symbol} contains only one class after cleaning. Cannot calculate ROC-AUC.\")\n",
        "\n",
        "                              else:\n",
        "                                  print(f\"\\nWarning: Test set for {symbol} contains only one class after cleaning. Cannot generate full classification report.\")\n",
        "                                  precision_test_alcista = None\n",
        "                                  recall_test_alcista = None\n",
        "                                  f1_test_alcista = None\n",
        "                                  roc_auc_test = None\n",
        "                                  ratio_1_test = None\n",
        "\n",
        "\n",
        "                          else:\n",
        "                              print(f\"\\nWarning: X_test became empty after cleaning or contains non-finite values for {symbol}. Skipping test evaluation.\")\n",
        "                              precision_test_alcista = None\n",
        "                              recall_test_alcista = None\n",
        "                              f1_test_alcista = None\n",
        "                              roc_auc_test = None\n",
        "                              ratio_1_test = None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                      else:\n",
        "                          print(f\"\\nAdvertencia: Conjunto de prueba insuficiente o con una sola clase para evaluación para {symbol}.\")\n",
        "                          precision_test_alcista = None\n",
        "                          recall_test_alcista = None\n",
        "                          f1_test_alcista = None\n",
        "                          roc_auc_test = None\n",
        "                          ratio_1_test = None\n",
        "\n",
        "                      # Prediction for the next day is done only if model was trained successfully\n",
        "                      # Use the single last row from the full DataFrame for prediction\n",
        "                      last_features = X_predict_latest.copy() # Use the pre-selected last row\n",
        "\n",
        "                      last_features_cleaned = None\n",
        "                      last_features_scaled = None\n",
        "                      future_pred_prob = None\n",
        "                      future_pred = None\n",
        "\n",
        "                      if not last_features.empty:\n",
        "                          # Ensure last_features is a single row DataFrame before cleaning\n",
        "                          if not isinstance(last_features, pd.DataFrame) or len(last_features) != 1:\n",
        "                               print(f\"\\nError: last_features is not a single row DataFrame for {symbol}. Skipping prediction.\")\n",
        "                               # Set prediction results to skipped\n",
        "                               last_data_date = df.index[-1] if not df.empty else None\n",
        "                               last_close = df['Close'].iloc[-1] if not df.empty and 'Close' in df.columns else None\n",
        "                               resultsp.append({\n",
        "                                   'Papel': symbol,\n",
        "                                   'Fecha Predicción': next_day,\n",
        "                                   'Fecha Datos': last_data_date,\n",
        "                                   'Predicción': 'Skipped (Prediction Data Error)',\n",
        "                                   'Precio actual': last_close,\n",
        "                                   'Probabilidad Alcista (Modelo)': None,\n",
        "                                   'Umbral de Clasificación': None,\n",
        "                                   'Mejores hiperparámetros (Incluye scale_pos_weight)': str(random_search.best_params_) if best_model is not None else 'Skipped (Tuning Error)',\n",
        "                                   'Precision Test (Alcista)': precision_test_alcista,\n",
        "                                   'Recall Test (Alcista)': recall_test_alcista,\n",
        "                                   'F1 Test (Alcista)': f1_test_alcista,\n",
        "                                   'ROC-AUC Test': roc_auc_test,\n",
        "                                   'clase 1 en test (cleaned)': ratio_1_test,\n",
        "                                   'Features Limpias (Predicción)': None,\n",
        "                                   'Features Escaladas (Predicción)': None\n",
        "                               })\n",
        "                               # No need to continue here, the append is done and we move to the next ticker\n",
        "                               continue # Skip to the next ticker\n",
        "\n",
        "\n",
        "                          last_features_cleaned = last_features.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "                          last_features_cleaned = last_features_cleaned.clip(lower=lower_bound, upper=upper_bound)\n",
        "\n",
        "                          # Ensure features for prediction match model features\n",
        "                          missing_features_pred = [f for f in features_for_prediction if f not in last_features_cleaned.columns]\n",
        "                          if missing_features_pred:\n",
        "                               print(f\"ERROR: Missing features in prediction data for {symbol} that are present in the model: {missing_features_pred}. Skipping prediction.\")\n",
        "                               last_data_date = df.index[-1] if not df.empty else None\n",
        "                               last_close = df['Close'].iloc[-1] if not df.empty and 'Close' in df.columns else None\n",
        "                               # Explicitly use None for feature columns if prediction skipped due to data issues\n",
        "                               resultsp.append({\n",
        "                                   'Papel': symbol,\n",
        "                                   'Fecha Predicción': next_day,\n",
        "                                   'Fecha Datos': last_data_date,\n",
        "                                   'Predicción': f'Skipped (Missing Features in Prediction Data: {\", \".join(missing_features_pred)})',\n",
        "                                   'Precio actual': last_close,\n",
        "                                   'Probabilidad Alcista (Modelo)': None,\n",
        "                                   'Umbral de Clasificación': None,\n",
        "                                   'Mejores hiperparámetros (Incluye scale_pos_weight)': str(random_search.best_params_) if best_model is not None else 'Skipped (Tuning Error)',\n",
        "                                   'Precision Test (Alcista)': precision_test_alcista,\n",
        "                                   'Recall Test (Alcista)': recall_test_alcista,\n",
        "                                   'F1 Test (Alcista)': f1_test_alcista,\n",
        "                                   'ROC-AUC Test': roc_auc_test,\n",
        "                                   'clase 1 en test (cleaned)': ratio_1_test,\n",
        "                                    'Features Limpias (Predicción)': last_features_cleaned.to_dict() if last_features_cleaned is not None and not last_features_cleaned.empty else None,\n",
        "                                    'Features Escaladas (Predicción)': last_features_scaled.tolist()[0] if last_features_scaled is not None and last_features_scaled.size > 0 else None\n",
        "                               })\n",
        "                               continue # Skip to the next ticker\n",
        "\n",
        "                          last_features_cleaned = last_features_cleaned[features_for_prediction] # Select only the features the model was trained on\n",
        "\n",
        "\n",
        "                          # Add checks for NaN, Inf, and Zero in cleaned features BEFORE scaling\n",
        "                          has_nan_cleaned = last_features_cleaned.isna().any().any()\n",
        "                          has_inf_cleaned = np.isinf(last_features_cleaned).any().any()\n",
        "                          has_zero_cleaned = (last_features_cleaned == 0).any().any()\n",
        "\n",
        "                          if has_nan_cleaned:\n",
        "                              print(f\"\\nDEBUG: NaN values detected in last_features_cleaned for {symbol}.\")\n",
        "                          if has_inf_cleaned:\n",
        "                              print(f\"\\nDEBUG: Inf values detected in last_features_cleaned for {symbol}.\")\n",
        "                          if has_zero_cleaned:\n",
        "                               print(f\"\\nDEBUG: Zero values detected in last_features_cleaned for {symbol}.\")\n",
        "\n",
        "\n",
        "                          if not last_features_cleaned.empty and np.isfinite(last_features_cleaned).all().all():\n",
        "                              # Ensure scaler is fitted on the *cleaned* full training data\n",
        "                              scaler = RobustScaler()\n",
        "                              # Fit scaler on the ORIGINAL cleaned training data (NOT resampled)\n",
        "                              X_train_full_cleaned_for_scaler_pred = X_train_full.replace([np.inf, -np.inf], np.nan)\n",
        "                              X_train_full_cleaned_for_scaler_pred.dropna(inplace=True)\n",
        "                              X_train_full_cleaned_for_scaler_pred = X_train_full_cleaned_for_scaler_pred.clip(lower=lower_bound, upper=upper_bound)\n",
        "                              X_train_full_cleaned_for_scaler_pred = X_train_full_cleaned_for_scaler_pred.astype(np.float64)\n",
        "                              # Ensure features for scaler fitting match model features\n",
        "                              X_train_full_cleaned_for_scaler_pred = X_train_full_cleaned_for_scaler_pred[features_for_prediction]\n",
        "\n",
        "\n",
        "                              if not X_train_full_cleaned_for_scaler_pred.empty and np.isfinite(X_train_full_cleaned_for_scaler_pred).all().all():\n",
        "                                   scaler.fit(X_train_full_cleaned_for_scaler_pred)\n",
        "\n",
        "                                   last_features_cleaned = last_features_cleaned.astype(np.float64)\n",
        "                                   # Add explicit fallback for non-finite values AFTER dropna for prediction data\n",
        "                                   # This fallback is actually redundant if dropna() was called just above and np.isfinite checked,\n",
        "                                   # but keeping it for safety if flow changes. The main checks should be BEFORE scaling.\n",
        "                                   # Let's rely on the check and skip if still non-finite after cleaning.\n",
        "\n",
        "                                   if not last_features_cleaned.empty and np.isfinite(last_features_cleaned).all().all():\n",
        "                                       last_features_scaled = scaler.transform(last_features_cleaned)\n",
        "\n",
        "                                       # Add checks for NaN, Inf, and Zero in scaled features BEFORE prediction\n",
        "                                       has_nan_scaled = np.isnan(last_features_scaled).any()\n",
        "                                       has_inf_scaled = np.isinf(last_features_scaled).any()\n",
        "                                       has_zero_scaled = (last_features_scaled == 0).any()\n",
        "\n",
        "                                       if has_nan_scaled:\n",
        "                                           print(f\"\\nDEBUG: NaN values detected in last_features_scaled for {symbol}.\")\n",
        "                                       if has_inf_scaled:\n",
        "                                           print(f\"\\nDEBUG: Inf values detected in last_features_scaled for {symbol}.\")\n",
        "                                       if has_zero_scaled:\n",
        "                                            print(f\"\\nDEBUG: Zero values detected in last_features_scaled for {symbol}.\")\n",
        "\n",
        "\n",
        "                                       future_pred_prob = best_model.predict_proba(last_features_scaled)[:, 1][0].round(4)\n",
        "                                       future_pred = 1 if future_pred_prob >= best_threshold else 0\n",
        "\n",
        "                                       last_close = None\n",
        "                                       last_open = None\n",
        "                                       last_max = None\n",
        "                                       last_data_date = None\n",
        "                                       # Get the actual date of the last data point used for prediction\n",
        "                                       if not df.empty:\n",
        "                                           last_data_date = df.index[-1]\n",
        "                                           if 'Close' in df.columns:\n",
        "                                               last_close = df['Close'].iloc[-1]\n",
        "                                           if 'Open' in df.columns:\n",
        "                                               last_open = df['Open'].iloc[-1]\n",
        "                                           if 'High' in df.columns:\n",
        "                                               last_max = df['High'].iloc[-1]\n",
        "                                       else:\n",
        "                                           print(f\"Warning: DataFrame 'df' is empty for {symbol}. Cannot get last prices.\")\n",
        "\n",
        "\n",
        "                                       action = 'BUY' if future_pred == 1 else 'SELL'\n",
        "                                       direction = 1 if future_pred == 1 else -1\n",
        "\n",
        "                                       # Explicitly use to_dict() and tolist()\n",
        "                                       cleaned_features_dict = last_features_cleaned.to_dict() if last_features_cleaned is not None and not last_features_cleaned.empty else None\n",
        "                                       # Convert numpy array to list for JSON serialization if needed later\n",
        "                                       scaled_features_list = last_features_scaled.tolist()[0] if last_features_scaled is not None and last_features_scaled.size > 0 else None\n",
        "\n",
        "\n",
        "                                       resultsp.append({\n",
        "                                                   'Papel': symbol,\n",
        "                                                   'Fecha Predicción': next_day,\n",
        "                                                   'Fecha Datos': last_data_date, # Use the date of the last data point\n",
        "                                                   'Predicción': 'Alcista' if future_pred == 1 else 'Bajista',\n",
        "                                                   'Probabilidad Alcista (Modelo)': future_pred_prob,\n",
        "                                                   'Umbral de Clasificación': best_threshold.round(4),\n",
        "                                                   'Mejores hiperparámetros (Incluye scale_pos_weight)': str(random_search.best_params_) if best_model is not None else 'Skipped (Tuning Error)', # Add check for best_model\n",
        "                                                   'Precision Test (Alcista)': precision_test_alcista,\n",
        "                                                   'Recall Test (Alcista)': recall_test_alcista,\n",
        "                                                   'F1 Test (Alcista)': f1_test_alcista,\n",
        "                                                   'ROC-AUC Test': roc_auc_test,\n",
        "                                                   'clase 1 en test (cleaned)': ratio_1_test,\n",
        "                                                   'Features Limpias (Predicción)': cleaned_features_dict, # Add cleaned features using to_dict\n",
        "                                                   'Features Escaladas (Predicción)': scaled_features_list # Add scaled features as a list\n",
        "                                           })\n",
        "\n",
        "\n",
        "\n",
        "                          else:\n",
        "                               print(f\"Warning: Training data (X_train_full) became empty or contains non-finite values after cleaning for scaler fitting for prediction. Skipping prediction for {symbol}.\")\n",
        "                               last_data_date = df.index[-1] if not df.empty else None\n",
        "                               last_close = df['Close'].iloc[-1] if not df.empty and 'Close' in df.columns else None\n",
        "                               # Explicitly use None for feature columns if prediction skipped due to data issues\n",
        "                               resultsp.append({\n",
        "                                   'Papel': symbol,\n",
        "                                   'Fecha Predicción': next_day,\n",
        "                                   'Fecha Datos': last_data_date,\n",
        "                                   'Predicción': 'Skipped (Prediction Data Issue)',\n",
        "                                   'Precio actual': last_close,\n",
        "                                   'Probabilidad Alcista (Modelo)': None,\n",
        "                                   'Umbral de Clasificación': None,\n",
        "                                   'Mejores hiperparámetros (Incluye scale_pos_weight)': str(random_search.best_params_) if best_model is not None else 'Skipped (Tuning Error)',\n",
        "                                   'Precision Test (Alcista)': precision_test_alcista,\n",
        "                                   'Recall Test (Alcista)': recall_test_alcista,\n",
        "                                   'F1 Test (Alcista)': f1_test_alcista,\n",
        "                                   'ROC-AUC Test': roc_auc_test,\n",
        "                                   'clase 1 en test (cleaned)': ratio_1_test,\n",
        "                                    'Features Limpias (Predicción)': last_features_cleaned.to_dict() if last_features_cleaned is not None and not last_features_cleaned.empty else None,\n",
        "                                    'Features Escaladas (Predicción)': last_features_scaled.tolist()[0] if last_features_scaled is not None and last_features_scaled.size > 0 else None # Ensure list conversion\n",
        "                               })\n",
        "\n",
        "\n",
        "                      else:\n",
        "                          print(f\"Warning: Could not make prediction for {symbol} as last_features was initially empty.\")\n",
        "                          last_data_date = df.index[-1] if not df.empty else None\n",
        "                          last_close = df['Close'].iloc[-1] if not df.empty and 'Close' in df.columns else None\n",
        "                          # Explicitly use None for feature columns if prediction skipped due to data issues\n",
        "                          resultsp.append({\n",
        "                              'Papel': symbol,\n",
        "                              'Fecha Predicción': next_day,\n",
        "                              'Fecha Datos': last_data_date,\n",
        "                              'Predicción': 'Skipped (Prediction Data Issue)',\n",
        "                              'Precio actual': last_close,\n",
        "                              'Probabilidad Alcista (Modelo)': None,\n",
        "                              'Umbral de Clasificación': None,\n",
        "                              'Mejores hiperparámetros (Incluye scale_pos_weight)': str(random_search.best_params_) if best_model is not None else 'Skipped (Tuning Error)',\n",
        "                              'Precision Test (Alcista)': precision_test_alcista,\n",
        "                              'Recall Test (Alcista)': recall_test_alcista,\n",
        "                              'F1 Test (Alcista)': f1_test_alcista,\n",
        "                              'ROC-AUC Test': roc_auc_test,\n",
        "                              'clase 1 en test (cleaned)': ratio_1_test,\n",
        "                       'Features Limpias (Predicción)': last_features_cleaned.to_dict() if last_features_cleaned is not None and not last_features_cleaned.empty else None,\n",
        "                       'Features Escaladas (Predicción)': last_features_scaled.tolist()[0] if last_features_scaled is not None and last_features_scaled.size > 0 else None # Ensure list conversion\n",
        "                          })\n",
        "\n",
        "\n",
        "              # Catch a general Exception during tuning/evaluation/prediction to prevent script crash\n",
        "              except Exception as e:\n",
        "                  print(f\"ERROR: An error occurred during tuning, evaluation, or prediction for {symbol}: {e}. Skipping this ticker.\")\n",
        "                  best_model = None\n",
        "                  best_threshold = 0.5\n",
        "                  precision_test_alcista = None\n",
        "                  recall_test_alcista = None\n",
        "                  f1_test_alcista = None\n",
        "                  roc_auc_test = None\n",
        "                  ratio_1_test = None\n",
        "\n",
        "                  last_data_date = df.index[-1] if not df.empty else None\n",
        "                  last_close = df['Close'].iloc[-1] if not df.empty and 'Close' in df.columns else None\n",
        "\n",
        "                  # Append entry indicating skip due to general error, explicitly add None for feature columns\n",
        "                  resultsp.append({\n",
        "                        'Papel': symbol,\n",
        "                        'Fecha Predicción': next_day,\n",
        "                        'Fecha Datos': last_data_date,\n",
        "                        'Predicción': 'Skipped (Error)',\n",
        "                        'Precio actual': last_close,\n",
        "                        'Probabilidad Alcista (Modelo)': None,\n",
        "                        'Umbral de Clasificación': None,\n",
        "                        'Mejores hiperparámetros (Incluye scale_pos_weight)': str(random_search.best_params_) if best_model is not None else 'Skipped (Tuning Error)',\n",
        "                        'Precision Test (Alcista)': precision_test_alcista,\n",
        "                        'Recall Test (Alcista)': recall_test_alcista,\n",
        "                        'F1 Test (Alcista)': f1_test_alcista,\n",
        "                        'ROC-AUC Test': roc_auc_test,\n",
        "                        'clase 1 en test (cleaned)': ratio_1_test,\n",
        "                        'Features Limpias (Predicción)': None,\n",
        "                        'Features Escaladas (Predicción)': None\n",
        "                  })\n",
        "\n",
        "          else: # This else belongs to the check for sufficient RESAMPLED data\n",
        "              print(f\"Warning: Resampled training data is insufficient or has only one class for {symbol}. Skipping model training and prediction.\")\n",
        "              last_data_date = df.index[-1] if not df.empty else None\n",
        "              last_close = df['Close'].iloc[-1] if not df.empty and 'Close' in df.columns else None\n",
        "\n",
        "              # Append entry indicating skip due to insufficient training data (resampled)\n",
        "              resultsp.append({\n",
        "                    'Papel': symbol,\n",
        "                    'Fecha Predicción': next_day,\n",
        "                    'Fecha Datos': last_data_date,\n",
        "                    'Predicción': 'Skipped (Insufficient Resampled Training Data)', # More specific message\n",
        "                    'Precio actual': last_close,\n",
        "                    'Probabilidad Alcista (Modelo)': None,\n",
        "                    'Umbral de Clasificación': None,\n",
        "                    'Mejores hiperparámetros (Incluye scale_pos_weight)': 'Skipped (Insufficient Resampled Training Data)',\n",
        "                    'Precision Test (Alcista)': None, # Set to None as no model ran\n",
        "                    'Recall Test (Alcista)': None, # Set to None as no model ran\n",
        "                    'F1 Test (Alcista)': None, # Set to None as no model ran\n",
        "                    'ROC-AUC Test': None, # Set to None as no model ran\n",
        "                    'clase 1 en test (cleaned)': None, # Set to None as no model ran\n",
        "                    'Features Limpias (Predicción)': None,\n",
        "                    'Features Escaladas (Predicción)': None\n",
        "              })\n",
        "\n",
        "\n",
        "      else: # This else belongs to the initial check for sufficient ORIGINAL training data\n",
        "          print(f\"Warning: Original training data is insufficient or has only one class for {symbol}. Skipping model training and prediction.\")\n",
        "          last_data_date = df.index[-1] if not df.empty else None\n",
        "          last_close = df['Close'].iloc[-1] if not df.empty and 'Close' in df.columns else None\n",
        "\n",
        "          # Append entry indicating skip due to insufficient training data (original)\n",
        "          resultsp.append({\n",
        "                'Papel': symbol,\n",
        "                'Fecha Predicción': next_day,\n",
        "                'Fecha Datos': last_data_date,\n",
        "                'Predicción': 'Skipped (Insufficient Original Training Data)', # More specific message\n",
        "                'Precio actual': last_close,\n",
        "                'Probabilidad Alcista (Modelo)': None,\n",
        "                'Umbral de Clasificación': None,\n",
        "                'Mejores hiperparámetros (Incluye scale_pos_weight)': 'Skipped (Insufficient Original Training Data)',\n",
        "                'Precision Test (Alcista)': None, # Set to None as no model ran\n",
        "                'Recall Test (Alcista)': None, # Set to None as no model ran\n",
        "                'F1 Test (Alcista)': None, # Set to None as no model ran\n",
        "                'ROC-AUC Test': None, # Set to None as no model ran\n",
        "                'clase 1 en test (cleaned)': None, # Set to None as no model ran\n",
        "                'Features Limpias (Predicción)': None,\n",
        "                'Features Escaladas (Predicción)': None\n",
        "          })\n",
        "\n",
        "\n",
        "  # --- End of block for full model run ---\n",
        "\n",
        "\n",
        "# Create prediction results table ONLY if the full model was run\n",
        "if run_full_model: # Moved this entire block inside the if condition\n",
        "    resultsp_df = pd.DataFrame(resultsp)\n",
        "    print(resultsp_df)\n",
        "    if not resultsp_df.empty:\n",
        "        resultsp_df.set_index('Fecha Predicción', inplace=True)\n",
        "\n",
        "        # Mostrar resultados de predicción\n",
        "        pd.set_option('display.max_columns', None)\n",
        "        #pd.set_option('display.max_rows', None) # Optional: display all rows\n",
        "        pd.set_option('display.max_colwidth', None) # Optional: display full content of columns\n",
        "\n",
        "        print(f\"\\nPrediccion para el proximo dia (hasta {next_day.strftime('%Y-%m-%d')}):\")\n",
        "        print(\"Nota: 'Fecha Predicción' es la fecha predicha; 'Fecha Datos' es la fecha de los datos usados.\")\n",
        "        display(resultsp_df) # Use display for better formatting\n",
        "\n",
        "        # Guardar y descargar el CSV de predicciones\n",
        "        resultsp_df.to_csv(f\"Predic_results_{end_date.strftime('%Y-%m-%d')}.csv\", sep=\";\")\n",
        "        files.download(f\"Predic_results_{end_date.strftime('%Y-%m-%d')}.csv\")\n",
        "        print(f\"\\nArchivo 'Predic_results_{end_date.strftime('%Y-%m-%d')}.csv' generado y descargado.\")\n",
        "    else:\n",
        "        print(\"\\nNo hay resultados de predicción para mostrar.\")\n",
        "else: # Add a message when skipping the full model run\n",
        "    print(\"\\n'run_full_model' is set to False. Skipping model training, tuning, evaluation, and prediction.\")\n",
        "    print(\"Feature distribution analysis tables for all tickers are displayed above.\")"
      ],
      "metadata": {
        "id": "NKQcw1hmEzhT",
        "outputId": "dc735851-d1c2-4ded-9738-eb58db9f856e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading data for ALUA.BA...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultiIndex columns detected for ALUA.BA.\n",
            "Successfully extracted and flattened MultiIndex columns for ALUA.BA.\n",
            "Últimas filas del DataFrame antes de crear features:\n",
            "             Open   High    Low  Close   Volume  Adj Close\n",
            "Date                                                      \n",
            "2025-09-08  670.0  705.0  600.0  696.5  1572445      696.5\n",
            "2025-09-09  700.0  713.0  678.5  694.0  1217950      694.0\n",
            "2025-09-10  700.0  716.5  691.5  702.0   641358      702.0\n",
            "2025-09-11  702.0  713.0  676.0  682.0  1443135      682.0\n",
            "2025-09-12  686.0  702.0  660.0  666.0   420245      666.0\n",
            "                  RSI       ROC       PPO  PPO_Signal  PPO_Histogram  \\\n",
            "Date                                                                   \n",
            "2025-09-12  26.056338 -2.915452 -1.130074    -0.73869      -0.391383   \n",
            "\n",
            "                 EWO  EWO_Signal  EWO_Histogram  Volatility   SMA5     SMA13  \\\n",
            "Date                                                                           \n",
            "2025-09-12 -3.056848   -1.972626      -1.084222    22.36296  688.1  705.1154   \n",
            "\n",
            "               SMA26   SMA50    SMA200  lag_change1  lag_change2  lag_change3  \\\n",
            "Date                                                                            \n",
            "2025-09-12  706.1346  711.77  783.4925     -0.02849     0.011527    -0.003589   \n",
            "\n",
            "            lag_change4  lag_change5  close_lag_1  close_lag_2  close_lag_3  \\\n",
            "Date                                                                          \n",
            "2025-09-12     0.015306    -0.014368        682.0        702.0        694.0   \n",
            "\n",
            "            close_lag_4  close_lag_5  open_lag_1  open_lag_2  open_lag_3  \\\n",
            "Date                                                                       \n",
            "2025-09-12        696.5        686.0       702.0       700.0       700.0   \n",
            "\n",
            "            open_lag_4  open_lag_5  high_lag_1  high_lag_2  high_lag_3  \\\n",
            "Date                                                                     \n",
            "2025-09-12       670.0       680.0       713.0       716.5       713.0   \n",
            "\n",
            "            high_lag_4  high_lag_5  Max_Gain_from_Open_Current  \\\n",
            "Date                                                             \n",
            "2025-09-12       705.0       696.0                    0.023324   \n",
            "\n",
            "            Max_Gain_from_Open_Lag_1  Max_Gain_from_Open_Lag_2  \\\n",
            "Date                                                             \n",
            "2025-09-12                   0.01567                  0.023571   \n",
            "\n",
            "            Max_Gain_from_Open_Lag_3  Max_Gain_from_Open_Lag_4  \\\n",
            "Date                                                             \n",
            "2025-09-12                  0.018571                  0.052239   \n",
            "\n",
            "            Max_Gain_from_Open_Lag_5  Max_Gain_from_Open_Lag_6  \\\n",
            "Date                                                             \n",
            "2025-09-12                  0.023529                  0.009859   \n",
            "\n",
            "            day_of_week_sin  day_of_week_cos  price_open_pos_5d_sin  \\\n",
            "Date                                                                  \n",
            "2025-09-12        -0.433884        -0.900969           1.224647e-16   \n",
            "\n",
            "            price_open_pos_5d_cos  price_high_pos_5d_sin  \\\n",
            "Date                                                       \n",
            "2025-09-12                   -1.0                    0.0   \n",
            "\n",
            "            price_high_pos_5d_cos  price_low_pos_5d_sin  price_low_pos_5d_cos  \\\n",
            "Date                                                                            \n",
            "2025-09-12                    1.0             -0.829677             -0.558244   \n",
            "\n",
            "            price_close_pos_5d_sin  price_close_pos_5d_cos  \\\n",
            "Date                                                         \n",
            "2025-09-12                     0.0                     1.0   \n",
            "\n",
            "            price_open_pos_20d_sin  price_open_pos_20d_cos  \\\n",
            "Date                                                         \n",
            "2025-09-12                0.946148                0.323734   \n",
            "\n",
            "            price_high_pos_20d_sin  price_high_pos_20d_cos  \\\n",
            "Date                                                         \n",
            "2025-09-12                0.999741                0.022763   \n",
            "\n",
            "            price_low_pos_20d_sin  price_low_pos_20d_cos  \\\n",
            "Date                                                       \n",
            "2025-09-12              -0.108119              -0.994138   \n",
            "\n",
            "            price_close_pos_20d_sin  price_close_pos_20d_cos  \\\n",
            "Date                                                           \n",
            "2025-09-12                      0.0                      1.0   \n",
            "\n",
            "            price_open_pos_252d_sin  price_open_pos_252d_cos  \\\n",
            "Date                                                           \n",
            "2025-09-12                 0.995379                 0.096023   \n",
            "\n",
            "            price_high_pos_252d_sin  price_high_pos_252d_cos  \\\n",
            "Date                                                           \n",
            "2025-09-12                 0.961826                 0.273663   \n",
            "\n",
            "            price_low_pos_252d_sin  price_low_pos_252d_cos  \\\n",
            "Date                                                         \n",
            "2025-09-12                 0.95694                0.290285   \n",
            "\n",
            "            price_close_pos_252d_sin  price_close_pos_252d_cos  \n",
            "Date                                                            \n",
            "2025-09-12                  0.959393                  0.282074  \n",
            "\n",
            "Últimas filas del DataFrame después de crear features:\n",
            "             Open   High    Low  Close   Volume  Adj Close  close_lag_1  \\\n",
            "Date                                                                      \n",
            "2025-09-05  680.0  696.0  664.0  686.0  2029661      686.0        696.0   \n",
            "2025-09-08  670.0  705.0  600.0  696.5  1572445      696.5        686.0   \n",
            "2025-09-09  700.0  713.0  678.5  694.0  1217950      694.0        696.5   \n",
            "2025-09-10  700.0  716.5  691.5  702.0   641358      702.0        694.0   \n",
            "2025-09-11  702.0  713.0  676.0  682.0  1443135      682.0        702.0   \n",
            "\n",
            "            close_lag_2  close_lag_3  close_lag_4  close_lag_5  open_lag_1  \\\n",
            "Date                                                                         \n",
            "2025-09-05        700.0        738.0        749.0        723.0       710.0   \n",
            "2025-09-08        696.0        700.0        738.0        749.0       680.0   \n",
            "2025-09-09        686.0        696.0        700.0        738.0       670.0   \n",
            "2025-09-10        696.5        686.0        696.0        700.0       700.0   \n",
            "2025-09-11        694.0        696.5        686.0        696.0       700.0   \n",
            "\n",
            "            open_lag_2  open_lag_3  open_lag_4  open_lag_5  high_lag_1  \\\n",
            "Date                                                                     \n",
            "2025-09-05       735.0       751.0       710.0       700.0       717.0   \n",
            "2025-09-08       710.0       735.0       751.0       710.0       696.0   \n",
            "2025-09-09       680.0       710.0       735.0       751.0       705.0   \n",
            "2025-09-10       670.0       680.0       710.0       735.0       713.0   \n",
            "2025-09-11       700.0       670.0       680.0       710.0       716.5   \n",
            "\n",
            "            high_lag_2  high_lag_3  high_lag_4  high_lag_5  Pct_change  \\\n",
            "Date                                                                     \n",
            "2025-09-05       742.0       754.0       752.0       727.0   -0.014368   \n",
            "2025-09-08       717.0       742.0       754.0       752.0    0.015306   \n",
            "2025-09-09       696.0       717.0       742.0       754.0   -0.003589   \n",
            "2025-09-10       705.0       696.0       717.0       742.0    0.011527   \n",
            "2025-09-11       713.0       705.0       696.0       717.0   -0.028490   \n",
            "\n",
            "            lag_change1  lag_change2  lag_change3  lag_change4  lag_change5  \\\n",
            "Date                                                                          \n",
            "2025-09-05    -0.005714    -0.051491    -0.014686     0.035961     0.037303   \n",
            "2025-09-08    -0.014368    -0.005714    -0.051491    -0.014686     0.035961   \n",
            "2025-09-09     0.015306    -0.014368    -0.005714    -0.051491    -0.014686   \n",
            "2025-09-10    -0.003589     0.015306    -0.014368    -0.005714    -0.051491   \n",
            "2025-09-11     0.011527    -0.003589     0.015306    -0.014368    -0.005714   \n",
            "\n",
            "                  RSI       ROC       PPO  PPO_Signal  PPO_Histogram  \\\n",
            "Date                                                                   \n",
            "2025-09-05  33.548387 -5.117566 -0.705336   -0.090195      -0.615141   \n",
            "2025-09-08  49.800797 -7.009346 -0.695150   -0.291847      -0.403303   \n",
            "2025-09-09  35.784314 -5.962060 -0.691674   -0.425122      -0.266552   \n",
            "2025-09-10  22.023810  0.285714 -0.491059   -0.447101      -0.043958   \n",
            "2025-09-11  19.892473 -2.011494 -0.734793   -0.542999      -0.191795   \n",
            "\n",
            "                 EWO  EWO_Signal  EWO_Histogram   SMA5     SMA13     SMA26  \\\n",
            "Date                                                                         \n",
            "2025-09-05 -1.276330   -0.131998      -1.144332  713.8  708.6154  718.2692   \n",
            "2025-09-08 -1.473545   -0.579180      -0.894365  703.3  709.9615  714.7885   \n",
            "2025-09-09 -1.669313   -0.942558      -0.726756  694.5  711.1923  711.7115   \n",
            "2025-09-10 -1.448619   -1.111245      -0.337374  694.9  711.9615  710.0577   \n",
            "2025-09-11 -2.069056   -1.430515      -0.638540  692.1  709.6538  708.4038   \n",
            "\n",
            "             SMA50    SMA200  Volatility  Max_Gain_from_Open_Current  \\\n",
            "Date                                                                   \n",
            "2025-09-05  714.30  786.6500   20.709520                    0.023529   \n",
            "2025-09-08  714.43  786.1225   20.323874                    0.052239   \n",
            "2025-09-09  714.19  785.5375   20.500497                    0.018571   \n",
            "2025-09-10  713.85  784.9925   20.122830                    0.023571   \n",
            "2025-09-11  713.17  784.3525   20.702450                    0.015670   \n",
            "\n",
            "            Max_Gain_from_Open_Lag_1  Max_Gain_from_Open_Lag_2  \\\n",
            "Date                                                             \n",
            "2025-09-05                  0.009859                  0.009524   \n",
            "2025-09-08                  0.023529                  0.009859   \n",
            "2025-09-09                  0.052239                  0.023529   \n",
            "2025-09-10                  0.018571                  0.052239   \n",
            "2025-09-11                  0.023571                  0.018571   \n",
            "\n",
            "            Max_Gain_from_Open_Lag_3  Max_Gain_from_Open_Lag_4  \\\n",
            "Date                                                             \n",
            "2025-09-05                  0.003995                  0.059155   \n",
            "2025-09-08                  0.009524                  0.003995   \n",
            "2025-09-09                  0.009859                  0.009524   \n",
            "2025-09-10                  0.023529                  0.009859   \n",
            "2025-09-11                  0.052239                  0.023529   \n",
            "\n",
            "            Max_Gain_from_Open_Lag_5  Max_Gain_from_Open_Lag_6  \\\n",
            "Date                                                             \n",
            "2025-09-05                  0.038571                  0.009498   \n",
            "2025-09-08                  0.059155                  0.038571   \n",
            "2025-09-09                  0.003995                  0.059155   \n",
            "2025-09-10                  0.009524                  0.003995   \n",
            "2025-09-11                  0.009859                  0.009524   \n",
            "\n",
            "            day_of_week_sin  day_of_week_cos  price_open_pos_5d_sin  \\\n",
            "Date                                                                  \n",
            "2025-09-05        -0.433884        -0.900969           0.000000e+00   \n",
            "2025-09-08         0.000000         1.000000           0.000000e+00   \n",
            "2025-09-09         0.781831         0.623490           2.393157e-01   \n",
            "2025-09-10         0.974928        -0.222521          -1.000000e+00   \n",
            "2025-09-11         0.433884        -0.900969          -2.449294e-16   \n",
            "\n",
            "            price_open_pos_5d_cos  price_high_pos_5d_sin  \\\n",
            "Date                                                       \n",
            "2025-09-05           1.000000e+00               0.000000   \n",
            "2025-09-08           1.000000e+00               0.827689   \n",
            "2025-09-09          -9.709418e-01               0.730836   \n",
            "2025-09-10          -1.836970e-16              -0.149042   \n",
            "2025-09-11           1.000000e+00              -0.878512   \n",
            "\n",
            "            price_high_pos_5d_cos  price_low_pos_5d_sin  price_low_pos_5d_cos  \\\n",
            "Date                                                                            \n",
            "2025-09-05               1.000000          0.000000e+00              1.000000   \n",
            "2025-09-08               0.561187          0.000000e+00              1.000000   \n",
            "2025-09-09              -0.682553         -8.604016e-01              0.509617   \n",
            "2025-09-10               0.988831         -2.449294e-16              1.000000   \n",
            "2025-09-11               0.477720         -8.744810e-01              0.485060   \n",
            "\n",
            "            price_close_pos_5d_sin  price_close_pos_5d_cos  \\\n",
            "Date                                                         \n",
            "2025-09-05            0.000000e+00                1.000000   \n",
            "2025-09-08            9.547209e-01                0.297503   \n",
            "2025-09-09           -4.338837e-01               -0.900969   \n",
            "2025-09-10           -2.449294e-16                1.000000   \n",
            "2025-09-11            0.000000e+00                1.000000   \n",
            "\n",
            "            price_open_pos_20d_sin  price_open_pos_20d_cos  \\\n",
            "Date                                                         \n",
            "2025-09-05                0.328867                0.944376   \n",
            "2025-09-08                0.000000                1.000000   \n",
            "2025-09-09                0.727374               -0.686242   \n",
            "2025-09-10                0.727374               -0.686242   \n",
            "2025-09-11                0.612601               -0.790393   \n",
            "\n",
            "            price_high_pos_20d_sin  price_high_pos_20d_cos  \\\n",
            "Date                                                         \n",
            "2025-09-05                0.842371                0.538899   \n",
            "2025-09-08                0.968809               -0.247808   \n",
            "2025-09-09                0.557934               -0.829885   \n",
            "2025-09-10                0.269797               -0.962917   \n",
            "2025-09-11                0.557934               -0.829885   \n",
            "\n",
            "            price_low_pos_20d_sin  price_low_pos_20d_cos  \\\n",
            "Date                                                       \n",
            "2025-09-05               0.000000               1.000000   \n",
            "2025-09-08               0.000000               1.000000   \n",
            "2025-09-09              -0.895872              -0.444312   \n",
            "2025-09-10              -0.970441               0.241338   \n",
            "2025-09-11              -0.827689              -0.561187   \n",
            "\n",
            "            price_close_pos_20d_sin  price_close_pos_20d_cos  \\\n",
            "Date                                                           \n",
            "2025-09-05                 0.650289                 0.759687   \n",
            "2025-09-08                 0.997798                -0.066323   \n",
            "2025-09-09                 0.988032                 0.154249   \n",
            "2025-09-10                 0.850902                -0.525325   \n",
            "2025-09-11                 0.346636                 0.938000   \n",
            "\n",
            "            price_open_pos_252d_sin  price_open_pos_252d_cos  \\\n",
            "Date                                                           \n",
            "2025-09-05                 0.981559                 0.191159   \n",
            "2025-09-08                 0.938468                 0.345365   \n",
            "2025-09-09                 0.991790                -0.127877   \n",
            "2025-09-10                 0.991790                -0.127877   \n",
            "2025-09-11                 0.987182                -0.159600   \n",
            "\n",
            "            price_high_pos_252d_sin  price_high_pos_252d_cos  \\\n",
            "Date                                                           \n",
            "2025-09-05                 0.929405                 0.369062   \n",
            "2025-09-08                 0.974391                 0.224860   \n",
            "2025-09-09                 0.995734                 0.092268   \n",
            "2025-09-10                 0.999436                 0.033594   \n",
            "2025-09-11                 0.995734                 0.092268   \n",
            "\n",
            "            price_low_pos_252d_sin  price_low_pos_252d_cos  \\\n",
            "Date                                                         \n",
            "2025-09-05                0.973877                0.227076   \n",
            "2025-09-08                0.290285                0.956940   \n",
            "2025-09-09                0.999967               -0.008181   \n",
            "2025-09-10                0.975702               -0.219101   \n",
            "2025-09-11                0.999465                0.032719   \n",
            "\n",
            "            price_close_pos_252d_sin  price_close_pos_252d_cos  Label_raw  \\\n",
            "Date                                                                        \n",
            "2025-09-05                  0.999624                 -0.027417          1   \n",
            "2025-09-08                  0.981635                 -0.190766          0   \n",
            "2025-09-09                  0.988353                 -0.152177          0   \n",
            "2025-09-10                  0.961573                 -0.274549          1   \n",
            "2025-09-11                  0.999379                  0.035247          1   \n",
            "\n",
            "            Label  \n",
            "Date               \n",
            "2025-09-05    0.0  \n",
            "2025-09-08    0.0  \n",
            "2025-09-09    1.0  \n",
            "2025-09-10    1.0  \n",
            "2025-09-11    0.0  \n",
            "Index(['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close', 'close_lag_1',\n",
            "       'close_lag_2', 'close_lag_3', 'close_lag_4', 'close_lag_5',\n",
            "       'open_lag_1', 'open_lag_2', 'open_lag_3', 'open_lag_4', 'open_lag_5',\n",
            "       'high_lag_1', 'high_lag_2', 'high_lag_3', 'high_lag_4', 'high_lag_5',\n",
            "       'Pct_change', 'lag_change1', 'lag_change2', 'lag_change3',\n",
            "       'lag_change4', 'lag_change5', 'RSI', 'ROC', 'PPO', 'PPO_Signal',\n",
            "       'PPO_Histogram', 'EWO', 'EWO_Signal', 'EWO_Histogram', 'SMA5', 'SMA13',\n",
            "       'SMA26', 'SMA50', 'SMA200', 'Volatility', 'Max_Gain_from_Open_Current',\n",
            "       'Max_Gain_from_Open_Lag_1', 'Max_Gain_from_Open_Lag_2',\n",
            "       'Max_Gain_from_Open_Lag_3', 'Max_Gain_from_Open_Lag_4',\n",
            "       'Max_Gain_from_Open_Lag_5', 'Max_Gain_from_Open_Lag_6',\n",
            "       'day_of_week_sin', 'day_of_week_cos', 'price_open_pos_5d_sin',\n",
            "       'price_open_pos_5d_cos', 'price_high_pos_5d_sin',\n",
            "       'price_high_pos_5d_cos', 'price_low_pos_5d_sin', 'price_low_pos_5d_cos',\n",
            "       'price_close_pos_5d_sin', 'price_close_pos_5d_cos',\n",
            "       'price_open_pos_20d_sin', 'price_open_pos_20d_cos',\n",
            "       'price_high_pos_20d_sin', 'price_high_pos_20d_cos',\n",
            "       'price_low_pos_20d_sin', 'price_low_pos_20d_cos',\n",
            "       'price_close_pos_20d_sin', 'price_close_pos_20d_cos',\n",
            "       'price_open_pos_252d_sin', 'price_open_pos_252d_cos',\n",
            "       'price_high_pos_252d_sin', 'price_high_pos_252d_cos',\n",
            "       'price_low_pos_252d_sin', 'price_low_pos_252d_cos',\n",
            "       'price_close_pos_252d_sin', 'price_close_pos_252d_cos', 'Label_raw',\n",
            "       'Label'],\n",
            "      dtype='object')\n",
            "\n",
            "Overall historical target distribution for ALUA.BA (before train/test split):\n",
            "Label\n",
            "0.0    0.692906\n",
            "1.0    0.307094\n",
            "Name: proportion, dtype: float64\n",
            "Correlacion con label para ALUA.BA:\n",
            "Label                       1.000000\n",
            "Max_Gain_from_Open_Lag_3    0.122145\n",
            "close_lag_1                 0.109803\n",
            "Volatility                  0.109717\n",
            "high_lag_1                  0.109302\n",
            "                              ...   \n",
            "price_open_pos_20d_sin     -0.017051\n",
            "day_of_week_sin            -0.019509\n",
            "price_high_pos_5d_cos      -0.019942\n",
            "price_low_pos_5d_sin       -0.021092\n",
            "price_high_pos_20d_sin     -0.021775\n",
            "Name: Label, Length: 68, dtype: float64\n",
            "Optimizar hiperparámetros con RandomizedSearchCV para ALUA.BA\n",
            "\n",
            "Applying SMOTE to training data for ALUA.BA...\n",
            "Original training data shape: (5007, 67), Resampled shape: (7094, 67)\n",
            "Original training target distribution: Label\n",
            "0.0    3547\n",
            "1.0    1460\n",
            "Name: count, dtype: int64, Resampled target distribution: Label\n",
            "0.0    3547\n",
            "1.0    3547\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4a316f4"
      },
      "source": [
        "# Task\n",
        "Analyze the reasons for the poor performance of the XGBoost model in predicting downward movements (class 0) in the stock data, specifically addressing the class imbalance and the potential impact of features, and propose steps to improve the model's ability to identify these instances."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a66c9a9d"
      },
      "source": [
        "## Analyze class imbalance impact\n",
        "\n",
        "### Subtask:\n",
        "Re-examine the class distribution in the training data to understand the extent of the imbalance and how `scale_pos_weight` was tuned.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b7234a9"
      },
      "source": [
        "**Reasoning**:\n",
        "Re-examine the class distribution in the training data and how `scale_pos_weight` was tuned.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "id": "8315ca88",
        "outputId": "4a01856b-b549-4bbd-f7f8-ac6ff8b80e0d"
      },
      "source": [
        "# 1. Print original training data class distribution\n",
        "print(\"Original training data class distribution (y_train_full):\")\n",
        "print(y_train_full.value_counts())\n",
        "print(y_train_full.value_counts(normalize=True))\n",
        "\n",
        "# 2. Print resampled training data class distribution\n",
        "print(\"\\nResampled training data class distribution (y_train_res):\")\n",
        "print(y_train_res.value_counts())\n",
        "print(y_train_res.value_counts(normalize=True))\n",
        "\n",
        "# 3. Review the tuned scale_pos_weight\n",
        "if 'random_search' in locals():\n",
        "    print(\"\\nBest hyperparameters from RandomizedSearchCV:\")\n",
        "    print(random_search.best_params_)\n",
        "    tuned_scale_pos_weight = random_search.best_params_.get('scale_pos_weight')\n",
        "    print(f\"\\nTuned scale_pos_weight: {tuned_scale_pos_weight}\")\n",
        "\n",
        "    # 4. Compare original class ratio with tuned scale_pos_weight\n",
        "    original_counts = y_train_full.value_counts()\n",
        "    if 0 in original_counts and 1 in original_counts:\n",
        "        original_ratio = original_counts[0] / original_counts[1]\n",
        "        print(f\"\\nOriginal training data ratio (Class 0 / Class 1): {original_ratio:.4f}\")\n",
        "        print(f\"Is tuned scale_pos_weight close to the original ratio? {'Yes' if abs(tuned_scale_pos_weight - original_ratio) / original_ratio < 0.2 else 'No'} (within 20%)\") # Simple check\n",
        "    else:\n",
        "        print(\"\\nCannot calculate original class ratio (one or both classes missing).\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nRandomizedSearchCV object not found. Cannot retrieve best hyperparameters or tuned scale_pos_weight.\")\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original training data class distribution (y_train_full):\n",
            "Label\n",
            "0.0    2188\n",
            "1.0    1583\n",
            "Name: count, dtype: int64\n",
            "Label\n",
            "0.0    0.580217\n",
            "1.0    0.419783\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Resampled training data class distribution (y_train_res):\n",
            "Label\n",
            "1.0    2188\n",
            "0.0    2188\n",
            "Name: count, dtype: int64\n",
            "Label\n",
            "1.0    0.5\n",
            "0.0    0.5\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Best hyperparameters from RandomizedSearchCV:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'RandomizedSearchCV' object has no attribute 'best_params_'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-172938024.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'random_search'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nBest hyperparameters from RandomizedSearchCV:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mtuned_scale_pos_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'scale_pos_weight'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nTuned scale_pos_weight: {tuned_scale_pos_weight}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'RandomizedSearchCV' object has no attribute 'best_params_'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "408db8d2"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt failed because the `random_search` object apparently does not have the `best_params_` attribute after execution in the current state. I need to find the best parameters, including `scale_pos_weight`, from the `random_search` object. I will assume the object exists and try accessing the `best_params_` attribute again, perhaps there was a temporary issue. I will also ensure the code is robust to the attribute not existing by adding a check.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjo_kwelfI_s",
        "outputId": "ec922a87-746e-4067-e403-a82032ca57cc"
      },
      "source": [
        "# 1. Print original training data class distribution\n",
        "print(\"Original training data class distribution (y_train_full):\")\n",
        "print(y_train_full.value_counts())\n",
        "print(y_train_full.value_counts(normalize=True))\n",
        "\n",
        "# 2. Print resampled training data class distribution\n",
        "print(\"\\nResampled training data class distribution (y_train_res):\")\n",
        "print(y_train_res.value_counts())\n",
        "print(y_train_res.value_counts(normalize=True))\n",
        "\n",
        "# 3. Review the tuned scale_pos_weight\n",
        "if 'random_search' in locals() and hasattr(random_search, 'best_params_'):\n",
        "    print(\"\\nBest hyperparameters from RandomizedSearchCV:\")\n",
        "    print(random_search.best_params_)\n",
        "    tuned_scale_pos_weight = random_search.best_params_.get('scale_pos_weight')\n",
        "    print(f\"\\nTuned scale_pos_weight: {tuned_scale_pos_weight}\")\n",
        "\n",
        "    # 4. Compare original class ratio with tuned scale_pos_weight\n",
        "    original_counts = y_train_full.value_counts()\n",
        "    if 0 in original_counts and 1 in original_counts:\n",
        "        original_ratio = original_counts[0] / original_counts[1]\n",
        "        print(f\"\\nOriginal training data ratio (Class 0 / Class 1): {original_ratio:.4f}\")\n",
        "        # Add check to prevent division by zero if tuned_scale_pos_weight is None or 0\n",
        "        if tuned_scale_pos_weight is not None and tuned_scale_pos_weight != 0:\n",
        "            print(f\"Is tuned scale_pos_weight close to the original ratio? {'Yes' if abs(tuned_scale_pos_weight - original_ratio) / original_ratio < 0.2 else 'No'} (within 20%)\") # Simple check\n",
        "        else:\n",
        "            print(\"Cannot compare tuned scale_pos_weight as it is None or zero.\")\n",
        "    else:\n",
        "        print(\"\\nCannot calculate original class ratio (one or both classes missing).\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nRandomizedSearchCV object or best_params_ attribute not found. Cannot retrieve best hyperparameters or tuned scale_pos_weight.\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original training data class distribution (y_train_full):\n",
            "Label\n",
            "0.0    2188\n",
            "1.0    1583\n",
            "Name: count, dtype: int64\n",
            "Label\n",
            "0.0    0.580217\n",
            "1.0    0.419783\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Resampled training data class distribution (y_train_res):\n",
            "Label\n",
            "1.0    2188\n",
            "0.0    2188\n",
            "Name: count, dtype: int64\n",
            "Label\n",
            "1.0    0.5\n",
            "0.0    0.5\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "RandomizedSearchCV object or best_params_ attribute not found. Cannot retrieve best hyperparameters or tuned scale_pos_weight.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92463ca2"
      },
      "source": [
        "## Review feature importance\n",
        "\n",
        "### Subtask:\n",
        "Look closely at the feature importance scores from the trained model to see which features it is relying on and whether they seem relevant or if there are unexpected results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac7dac28"
      },
      "source": [
        ":**Reasoning**:\n",
        "Check if feature_importance exists and print it for analysis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4802aca4",
        "outputId": "40a01ba6-c589-4d59-c0f0-45fb645a3fde"
      },
      "source": [
        "# 1. Check if feature_importance variable exists and is not empty.\n",
        "if 'feature_importance' in locals() and not feature_importance.empty:\n",
        "    # 2. Print the feature_importance Series.\n",
        "    print(\"Feature Importance Scores:\")\n",
        "    print(feature_importance)\n",
        "else:\n",
        "    print(\"Feature importance data not available or is empty.\")\n",
        "\n",
        "# 3. Analysis will be done manually after the output is displayed."
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importance Scores:\n",
            "lag_change2    657.0\n",
            "lag_change3    653.0\n",
            "lag_change1    632.0\n",
            "lag_change4    613.0\n",
            "lag_change5    612.0\n",
            "               ...  \n",
            "high_lag_3      52.0\n",
            "high_lag_1      52.0\n",
            "high_lag_5      52.0\n",
            "close_lag_2     49.0\n",
            "high_lag_2      33.0\n",
            "Length: 67, dtype: float64\n"
          ]
        }
      ]
    }
  ]
}