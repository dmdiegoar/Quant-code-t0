{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMh9fq4ljx4X2ROVODADTm7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmdiegoar/Quant-code-t0/blob/main/caminos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAhj-FMopUPi"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium stable-baselines3 numpy matplotlib\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "from gym import spaces\n",
        "from stable_baselines3 import PPO\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Simular 1200 carreteras (reemplazar con tus datos reales)\n",
        "roads = [[0] + [np.clip(np.random.uniform(-0.5, 0.5) + y, -1, 1) for y in [0]*31] for _ in range(1200)]\n",
        "# Ejemplo de cómo cargar tus carreteras desde un CSV:\n",
        "# import pandas as pd\n",
        "# df = pd.read_csv(\"carreteras.csv\")  # 1200 filas, 32 columnas\n",
        "# roads = [row.tolist() for _, row in df.iterrows()]\n",
        "\n",
        "class RoadFollowingEnv(gym.Env):\n",
        "    def __init__(self, roads):\n",
        "        super(RoadFollowingEnv, self).__init__()\n",
        "        self.action_space = spaces.Discrete(3)  # 0: mantener, 1: izquierda, 2: derecha\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(6,), dtype=np.float32)  # x, y, ángulo, y_traza, delta_y1, delta_y2\n",
        "        self.roads = roads\n",
        "        self.current_road_idx = 0\n",
        "        self.position = np.array([0.0, 0.0])\n",
        "        self.angle = 0.0\n",
        "        self.velocity = 1.0\n",
        "        self.step_count = 0\n",
        "        self.max_steps = 1000\n",
        "        self.current_road = self.roads[0]\n",
        "\n",
        "    def reset(self):\n",
        "        self.position = np.array([0.0, 0.0])\n",
        "        self.angle = 0.0\n",
        "        self.step_count = 0\n",
        "        self.current_road_idx = (self.current_road_idx + 1) % len(self.roads)  # Siguiente carretera\n",
        "        self.current_road = self.roads[self.current_road_idx]\n",
        "        return self._get_observation()\n",
        "\n",
        "    def _get_observation(self):\n",
        "        x = self.position[0]\n",
        "        idx = int(x)\n",
        "        if idx >= len(self.current_road) - 1:\n",
        "            idx = len(self.current_road) - 2\n",
        "        frac = x - idx\n",
        "        y_traza = self.current_road[idx] + frac * (self.current_road[idx + 1] - self.current_road[idx])\n",
        "        delta_y1 = self.current_road[min(idx + 1, len(self.current_road) - 1)] - self.current_road[idx]\n",
        "        delta_y2 = self.current_road[min(idx + 2, len(self.current_road) - 1)] - self.current_road[min(idx + 1, len(self.current_road) - 1)]\n",
        "        return np.array([self.position[0], self.position[1], self.angle, y_traza, delta_y1, delta_y2], dtype=np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        self.step_count += 1\n",
        "        if action == 1:  # Izquierda\n",
        "            self.angle -= 0.1\n",
        "        elif action == 2:  # Derecha\n",
        "            self.angle += 0.1\n",
        "        self.position[0] += self.velocity * np.cos(self.angle) * 0.1\n",
        "        self.position[1] += self.velocity * np.sin(self.angle) * 0.1\n",
        "\n",
        "        x = self.position[0]\n",
        "        idx = int(x)\n",
        "        if idx >= len(self.current_road) - 1:\n",
        "            idx = len(self.current_road) - 2\n",
        "        frac = x - idx\n",
        "        y_traza = self.current_road[idx] + frac * (self.current_road[idx + 1] - self.current_road[idx])\n",
        "        distance_to_track = abs(self.position[1] - y_traza)\n",
        "        reward = 1.0 - distance_to_track\n",
        "        if distance_to_track > 0.5:\n",
        "            reward = -10.0\n",
        "            done = True\n",
        "        else:\n",
        "            done = self.step_count >= self.max_steps or x >= len(self.current_road) - 1\n",
        "        return self._get_observation(), reward, done, {}\n",
        "\n",
        "    def render(self):\n",
        "        plt.clf()\n",
        "        x = np.arange(0, len(self.current_road), 1)\n",
        "        y = self.current_road\n",
        "        plt.plot(x, y, 'b-', label='Traza ideal')\n",
        "        plt.plot(self.position[0], self.position[1], 'ro', label='Vehículo')\n",
        "        plt.legend()\n",
        "        plt.title(f\"Carretera {self.current_road_idx}\")\n",
        "        plt.show()\n",
        "\n",
        "# Entrenar el agente\n",
        "env = RoadFollowingEnv(roads)\n",
        "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
        "episode_rewards = []\n",
        "for episode in range(1200):  # Entrenar en cada carretera\n",
        "    obs = env.reset()\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action, _ = model.predict(obs)\n",
        "        obs, reward, done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "        if steps % 100 == 0:\n",
        "            env.render()\n",
        "    avg_reward = total_reward / steps\n",
        "    episode_rewards.append(avg_reward)\n",
        "    print(f\"Episodio {episode + 1}, Carretera {env.current_road_idx}, Recompensa promedio: {avg_reward:.2f}\")\n",
        "    if avg_reward > 0.8:  # Criterio de aprendizaje\n",
        "        print(f\"Carretera {env.current_road_idx} aprendida!\")\n",
        "    model.learn(total_timesteps=1000)  # Entrenar 1000 pasos por episodio\n",
        "\n",
        "# Guardar el modelo\n",
        "model.save(\"ppo_road_follower\")\n",
        "\n",
        "# Probar en una carretera nueva\n",
        "test_road = [0, 0.3, -0.2, -0.5, 0.1, 0.7, -0.2, 0.4] + [0]*24  # Nueva carretera con 32 puntos\n",
        "env.current_road = test_road\n",
        "obs = env.reset()\n",
        "positions = [(obs[0], obs[1])]\n",
        "for _ in range(1000):\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, reward, done, _ = env.step(action)\n",
        "    positions.append((obs[0], obs[1]))\n",
        "    env.render()\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "# Visualizar resultados\n",
        "plt.plot(range(32), test_road, 'b-', label='Traza ideal')\n",
        "plt.plot([p[0] for p in positions], [p[1] for p in positions], 'r-', label='Vehículo')\n",
        "plt.legend()\n",
        "plt.title(\"Prueba en carretera nueva\")\n",
        "plt.show()\n",
        "\n",
        "# Guardar modelo para descargar\n",
        "from google.colab import files\n",
        "files.download(\"ppo_road_follower.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Caminos"
      ],
      "metadata": {
        "id": "7tHU2Fcgpwvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"carreteras.csv\")  # 1200 filas, 32 columnas\n",
        "roads = [row.tolist() for _, row in df.iterrows()]"
      ],
      "metadata": {
        "id": "zLnhxUsWpuQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Giros"
      ],
      "metadata": {
        "id": "Qbm8NkfhpXoz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lxz_bJiPpZ7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium stable-baselines3 numpy matplotlib\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "from gym import spaces\n",
        "from stable_baselines3 import PPO\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Simular 1200 carreteras (reemplazar con tus datos reales)\n",
        "roads = [[0] + [np.clip(np.random.uniform(-0.5, 0.5) + y, -1, 1) for y in [0]*31] for _ in range(1200)]\n",
        "# Ejemplo de cómo cargar tus carreteras desde un CSV:\n",
        "# import pandas as pd\n",
        "# df = pd.read_csv(\"carreteras.csv\")  # 1200 filas, 32 columnas\n",
        "# roads = [row.tolist() for _, row in df.iterrows()]\n",
        "\n",
        "class RoadFollowingEnv(gym.Env):\n",
        "    def __init__(self, roads):\n",
        "        super(RoadFollowingEnv, self).__init__()\n",
        "        self.action_space = spaces.Discrete(3)  # 0: mantener, 1: izquierda, 2: derecha\n",
        "        self.observation_space = spaces.Box(low=-1, high=1, shape=(3,), dtype=np.float32)  # y_vehículo - y_traza, signo(delta_y1), signo(delta_y2)\n",
        "        self.roads = roads\n",
        "        self.current_road_idx = 0\n",
        "        self.position = np.array([0.0, 0.0])\n",
        "        self.angle = 0.0\n",
        "        self.velocity = 1.0\n",
        "        self.step_count = 0\n",
        "        self.max_steps = 1000\n",
        "        self.current_road = self.roads[0]\n",
        "\n",
        "    def reset(self):\n",
        "        self.position = np.array([0.0, 0.0])\n",
        "        self.angle = 0.0\n",
        "        self.step_count = 0\n",
        "        self.current_road_idx = (self.current_road_idx + 1) % len(self.roads)  # Siguiente carretera\n",
        "        self.current_road = self.roads[self.current_road_idx]\n",
        "        return self._get_observation()\n",
        "\n",
        "    def _get_observation(self):\n",
        "        x = self.position[0]\n",
        "        idx = int(x)\n",
        "        if idx >= len(self.current_road) - 1:\n",
        "            idx = len(self.current_road) - 2\n",
        "        frac = x - idx\n",
        "        y_traza = self.current_road[idx] + frac * (self.current_road[idx + 1] - self.current_road[idx])\n",
        "        delta_y1 = self.current_road[min(idx + 1, len(self.current_road) - 1)] - self.current_road[idx]\n",
        "        delta_y2 = self.current_road[min(idx + 2, len(self.current_road) - 1)] - self.current_road[min(idx + 1, len(self.current_road) - 1)]\n",
        "        sign_delta_y1 = np.sign(delta_y1)\n",
        "        sign_delta_y2 = np.sign(delta_y2)\n",
        "        return np.array([self.position[1] - y_traza, sign_delta_y1, sign_delta_y2], dtype=np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        self.step_count += 1\n",
        "        if action == 1:  # Izquierda\n",
        "            self.angle -= 0.1\n",
        "        elif action == 2:  # Derecha\n",
        "            self.angle += 0.1\n",
        "        self.position[0] += self.velocity * np.cos(self.angle) * 0.1\n",
        "        self.position[1] += self.velocity * np.sin(self.angle) * 0.1\n",
        "\n",
        "        x = self.position[0]\n",
        "        idx = int(x)\n",
        "        if idx >= len(self.current_road) - 1:\n",
        "            idx = len(self.current_road) - 2\n",
        "        frac = x - idx\n",
        "        y_traza = self.current_road[idx] + frac * (self.current_road[idx + 1] - self.current_road[idx])\n",
        "        distance_to_track = abs(self.position[1] - y_traza)\n",
        "        reward = 1.0 - distance_to_track\n",
        "        if action != 0:  # Penalizar giros innecesarios\n",
        "            delta_y1 = self.current_road[min(idx + 1, len(self.current_road) - 1)] - self.current_road[idx]\n",
        "            delta_y2 = self.current_road[min(idx + 2, len(self.current_road) - 1)] - self.current_road[min(idx + 1, len(self.current_road) - 1)]\n",
        "            if np.sign(delta_y1) == np.sign(delta_y2):  # No hay cambio de sentido\n",
        "                reward -= 0.1\n",
        "        if distance_to_track > 0.5:\n",
        "            reward = -10.0\n",
        "            done = True\n",
        "        else:\n",
        "            done = self.step_count >= self.max_steps or x >= len(self.current_road) - 1\n",
        "        return self._get_observation(), reward, done, {}\n",
        "\n",
        "    def render(self):\n",
        "        plt.clf()\n",
        "        x = np.arange(0, len(self.current_road), 1)\n",
        "        y = self.current_road\n",
        "        plt.plot(x, y, 'b-', label='Traza ideal')\n",
        "        plt.plot(self.position[0], self.position[1], 'ro', label='Vehículo')\n",
        "        plt.legend()\n",
        "        plt.title(f\"Carretera {self.current_road_idx}\")\n",
        "        plt.show()\n",
        "\n",
        "# Entrenar el agente\n",
        "env = RoadFollowingEnv(roads)\n",
        "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
        "episode_rewards = []\n",
        "for episode in range(1200):  # Entrenar en cada carretera\n",
        "    obs = env.reset()\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action, _ = model.predict(obs)\n",
        "        obs, reward, done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "        if steps % 100 == 0:\n",
        "            env.render()\n",
        "    avg_reward = total_reward / steps\n",
        "    episode_rewards.append(avg_reward)\n",
        "    print(f\"Episodio {episode + 1}, Carretera {env.current_road_idx}, Recompensa promedio: {avg_reward:.2f}\")\n",
        "    if avg_reward > 0.8:  # Criterio de aprendizaje\n",
        "        print(f\"Carretera {env.current_road_idx} aprendida!\")\n",
        "    model.learn(total_timesteps=1000)  # Entrenar 1000 pasos por episodio\n",
        "\n",
        "# Guardar el modelo\n",
        "model.save(\"ppo_road_follower_change\")\n",
        "\n",
        "# Probar en una carretera nueva\n",
        "test_road = [0, 0.3, -0.2, -0.5, 0.1, 0.7, -0.2, 0.4] + [0]*24  # Nueva carretera con 32 puntos\n",
        "env.current_road = test_road\n",
        "obs = env.reset()\n",
        "positions = [(obs[0], obs[1])]\n",
        "for _ in range(1000):\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, reward, done, _ = env.step(action)\n",
        "    positions.append((obs[0], obs[1]))\n",
        "    env.render()\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "# Visualizar resultados\n",
        "plt.plot(range(32), test_road, 'b-', label='Traza ideal')\n",
        "plt.plot([p[0] for p in positions], [p[1] for p in positions], 'r-', label='Vehículo')\n",
        "plt.legend()\n",
        "plt.title(\"Prueba en carretera nueva\")\n",
        "plt.show()\n",
        "\n",
        "# Guardar modelo para descargar\n",
        "from google.colab import files\n",
        "files.download(\"ppo_road_follower_change.zip\")"
      ],
      "metadata": {
        "id": "oM_XgBXMpbdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Probar modelos guardado"
      ],
      "metadata": {
        "id": "g0HV5dQyqZkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO\n",
        "model = PPO.load(\"ppo_road_follower_angle\")  # O ppo_road_follower_change\n",
        "env = RoadFollowingEnv(roads)  # Cargar con cualquier lista de carreteras\n",
        "test_road = [0, 0.3, -0.2, -0.5, 0.1, 0.7, -0.2, 0.4] + [0]*24\n",
        "env.current_road = test_road\n",
        "obs = env.reset()\n",
        "positions = [(obs[0], obs[1])]\n",
        "for _ in range(1000):\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, reward, done, _ = env.step(action)\n",
        "    positions.append((obs[0], obs[1]))\n",
        "    env.render()\n",
        "    if done:\n",
        "        break\n",
        "plt.plot(range(32), test_road, 'b-', label='Traza ideal')\n",
        "plt.plot([p[0] for p in positions], [p[1] for p in positions], 'r-', label='Vehículo')\n",
        "plt.legend()\n",
        "plt.title(\"Prueba en carretera nueva\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OTerAevHqcwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verla"
      ],
      "metadata": {
        "id": "kqIbSS8OtS5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Pega aquí tu carretera (32 puntos)\n",
        "road = [0, 0.3, -0.2, -0.5, 0.1, 0.7, -0.2, 0.4, 0.0, 0.2, -0.1, 0.3, -0.3, 0.5, -0.4, 0.1, 0.0, 0.2, -0.2, 0.4, -0.5, 0.0, 0.3, -0.1, 0.2, -0.4, 0.5, -0.3, 0.1, 0.0, -0.2, 0.3]  # Ejemplo, reemplaza con tus 32 puntos\n",
        "\n",
        "# Verificar que la carretera tenga 32 puntos\n",
        "if len(road) != 32:\n",
        "    print(\"Error: La carretera debe tener exactamente 32 puntos\")\n",
        "else:\n",
        "    # Graficar\n",
        "    x = np.arange(0, 32, 1)  # x = 0, 1, ..., 31\n",
        "    y = road\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(x, y, 'b-', marker='o', markersize=3, label='Carretera')\n",
        "    plt.xlabel('x')\n",
        "    plt.ylabel('y')\n",
        "    plt.title('Carretera con 32 puntos')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "V0hpuTmytU2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Masima"
      ],
      "metadata": {
        "id": "DQ5C-7Nt64p-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hMaBAcMS67lE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar dependencias\n",
        "!pip uninstall -y gym gymnasium shimmy stable-baselines3 numpy\n",
        "!pip install gymnasium==0.29.1 shimmy[gym-v26]>=0.2.0 stable-baselines3==2.0.0 numpy==1.26.4\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from gymnasium import spaces\n",
        "from stable_baselines3 import PPO\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Simular 1200 carreteras (reemplazar con tus datos)\n",
        "roads = [[0] + [np.clip(np.random.uniform(-1, 1) + y, -1, 1) for y in [0]*31] for _ in range(1200)]\n",
        "# Para cargar desde CSV:\n",
        "# import pandas as pd\n",
        "# df = pd.read_csv(\"carreteras.csv\")\n",
        "# roads = [row.tolist() for _, row in df.iterrows()]\n",
        "\n",
        "class RoadFollowingEnv(gym.Env):\n",
        "    def __init__(self, roads=None, single_road=None):\n",
        "        super(RoadFollowingEnv, self).__init__()\n",
        "        self.action_space = spaces.Box(low=-0.3, high=0.3, shape=(1,), dtype=np.float32)\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(6,), dtype=np.float32)\n",
        "        self.roads = roads\n",
        "        self.single_road = single_road\n",
        "        self.current_road_idx = -1\n",
        "        self.position = np.array([0.0, 0.0])\n",
        "        self.angle = 0.0\n",
        "        self.velocity = 1.0\n",
        "        self.step_count = 0\n",
        "        self.max_steps = 1000\n",
        "        self.current_road = single_road if single_road is not None else None\n",
        "        self.trajectory = []\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.position = np.array([0.0, 0.0])\n",
        "        self.angle = 0.0\n",
        "        self.step_count = 0\n",
        "        self.trajectory = [(self.position[0], self.position[1])]\n",
        "        if self.roads is not None:\n",
        "            self.current_road_idx = (self.current_road_idx + 1) % len(self.roads)\n",
        "            self.current_road = self.roads[self.current_road_idx]\n",
        "        return self._get_observation(), {}\n",
        "\n",
        "    def _get_observation(self):\n",
        "        x = self.position[0]\n",
        "        idx = int(x)\n",
        "        if idx >= len(self.current_road) - 1:\n",
        "            idx = len(self.current_road) - 2\n",
        "        frac = x - idx\n",
        "        y_traza = self.current_road[idx] + frac * (self.current_road[idx + 1] - self.current_road[idx])\n",
        "        delta_y1 = 0 if idx == len(self.current_road) - 1 else self.current_road[idx + 1] - self.current_road[idx]\n",
        "        delta_y2 = 0 if idx >= len(self.current_road) - 2 else self.current_road[idx + 2] - self.current_road[idx + 1]\n",
        "        return np.array([self.position[0], self.position[1], self.angle, y_traza, delta_y1, delta_y2], dtype=np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        self.step_count += 1\n",
        "        self.angle += action[0]\n",
        "        self.position[0] += self.velocity * np.cos(self.angle) * 0.1\n",
        "        self.position[1] += self.velocity * np.sin(self.angle) * 0.1\n",
        "        self.trajectory.append((self.position[0], self.position[1]))\n",
        "        x = self.position[0]\n",
        "        idx = int(x)\n",
        "        if idx >= len(self.current_road) - 1:\n",
        "            idx = len(self.current_road) - 2\n",
        "        frac = x - idx\n",
        "        y_traza = self.current_road[idx] + frac * (self.current_road[idx + 1] - self.current_road[idx])\n",
        "        distance_to_track = abs(self.position[1] - y_traza)\n",
        "        reward = 1.0 - distance_to_track\n",
        "        terminated = distance_to_track > 0.5\n",
        "        truncated = self.step_count >= self.max_steps or x >= len(self.current_road) - 1\n",
        "        if terminated:\n",
        "            reward = -10.0\n",
        "        return self._get_observation(), reward, terminated, truncated, {}\n",
        "\n",
        "    def render(self, episode, avg_reward, steps):\n",
        "        plt.clf()\n",
        "        x = np.arange(0, len(self.current_road), 1)\n",
        "        y = self.current_road\n",
        "        plt.plot(x, y, 'b-', marker='o', markersize=3, label='Carretera')\n",
        "        traj_x = [p[0] for p in self.trajectory]\n",
        "        traj_y = [p[1] for p in self.trajectory]\n",
        "        plt.plot(traj_x, traj_y, 'r-', label='Trayectoria del vehículo')\n",
        "        plt.xlabel('x')\n",
        "        plt.ylabel('y')\n",
        "        plt.title(f'Carretera {self.current_road_idx + 1} - Aprendida!')\n",
        "        # Añadir cuadro con datos\n",
        "        textstr = f'Episodio: {episode}\\nCarretera: {self.current_road_idx + 1}\\nRecompensa: {avg_reward:.2f}\\nPasos: {steps}'\n",
        "        plt.text(0.02, 0.98, textstr, transform=plt.gca().transAxes, fontsize=10,\n",
        "                 verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "        plt.grid(True)\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "# Entrenar el agente\n",
        "env = RoadFollowingEnv(roads=roads)\n",
        "model = PPO(\"MlpPolicy\", env, verbose=0)\n",
        "episode_rewards = []\n",
        "log_data = []\n",
        "\n",
        "for episode in range(1200):\n",
        "    obs, _ = env.reset()\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action, _ = model.predict(obs)\n",
        "        obs, reward, terminated, truncated, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "        done = terminated or truncated\n",
        "    avg_reward = total_reward / steps\n",
        "    episode_rewards.append(avg_reward)\n",
        "    log_data.append({\"Episode\": episode + 1, \"Road\": env.current_road_idx + 1, \"Avg_Reward\": avg_reward})\n",
        "    # Mostrar mensaje y gráfica solo si aprendió (recompensa > 0.8)\n",
        "    if avg_reward > 0.8:\n",
        "        print(f\"Episodio {episode + 1}, Carretera {env.current_road_idx + 1}, Recompensa promedio: {avg_reward:.2f} - Aprendida!\")\n",
        "        env.render(episode + 1, avg_reward, steps)\n",
        "\n",
        "# Guardar logs y modelo\n",
        "pd.DataFrame(log_data).to_csv(\"training_log.csv\", index=False)\n",
        "model.save(\"ppo_road_follower_continuous\")\n",
        "print(\"Logs guardados en training_log.csv\")\n",
        "print(\"Modelo guardado como ppo_road_follower_continuous.zip\")\n",
        "\n",
        "# Visualizar conducción en carretera nueva\n",
        "test_road = [0, 0.3, -0.2, -0.5, 0.1, 0.7, -0.2, 0.4, 0.0, 0.2, -0.1, 0.3, -0.3, 0.5, -0.4, 0.1,\n",
        "             0.0, 0.2, -0.2, 0.4, -0.5, 0.0, 0.3, -0.1, 0.2, -0.4, 0.5, -0.3, 0.1, 0.0, -0.2, 0.3]\n",
        "env = RoadFollowingEnv(single_road=test_road)\n",
        "obs, _ = env.reset()\n",
        "positions = [(obs[0], obs[1])]\n",
        "for i in range(1000):\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, reward, terminated, truncated, _ = env.step(action)\n",
        "    positions.append((obs[0], obs[1]))\n",
        "    if i % 10 == 0:  # Animación cada 10 pasos\n",
        "        plt.clf()\n",
        "        plt.plot(range(len(test_road)), test_road, 'b-', marker='o', markersize=3, label='Carretera')\n",
        "        plt.plot([p[0] for p in env.trajectory], [p[1] for p in env.trajectory], 'r-', label='Trayectoria del vehículo')\n",
        "        plt.xlabel('x')\n",
        "        plt.ylabel('y')\n",
        "        plt.title('Conducción en carretera nueva')\n",
        "        plt.grid(True)\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "        time.sleep(0.1)\n",
        "    if terminated or truncated:\n",
        "        break\n",
        "\n",
        "# Gráfica final\n",
        "plt.plot(range(len(test_road)), test_road, 'b-', marker='o', markersize=3, label='Carretera')\n",
        "plt.plot([p[0] for p in positions], [p[1] for p in positions], 'r-', label='Trayectoria del vehículo')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('Trayectoria completa en carretera nueva')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Descargar resultados\n",
        "from google.colab import files\n",
        "files.download(\"ppo_road_follower_continuous.zip\")\n",
        "files.download(\"training_log.csv\")"
      ],
      "metadata": {
        "id": "g7_rFnIZ69DT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}